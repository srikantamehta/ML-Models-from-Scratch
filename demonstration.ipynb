{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from data_configs.configs import *\n",
    "from models.neural_networks import *\n",
    "from src.cross_validation import CrossValidation\n",
    "import numpy as np\n",
    "\n",
    "config = car_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srikanta\\Documents\\Intro to Machine Learning\\programming_assignment_1\\src\\data_preprocessor.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "data_1 = data_processor.impute_missing_values(raw_data)\n",
    "data_2 = data_processor.encode_ordinal_features(data_1)\n",
    "data_3 = data_processor.standardize_data(data_2,data_2,features=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     buying  maint  doors persons lug_boot safety  Class\n",
       "0     vhigh  vhigh      2       2    small    low  unacc\n",
       "1     vhigh  vhigh      2       2    small    med  unacc\n",
       "2     vhigh  vhigh      2       2    small   high  unacc\n",
       "3     vhigh  vhigh      2       2      med    low  unacc\n",
       "4     vhigh  vhigh      2       2      med    med  unacc\n",
       "...     ...    ...    ...     ...      ...    ...    ...\n",
       "1723    low    low  5more    more      med    med   good\n",
       "1724    low    low  5more    more      med   high  vgood\n",
       "1725    low    low  5more    more      big    low  unacc\n",
       "1726    low    low  5more    more      big    med   good\n",
       "1727    low    low  5more    more      big   high  vgood\n",
       "\n",
       "[1728 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_processor.encode_nominal_features(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class_acc</th>\n",
       "      <th>Class_good</th>\n",
       "      <th>Class_unacc</th>\n",
       "      <th>Class_vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        buying     maint     doors  persons  lug_boot   safety  Class_acc  \\\n",
       "599  -0.447084 -0.447084  0.447084 -1.22439   0.00000  1.22439          0   \n",
       "1201  0.447084  1.341253 -1.341253  0.00000   0.00000  0.00000          1   \n",
       "628  -0.447084 -0.447084  1.341253 -1.22439   1.22439  0.00000          0   \n",
       "1498  1.341253 -0.447084  1.341253  0.00000   0.00000  0.00000          1   \n",
       "1263  0.447084  1.341253  0.447084  1.22439   0.00000 -1.22439          0   \n",
       "...        ...       ...       ...      ...       ...      ...        ...   \n",
       "100  -1.341253 -1.341253  1.341253  1.22439  -1.22439  0.00000          0   \n",
       "274  -1.341253  0.447084  0.447084 -1.22439   0.00000  0.00000          0   \n",
       "1206  0.447084  1.341253 -1.341253  1.22439  -1.22439 -1.22439          0   \n",
       "101  -1.341253 -1.341253  1.341253  1.22439  -1.22439  1.22439          0   \n",
       "1084  0.447084  0.447084 -1.341253 -1.22439   0.00000  0.00000          0   \n",
       "\n",
       "      Class_good  Class_unacc  Class_vgood  \n",
       "599            0            1            0  \n",
       "1201           0            0            0  \n",
       "628            0            1            0  \n",
       "1498           0            0            0  \n",
       "1263           0            1            0  \n",
       "...          ...          ...          ...  \n",
       "100            0            1            0  \n",
       "274            0            1            0  \n",
       "1206           0            1            0  \n",
       "101            0            1            0  \n",
       "1084           0            1            0  \n",
       "\n",
       "[346 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Fold Cross-Validation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_linear = {'lr': 0.001, 'epochs': 17000}\n",
    "best_params_ffn = {'lr': 0.0001, 'epochs': 15000, 'n_hidden': 50, 'n_hidden_2': 50}\n",
    "best_params_auto = {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 4}\n",
    "best_params_combined = {'lr': 0.001, 'epochs': 19000, 'n_hidden_2': 40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Tested params: {'lr': 0.001, 'epochs': 17000}, Average Score: 0.40137900672626625\n",
      "FFN Model Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_hidden': 50}, Average Score: 0.09169245544147045\n",
      "Autoencoder Model Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 4}, Average Score: 0.30616363126136825\n",
      "Combined Model Tested params: {'lr': 0.001, 'epochs': 19000, 'n_hidden_2': 40}, Average Score: 0.1448744998842669\n"
     ]
    }
   ],
   "source": [
    "linear_scores = []\n",
    "ffn_scores = []\n",
    "auto_scores = []\n",
    "combined_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=1, random_state=42, stratify=True)):\n",
    "\n",
    "    train_set = data_processor.encode_nominal_features(train_set)\n",
    "    test_set = data_processor.encode_nominal_features(test_set)\n",
    "\n",
    "    train_data = train_set.to_numpy()\n",
    "    X_train = train_data[:,:-4]\n",
    "    y_train = train_data[:,-4:]\n",
    "\n",
    "    test_data = test_set.to_numpy()\n",
    "    X_test = test_data[:,:-4]\n",
    "    y_test = test_data[:,-4:]\n",
    "\n",
    "    linear = LinearNetwork(config)\n",
    "    _, linear_val_losses = linear.logistic_regression(X_train,y_train,X_test,y_test,epochs=best_params_linear['epochs'],lr=best_params_linear['lr'],patience=500)\n",
    "\n",
    "    ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=best_params_ffn['n_hidden'],n_hidden_2=best_params_ffn['n_hidden'],n_output=y_train.shape[1])\n",
    "    _, ffn_val_losses, _ = ffn.train(X_train,y_train,X_test,y_test,epochs=best_params_ffn['epochs'],lr=best_params_ffn['lr'],patience=500)\n",
    "\n",
    "    autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'])\n",
    "    auto_losses = autoE.train(X_train, max_epochs=best_params_auto['epochs'], lr=best_params_auto['lr'])\n",
    "    \n",
    "    combined = CombinedModel(autoE,n_hidden_2=best_params_combined['n_hidden_2'],n_output=y_test.shape[1])\n",
    "    _, combined_val_losses, _ = combined.train(X_train,y_train,X_test,y_test,epochs=best_params_combined['epochs'], lr=best_params_combined['lr'],patience=500)\n",
    "\n",
    "\n",
    "    linear_score = np.min(linear_val_losses)\n",
    "    ffn_score = np.min(ffn_val_losses)\n",
    "    auto_score = np.min(auto_losses)\n",
    "    combined_score = np.min(combined_val_losses)\n",
    "    \n",
    "    linear_scores.append(linear_score)\n",
    "    ffn_scores.append(ffn_score)\n",
    "    auto_scores.append(auto_score)\n",
    "    combined_scores.append(combined_score)\n",
    "\n",
    "avg_score_linear = np.mean(linear_scores)\n",
    "avg_score_ffn = np.mean(ffn_scores)\n",
    "avg_score_auto = np.mean(auto_scores)\n",
    "avg_score_combined = np.mean(combined_scores)\n",
    "\n",
    "print(f\"Linear Model Tested params: {best_params_linear}, Average Score: {avg_score_linear}\")\n",
    "print(f\"FFN Model Tested params: {best_params_ffn}, Average Score: {avg_score_ffn}\")\n",
    "print(f\"Autoencoder Model Tested params: {best_params_auto}, Average Score: {avg_score_auto}\")\n",
    "print(f\"Combined Model Tested params: {best_params_combined}, Average Score: {avg_score_combined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation and Back Propagation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n",
      " [[0.0048764  0.00451726 0.00033923 0.00475053]\n",
      " [0.00371172 0.00055143 0.00064923 0.00662943]\n",
      " [0.00342213 0.00584239 0.00453272 0.00475753]\n",
      " [0.00793712 0.00124215 0.00548516 0.00173043]\n",
      " [0.00246522 0.0090802  0.00472296 0.00313716]\n",
      " [0.00317706 0.00995521 0.00362567 0.00237262]\n",
      " [0.00985201 0.00304871 0.00266064 0.00859837]]\n",
      "X_train * W =\n",
      " [[ 0.02492487  0.02464716  0.01422556  0.00907256]\n",
      " [ 0.03802466  0.0195632   0.01656068  0.03193592]\n",
      " [ 0.0052771  -0.00367959 -0.00125494  0.00084617]\n",
      " ...\n",
      " [ 0.02867983  0.03757274  0.02428799  0.02316255]\n",
      " [ 0.00496497 -0.0201066  -0.00858616  0.00554278]\n",
      " [ 0.0036457   0.00574914  0.00154718  0.01000391]]\n",
      "\n",
      "Softmax(X_train * W) =\n",
      " [[0.25167662 0.25160674 0.24899821 0.24771843]\n",
      " [0.25288271 0.24825696 0.24751268 0.25134765]\n",
      " [0.25124672 0.24900643 0.24961092 0.25013593]\n",
      " ...\n",
      " [0.2500595  0.25229317 0.24896368 0.24868365]\n",
      " [0.25237495 0.24612618 0.24897805 0.25252082]\n",
      " [0.2496014  0.25012697 0.24907815 0.25119347]]\n",
      "\n",
      "Error =\n",
      " [[-0.25167662 -0.25160674  0.75100179 -0.24771843]\n",
      " [-0.25288271 -0.24825696  0.75248732 -0.25134765]\n",
      " [-0.25124672 -0.24900643  0.75038908 -0.25013593]\n",
      " ...\n",
      " [-0.2500595   0.74770683 -0.24896368 -0.24868365]\n",
      " [-0.25237495 -0.24612618  0.75102195 -0.25252082]\n",
      " [ 0.7503986  -0.25012697 -0.24907815 -0.25119347]]\n",
      "\n",
      "Gradient =\n",
      " [[ -22.97458807 -143.89914394  315.80539966 -148.93166765]\n",
      " [ -16.79915249   28.90768686  -36.88800936   24.77947499]\n",
      " [  12.10868698   39.98349914  -76.52399403   24.43180791]\n",
      " [  22.45417288   -0.95852583  -24.44226203    2.94661498]\n",
      " [  87.03656275   12.40125637 -105.58364844    6.14582932]\n",
      " [  26.90427364    9.82063242  -60.11390512   23.38899906]\n",
      " [ 110.76239326   13.97070739 -154.94690086   30.21380021]]\n",
      "\n",
      "Updated Weights:\n",
      " [[-1.80981858e-02 -1.39381881e-01  3.16144627e-01 -1.44181137e-01]\n",
      " [-1.30874357e-02  2.94591159e-02 -3.62387799e-02  3.14089071e-02]\n",
      " [ 1.55308152e-02  4.58258925e-02 -7.19912697e-02  2.91893361e-02]\n",
      " [ 3.03912919e-02  2.83626017e-04 -1.89570991e-02  4.67704155e-03]\n",
      " [ 8.95017835e-02  2.14814568e-02 -1.00860687e-01  9.28299034e-03]\n",
      " [ 3.00813373e-02  1.97758443e-02 -5.64882309e-02  2.57616166e-02]\n",
      " [ 1.20614407e-01  1.70194140e-02 -1.52286258e-01  3.88121721e-02]]\n",
      "\n",
      "Training Loss=\n",
      " 1.3867446334652658\n",
      "\n",
      "X_train * W =\n",
      " [[ 0.31348258 -0.16862422  0.05676785 -0.12875606]\n",
      " [ 0.1008667  -0.01927519  0.01342592  0.01106703]\n",
      " [-0.10130108 -0.15823944  0.4151777  -0.15444845]\n",
      " ...\n",
      " [ 0.1840602  -0.01385162 -0.0146984  -0.04180707]\n",
      " [-0.17542429 -0.17074604  0.48390652 -0.1559212 ]\n",
      " [ 0.04216057 -0.10376161  0.22190289 -0.13935593]]\n",
      "\n",
      "Softmax(X_train * W) =\n",
      " [[0.32963415 0.20354286 0.25500131 0.21182168]\n",
      " [0.26902099 0.23856637 0.24649671 0.24591593]\n",
      " [0.2188659  0.20675217 0.36684447 0.20753746]\n",
      " ...\n",
      " [0.2908656  0.23863841 0.23843643 0.23205956]\n",
      " [0.20169904 0.20264485 0.3899847  0.20567141]\n",
      " [0.25673029 0.22187272 0.30728277 0.21411421]]\n",
      "\n",
      "Error =\n",
      " [[-0.32963415 -0.20354286  0.74499869 -0.21182168]\n",
      " [-0.26902099 -0.23856637  0.75350329 -0.24591593]\n",
      " [-0.2188659  -0.20675217  0.63315553 -0.20753746]\n",
      " ...\n",
      " [-0.2908656   0.76136159 -0.23843643 -0.23205956]\n",
      " [-0.20169904 -0.20264485  0.6100153  -0.20567141]\n",
      " [ 0.74326971 -0.22187272 -0.30728277 -0.21411421]]\n",
      "\n",
      "Gradient =\n",
      " [[ -16.64682935 -116.47581013  253.94052263 -120.81788315]\n",
      " [ -13.17433406   24.05066491  -31.66867752   20.79234668]\n",
      " [   7.43100538   31.42713346  -56.71609507   17.85795624]\n",
      " [  17.81433282   -1.31528434  -18.49954992    2.00050144]\n",
      " [  71.77302404   10.51778879  -87.18055721    4.88974438]\n",
      " [  20.55013872    6.67232213  -45.37869352   18.15623267]\n",
      " [  89.48388669    9.21904282 -122.03438056   23.33145105]]\n",
      "\n",
      "Updated Weights:\n",
      " [[-0.03474502 -0.25585769  0.57008515 -0.26499902]\n",
      " [-0.02626177  0.05350978 -0.06790746  0.05220125]\n",
      " [ 0.02296182  0.07725303 -0.12870736  0.04704729]\n",
      " [ 0.04820562 -0.00103166 -0.03745665  0.00667754]\n",
      " [ 0.16127481  0.03199925 -0.18804124  0.01417273]\n",
      " [ 0.05063148  0.02644817 -0.10186692  0.04391785]\n",
      " [ 0.21009829  0.02623846 -0.27432064  0.06214362]]\n",
      "\n",
      "Training Loss=\n",
      " 1.1029217468926176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear = LinearNetwork(config)\n",
    "\n",
    "losses, val_losses = linear.logistic_regression(X_train,y_train,X_test,y_test,epochs=2,lr=best_params_linear['lr'],patience=np.inf,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1 Weights:\n",
      " [[0.00772433 0.00602232 0.00098712 0.00899257 0.00106798]\n",
      " [0.00238187 0.00105419 0.00947813 0.00323022 0.00969005]\n",
      " [0.00120666 0.00386683 0.00855773 0.00884138 0.00129811]\n",
      " [0.00976555 0.00011224 0.00954554 0.00911482 0.00552604]\n",
      " [0.00083935 0.00056432 0.00240898 0.0029178  0.00442341]\n",
      " [0.00823579 0.00733896 0.00556334 0.00576833 0.00948202]]\n",
      "\n",
      "Hidden Layer 2 Weights:\n",
      " [[0.00718173 0.00978488 0.00026664 0.00222894 0.00810236]\n",
      " [0.00315293 0.0014915  0.00568028 0.00180645 0.00448989]\n",
      " [0.00864898 0.00811809 0.00705292 0.00872068 0.00567593]\n",
      " [0.00561759 0.00260521 0.00598373 0.00119248 0.00636292]\n",
      " [0.00123757 0.00361576 0.0052665  0.00755197 0.00825498]]\n",
      "\n",
      "Output Layer Weights:\n",
      " [[0.00635798 0.00951931 0.00284499 0.00209112]\n",
      " [0.00780937 0.00123148 0.00613805 0.00023524]\n",
      " [0.00403521 0.00100673 0.0075278  0.0070322 ]\n",
      " [0.00095391 0.00121736 0.00135965 0.00273301]\n",
      " [0.00561701 0.0094126  0.00318333 0.00540948]]\n",
      "\n",
      "Hidden Layer 1 Biases:\n",
      " [[0. 0. 0. 0. 0.]]\n",
      "\n",
      "Hidden Layer 2 Biases:\n",
      " [[0. 0. 0. 0. 0.]]\n",
      "\n",
      "Output Layer Biases:\n",
      " [[0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "Forward Pass:\n",
      "Z1 = X*W1 + b1:\n",
      " [[ 0.01113183  0.00550914  0.01889022  0.01726     0.01110356]\n",
      " [ 0.01432807  0.02421707  0.02358838  0.02772747  0.02642998]\n",
      " [-0.0138059  -0.0006298  -0.00406525 -0.00978352 -0.00233087]\n",
      " ...\n",
      " [ 0.02125108  0.0101212   0.03926898  0.03494412  0.02739738]\n",
      " [-0.00314968  0.00850657 -0.01372452 -0.00016266 -0.01450148]\n",
      " [ 0.01490799  0.00088147  0.00959077  0.00909935  0.00557942]]\n",
      "\n",
      "A1 (Hidden 1) = tanh(Z1):\n",
      " [[ 0.01113137  0.00550909  0.01888798  0.01725829  0.01110311]\n",
      " [ 0.01432709  0.02421234  0.02358401  0.02772037  0.02642382]\n",
      " [-0.01380502 -0.0006298  -0.00406523 -0.0097832  -0.00233087]\n",
      " ...\n",
      " [ 0.02124789  0.01012085  0.0392488   0.0349299   0.02739052]\n",
      " [-0.00314967  0.00850636 -0.01372365 -0.00016266 -0.01450047]\n",
      " [ 0.01490689  0.00088147  0.00959048  0.0090991   0.00557937]]\n",
      "\n",
      "Z2 = A1*W2 + b2:\n",
      " [[ 3.71364808e-04  3.55577993e-04  3.29219952e-04  3.03909610e-04\n",
      "   4.23601438e-04]\n",
      " [ 5.71633592e-04  5.35518404e-04  6.12721127e-04  5.13949124e-04\n",
      "   7.53165695e-04]\n",
      " [-1.94132317e-04 -2.02936902e-04 -1.06745660e-04 -9.66287827e-05\n",
      "  -2.19246001e-04]\n",
      " ...\n",
      " [ 7.54088436e-04  7.31666097e-04  6.93236522e-04  6.56425066e-04\n",
      "   8.88737308e-04]\n",
      " [-1.33354782e-04 -1.82395819e-04 -1.26653161e-04 -2.21034787e-04\n",
      "  -1.85957628e-04]\n",
      " [ 2.50804105e-04  2.68911939e-04  1.60452868e-04  1.71440123e-04\n",
      "   2.83127992e-04]]\n",
      "A2 (Hidden 2) = tanh(Z2):\n",
      " [[ 3.71364791e-04  3.55577978e-04  3.29219940e-04  3.03909600e-04\n",
      "   4.23601413e-04]\n",
      " [ 5.71633530e-04  5.35518353e-04  6.12721050e-04  5.13949079e-04\n",
      "   7.53165553e-04]\n",
      " [-1.94132315e-04 -2.02936899e-04 -1.06745659e-04 -9.66287824e-05\n",
      "  -2.19245997e-04]\n",
      " ...\n",
      " [ 7.54088293e-04  7.31665967e-04  6.93236411e-04  6.56424972e-04\n",
      "   8.88737074e-04]\n",
      " [-1.33354781e-04 -1.82395817e-04 -1.26653160e-04 -2.21034784e-04\n",
      "  -1.85957626e-04]\n",
      " [ 2.50804100e-04  2.68911932e-04  1.60452867e-04  1.71440121e-04\n",
      "   2.83127984e-04]]\n",
      "\n",
      "Classification Output (w/ Softmax):\n",
      " [[0.25000031 0.25000019 0.2499999  0.2499996 ]\n",
      " [0.25000043 0.25000028 0.24999983 0.24999945]\n",
      " [0.2499998  0.24999984 0.25000008 0.25000028]\n",
      " ...\n",
      " [0.25000063 0.25000038 0.24999979 0.2499992 ]\n",
      " [0.24999986 0.24999996 0.25000003 0.25000016]\n",
      " [0.25000026 0.25000017 0.24999991 0.24999966]]\n",
      "\n",
      "Output Error:\n",
      "[[ 0.25000031  0.25000019 -0.7500001   0.2499996 ]\n",
      " [ 0.25000043  0.25000028 -0.75000017  0.24999945]\n",
      " [ 0.2499998   0.24999984 -0.74999992  0.25000028]\n",
      " ...\n",
      " [ 0.25000063 -0.74999962  0.24999979  0.2499992 ]\n",
      " [ 0.24999986  0.24999996 -0.74999997  0.25000016]\n",
      " [-0.74999974  0.25000017  0.24999991  0.24999966]]\n",
      "Equation: E_output = A_output - y_true\n",
      "\n",
      "Output Gradients (dW_output):\n",
      "[[-0.04179184 -0.01461627  0.07117541 -0.0147673 ]\n",
      " [-0.04329722 -0.01499573  0.07333536 -0.01504241]\n",
      " [-0.03695681 -0.01342984  0.06430541 -0.01391876]\n",
      " [-0.03644902 -0.01298857  0.06236521 -0.01292762]\n",
      " [-0.05282207 -0.0188205   0.0911729  -0.01953032]]\n",
      "Equation: ∂L/∂W_output = A2^T . E_output\n",
      "\n",
      "Output Bias Gradients (db_output):\n",
      "[  22.75000048  143.75000091 -315.25000025  148.74999886]\n",
      "Equation: ∂L/∂b_output = ΣE_output\n",
      "\n",
      "Hidden Layer 2 Error:\n",
      "[[ 0.00235836 -0.00228451 -0.00262732  0.00020633  0.00272228]\n",
      " [ 0.00235836 -0.00228451 -0.00262732  0.00020633  0.00272228]\n",
      " [ 0.00235836 -0.00228452 -0.00262732  0.00020633  0.00272227]\n",
      " ...\n",
      " [-0.00431595  0.00262205  0.00389375  0.00034862 -0.00350699]\n",
      " [ 0.00235836 -0.00228452 -0.00262732  0.00020633  0.00272227]\n",
      " [-0.00115463 -0.00395583  0.00086527  0.00061207  0.0002886 ]]\n",
      "Equation: E_hidden_2 = (E_output . W_output^T) * (1 - A2^2), where (1 - A2^2) is the derivative of tanh activation.\n",
      "\n",
      "Hidden Layer 2 Gradients (dW_hidden_2):\n",
      "[[-9.33126728e-03  3.35085955e-03  9.94158959e-03 -2.87277811e-05\n",
      "  -8.97614356e-03]\n",
      " [-4.74416555e-03  2.65212608e-03  5.21194167e-03 -1.98788809e-04\n",
      "  -4.96471269e-03]\n",
      " [-1.00731770e-02  3.34133903e-03  1.06407292e-02  5.19527390e-05\n",
      "  -9.51653609e-03]\n",
      " [-9.17636040e-03  3.97058698e-03  9.83399571e-03 -1.24067757e-04\n",
      "  -9.04468006e-03]\n",
      " [-1.01738325e-02  4.36805370e-03  1.08994487e-02 -1.32323019e-04\n",
      "  -1.00160955e-02]]\n",
      "Equation: ∂L/∂W_hidden_2 = A1^T . E_hidden_2\n",
      "\n",
      "Hidden Layer 2 Bias Gradients (db_hidden_2):\n",
      "[ 0.92721485 -1.54534023 -1.0905814   0.17460248  1.28196335]\n",
      "Equation: ∂L/∂b_hidden_2 = ΣE_hidden_2\n",
      "\n",
      "Hidden Layer 1 Error:\n",
      "[[ 1.63975960e-05  1.69989850e-06  5.71834090e-07  9.14044533e-06\n",
      "   4.85154031e-06]\n",
      " [ 1.63962714e-05  1.69895294e-06  5.71726729e-07  9.13614420e-06\n",
      "   4.84874722e-06]\n",
      " [ 1.63964294e-05  1.69994549e-06  5.71989746e-07  9.14227242e-06\n",
      "   4.85210915e-06]\n",
      " ...\n",
      " [-3.19246982e-05 -2.69549061e-06 -5.43702606e-06 -1.59945229e-05\n",
      "  -1.67021663e-06]\n",
      " [ 1.63994064e-05  1.69982440e-06  5.71898258e-07  9.14315245e-06\n",
      "   4.85111639e-06]\n",
      " [-4.30566849e-05 -2.22416245e-06 -2.90190551e-05 -9.04750062e-06\n",
      "  -4.17050035e-06]]\n",
      "Equation: E_hidden_1 = (E_hidden_2 . W_hidden_2^T) * (1 - A1^2), where (1 - A1^2) is the derivative of tanh activation for the first hidden layer.\n",
      "\n",
      "Hidden Layer 1 Gradients (dW_hidden_1):\n",
      "[[ 6.84072576e-04 -2.13434509e-05  1.16571497e-03 -2.41646898e-04\n",
      "  -1.33739016e-04]\n",
      " [-1.61780602e-03 -1.86110371e-04  2.20828858e-04 -1.06081995e-03\n",
      "  -4.63764870e-04]\n",
      " [-1.19660423e-03 -8.04710079e-05 -5.89465719e-04 -3.66159593e-04\n",
      "  -2.09111398e-04]\n",
      " [-5.54012595e-03 -3.88909298e-04 -2.44788811e-03 -1.86662531e-03\n",
      "  -8.89563630e-04]\n",
      " [-1.13309138e-03 -1.16710630e-04 -8.74595323e-05 -5.98212336e-04\n",
      "  -3.98581634e-04]\n",
      " [-5.98358378e-03 -4.49858852e-04 -2.35775391e-03 -2.15781662e-03\n",
      "  -1.21092328e-03]]\n",
      "Equation: ∂L/∂W_hidden_1 = X^T . E_hidden_1\n",
      "\n",
      "Hidden Layer 1 Bias Gradients (db_hidden_1):\n",
      "[ 0.00202274  0.00049498 -0.00341762  0.00302141  0.00171719]\n",
      "Equation: ∂L/∂b_hidden_1 = ΣE_hidden_1\n",
      "\n",
      "New Weights/Biases:\n",
      "Output W:\n",
      " [[0.00636216 0.00952077 0.00283788 0.00209259]\n",
      " [0.0078137  0.00123298 0.00613072 0.00023674]\n",
      " [0.00403891 0.00100808 0.00752137 0.00703359]\n",
      " [0.00095756 0.00121866 0.00135342 0.0027343 ]\n",
      " [0.00562229 0.00941448 0.00317421 0.00541143]]\n",
      "\n",
      "Output biases:\n",
      " [[-0.002275 -0.014375  0.031525 -0.014875]]\n",
      "\n",
      "Hidden 2 W:\n",
      " [[0.00718266 0.00978454 0.00026565 0.00222894 0.00810326]\n",
      " [0.00315341 0.00149124 0.00567975 0.00180647 0.00449039]\n",
      " [0.00864999 0.00811776 0.00705186 0.00872068 0.00567688]\n",
      " [0.0056185  0.00260482 0.00598274 0.00119249 0.00636383]\n",
      " [0.00123858 0.00361533 0.00526541 0.00755198 0.00825598]]\n",
      "\n",
      "Hidden 2 biases:\n",
      " [[-9.27214851e-05  1.54534023e-04  1.09058140e-04 -1.74602478e-05\n",
      "  -1.28196335e-04]]\n",
      "\n",
      "Hidden 1 W:\n",
      " [[0.00772426 0.00602232 0.00098701 0.00899259 0.00106799]\n",
      " [0.00238203 0.00105421 0.00947811 0.00323032 0.0096901 ]\n",
      " [0.00120678 0.00386684 0.00855779 0.00884142 0.00129813]\n",
      " [0.0097661  0.00011228 0.00954579 0.00911501 0.00552612]\n",
      " [0.00083947 0.00056434 0.00240899 0.00291786 0.00442345]\n",
      " [0.00823639 0.00733901 0.00556357 0.00576855 0.00948214]]\n",
      "\n",
      "Hidden 1 biases:\n",
      " [[-2.02274376e-07 -4.94976737e-08  3.41761897e-07 -3.02141317e-07\n",
      "  -1.71718635e-07]]\n",
      "\n",
      "Classification Loss: 1.365822689615614\n",
      "\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "Forward Pass:\n",
      "Z1 = X*W1 + b1:\n",
      " [[ 0.01113322  0.00550919  0.01889143  0.01726014  0.01110365]\n",
      " [ 0.01432834  0.02421708  0.02358861  0.0277275   0.02643   ]\n",
      " [-0.01380662 -0.00062988 -0.00406514 -0.00978399 -0.00233113]\n",
      " ...\n",
      " [ 0.02125205  0.01012125  0.03926963  0.03494432  0.02739746]\n",
      " [-0.00315081  0.00850645 -0.01372461 -0.00016326 -0.01450181]\n",
      " [ 0.01490832  0.00088146  0.00959131  0.00909925  0.00557933]]\n",
      "\n",
      "A1 (Hidden 1) = tanh(Z1):\n",
      " [[ 0.01113276  0.00550914  0.01888918  0.01725842  0.01110319]\n",
      " [ 0.01432736  0.02421235  0.02358424  0.0277204   0.02642385]\n",
      " [-0.01380575 -0.00062988 -0.00406512 -0.00978368 -0.00233112]\n",
      " ...\n",
      " [ 0.02124885  0.0101209   0.03924945  0.0349301   0.0273906 ]\n",
      " [-0.0031508   0.00850625 -0.01372374 -0.00016326 -0.01450079]\n",
      " [ 0.01490722  0.00088146  0.00959102  0.009099    0.00557927]]\n",
      "\n",
      "Z2 = A1*W2 + b2:\n",
      " [[ 2.78723878e-04  5.10112857e-04  4.38225396e-04  2.86464237e-04\n",
      "   2.95482366e-04]\n",
      " [ 4.79017289e-04  6.90015569e-04  7.21673348e-04  4.96492872e-04\n",
      "   6.25072241e-04]\n",
      " [-2.86889943e-04 -4.84003115e-05  2.33890981e-06 -1.14092525e-04\n",
      "  -3.47480875e-04]\n",
      " ...\n",
      " [ 6.61504938e-04  8.86166934e-04  8.02169042e-04  6.38974420e-04\n",
      "   7.60675102e-04]\n",
      " [-2.26116923e-04 -2.78666941e-05 -1.75726299e-05 -2.38501651e-04\n",
      "  -3.14196966e-04]\n",
      " [ 1.58126925e-04  4.23438437e-04  2.69473222e-04  1.53984672e-04\n",
      "   1.54972643e-04]]\n",
      "A2 (Hidden 2) = tanh(Z2):\n",
      " [[ 2.78723870e-04  5.10112812e-04  4.38225368e-04  2.86464229e-04\n",
      "   2.95482358e-04]\n",
      " [ 4.79017252e-04  6.90015459e-04  7.21673223e-04  4.96492831e-04\n",
      "   6.25072159e-04]\n",
      " [-2.86889936e-04 -4.84003115e-05  2.33890981e-06 -1.14092525e-04\n",
      "  -3.47480861e-04]\n",
      " ...\n",
      " [ 6.61504842e-04  8.86166702e-04  8.02168870e-04  6.38974333e-04\n",
      "   7.60674956e-04]\n",
      " [-2.26116919e-04 -2.78666941e-05 -1.75726299e-05 -2.38501647e-04\n",
      "  -3.14196956e-04]\n",
      " [ 1.58126924e-04  4.23438411e-04  2.69473215e-04  1.53984670e-04\n",
      "   1.54972642e-04]]\n",
      "\n",
      "Classification Output (w/ Softmax):\n",
      " [[0.24938757 0.24638752 0.25796071 0.24626419]\n",
      " [0.24938769 0.24638762 0.25796064 0.24626405]\n",
      " [0.24938706 0.24638718 0.25796091 0.24626486]\n",
      " ...\n",
      " [0.24938789 0.24638771 0.2579606  0.2462638 ]\n",
      " [0.24938712 0.24638729 0.25796085 0.24626474]\n",
      " [0.24938752 0.2463875  0.25796072 0.24626425]]\n",
      "\n",
      "Output Error:\n",
      "[[ 0.24938757  0.24638752 -0.74203929  0.24626419]\n",
      " [ 0.24938769  0.24638762 -0.74203936  0.24626405]\n",
      " [ 0.24938706  0.24638718 -0.74203909  0.24626486]\n",
      " ...\n",
      " [ 0.24938789 -0.75361229  0.2579606   0.2462638 ]\n",
      " [ 0.24938712  0.24638729 -0.74203915  0.24626474]\n",
      " [-0.75061248  0.2463875   0.25796072  0.24626425]]\n",
      "Equation: E_output = A_output - y_true\n",
      "\n",
      "Output Gradients (dW_output):\n",
      "[[-0.04387094 -0.0277176   0.0999131  -0.02832456]\n",
      " [-0.03984583  0.00683459  0.02546379  0.00754745]\n",
      " [-0.03451438  0.00198216  0.03050275  0.00202947]\n",
      " [-0.03683849 -0.01544693  0.067757   -0.01547158]\n",
      " [-0.05569225 -0.0369273   0.1308868  -0.03826724]]\n",
      "Equation: ∂L/∂W_output = A2^T . E_output\n",
      "\n",
      "Output Bias Gradients (db_output):\n",
      "[  22.3265985   141.25364869 -309.74907401  146.16882682]\n",
      "Equation: ∂L/∂b_output = ΣE_output\n",
      "\n",
      "Hidden Layer 2 Error:\n",
      "[[ 0.00234196 -0.0022385  -0.0025934   0.00020814  0.00269899]\n",
      " [ 0.00234196 -0.0022385  -0.0025934   0.00020814  0.00269899]\n",
      " [ 0.00234196 -0.00223851 -0.0025934   0.00020814  0.00269899]\n",
      " ...\n",
      " [-0.00434093  0.00265923  0.00391989  0.00034289 -0.00354127]\n",
      " [ 0.00234196 -0.0022385  -0.0025934   0.00020814  0.00269899]\n",
      " [-0.00118233 -0.00392148  0.00088906  0.000604    0.00025091]]\n",
      "Equation: E_hidden_2 = (E_output . W_output^T) * (1 - A2^2), where (1 - A2^2) is the derivative of tanh activation.\n",
      "\n",
      "Hidden Layer 2 Gradients (dW_hidden_2):\n",
      "[[-9.36317980e-03  3.32431873e-03  9.91837614e-03 -5.46938696e-05\n",
      "  -9.01693066e-03]\n",
      " [-4.75927226e-03  2.63435672e-03  5.19667314e-03 -2.12556164e-04\n",
      "  -4.98393561e-03]\n",
      " [-1.01024103e-02  3.30732181e-03  1.06116687e-02  2.55038056e-05\n",
      "  -9.55365468e-03]\n",
      " [-9.20814515e-03  3.94588697e-03  9.81181860e-03 -1.49706861e-04\n",
      "  -9.08553168e-03]\n",
      " [-1.01995610e-02  4.32521629e-03  1.08637710e-02 -1.58979080e-04\n",
      "  -1.00483410e-02]]\n",
      "Equation: ∂L/∂W_hidden_2 = A1^T . E_hidden_2\n",
      "\n",
      "Hidden Layer 2 Bias Gradients (db_hidden_2):\n",
      "[ 0.9137321  -1.51576325 -1.06907702  0.17397016  1.26312963]\n",
      "Equation: ∂L/∂b_hidden_2 = ΣE_hidden_2\n",
      "\n",
      "Hidden Layer 1 Error:\n",
      "[[ 1.65623778e-05  1.81259643e-06  9.34647894e-07  9.23314462e-06\n",
      "   5.00656049e-06]\n",
      " [ 1.65610439e-05  1.81159062e-06  9.34472295e-07  9.22880312e-06\n",
      "   5.00368161e-06]\n",
      " [ 1.65611916e-05  1.81264202e-06  9.34917767e-07  9.23498375e-06\n",
      "   5.00714078e-06]\n",
      " ...\n",
      " [-3.20358537e-05 -2.74113502e-06 -5.42424619e-06 -1.61184816e-05\n",
      "  -1.76860758e-06]\n",
      " [ 1.65641989e-05  1.81251297e-06  9.34764257e-07  9.23587298e-06\n",
      "   5.00611645e-06]\n",
      " [-4.32368757e-05 -2.30874790e-06 -2.90968444e-05 -9.22080728e-06\n",
      "  -4.32752560e-06]]\n",
      "Equation: E_hidden_1 = (E_hidden_2 . W_hidden_2^T) * (1 - A1^2), where (1 - A1^2) is the derivative of tanh activation for the first hidden layer.\n",
      "\n",
      "Hidden Layer 1 Gradients (dW_hidden_1):\n",
      "[[ 6.75429398e-04 -2.63706266e-05  1.15450335e-03 -2.48396464e-04\n",
      "  -1.41709803e-04]\n",
      " [-1.63771277e-03 -1.98093655e-04  1.90919381e-04 -1.07528394e-03\n",
      "  -4.82079473e-04]\n",
      " [-1.20547140e-03 -8.53450630e-05 -5.99073761e-04 -3.73380578e-04\n",
      "  -2.17075799e-04]\n",
      " [-5.57630147e-03 -4.09244787e-04 -2.49088503e-03 -1.89531447e-03\n",
      "  -9.22241944e-04]\n",
      " [-1.15046442e-03 -1.26930822e-04 -1.11694402e-04 -6.11187174e-04\n",
      "  -4.14416607e-04]\n",
      " [-6.03342838e-03 -4.78510722e-04 -2.42216288e-03 -2.19615464e-03\n",
      "  -1.25610869e-03]]\n",
      "Equation: ∂L/∂W_hidden_1 = X^T . E_hidden_1\n",
      "\n",
      "Hidden Layer 1 Bias Gradients (db_hidden_1):\n",
      "[ 0.0020706   0.00053505 -0.00325114  0.00303445  0.00176447]\n",
      "Equation: ∂L/∂b_hidden_1 = ΣE_hidden_1\n",
      "\n",
      "New Weights/Biases:\n",
      "Output W:\n",
      " [[0.00636655 0.00952355 0.00282788 0.00209543]\n",
      " [0.00781768 0.0012323  0.00612817 0.00023599]\n",
      " [0.00404236 0.00100788 0.00751832 0.00703339]\n",
      " [0.00096124 0.00122021 0.00134664 0.00273585]\n",
      " [0.00562786 0.00941817 0.00316112 0.00541526]]\n",
      "\n",
      "Output biases:\n",
      " [[-0.00450766 -0.02850036  0.06249991 -0.02949188]]\n",
      "\n",
      "Hidden 2 W:\n",
      " [[0.0071836  0.00978421 0.00026465 0.00222895 0.00810416]\n",
      " [0.00315388 0.00149097 0.00567923 0.00180649 0.00449088]\n",
      " [0.008651   0.00811743 0.00705079 0.00872067 0.00567783]\n",
      " [0.00561943 0.00260442 0.00598176 0.00119251 0.00636474]\n",
      " [0.0012396  0.00361489 0.00526432 0.007552   0.00825698]]\n",
      "\n",
      "Hidden 2 biases:\n",
      " [[-1.84094695e-04  3.06110348e-04  2.15965842e-04 -3.48572640e-05\n",
      "  -2.54509298e-04]]\n",
      "\n",
      "Hidden 1 W:\n",
      " [[0.00772419 0.00602233 0.00098689 0.00899261 0.00106801]\n",
      " [0.00238219 0.00105423 0.00947809 0.00323043 0.00969014]\n",
      " [0.0012069  0.00386685 0.00855785 0.00884145 0.00129815]\n",
      " [0.00976666 0.00011232 0.00954604 0.0091152  0.00552622]\n",
      " [0.00083958 0.00056435 0.002409   0.00291792 0.00442349]\n",
      " [0.00823699 0.00733905 0.00556381 0.00576877 0.00948227]]\n",
      "\n",
      "Hidden 1 biases:\n",
      " [[-4.09334405e-07 -1.03002353e-07  6.66876264e-07 -6.05586038e-07\n",
      "  -3.48165252e-07]]\n",
      "\n",
      "Classification Loss: 1.346061553146349\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=5,n_hidden_2=5,n_output=y_train.shape[1],verbose=True)\n",
    " \n",
    "metrics, val_metrics, final_metric = ffn.train(X_train,y_train,X_test,y_test,epochs=2,lr=best_params_ffn['lr'],patience=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Encoder Weights:\n",
      "[[0.00040495 0.00728966 0.00732425 0.00679254]\n",
      " [0.00918556 0.00495604 0.00256729 0.00943121]\n",
      " [0.00338202 0.00784474 0.00308926 0.0034591 ]\n",
      " [0.00952418 0.00874715 0.00205027 0.0016668 ]\n",
      " [0.00086363 0.00244828 0.00302101 0.0039168 ]\n",
      " [0.00232631 0.00773301 0.00817539 0.00815417]]\n",
      "\n",
      "Initial Decoder Weights:\n",
      "[[0.00720015 0.00305315 0.00593426 0.00850788 0.00258921 0.00068197]\n",
      " [0.00554123 0.00900657 0.00715757 0.00220975 0.00033281 0.00041953]\n",
      " [0.00028299 0.00311497 0.00873493 0.00559643 0.00637345 0.00484705]\n",
      " [0.00976626 0.00128913 0.00317859 0.00884707 0.00669565 0.00664766]]\n",
      "\n",
      "Initial Encoder Biases:\n",
      "[[0.00040495 0.00728966 0.00732425 0.00679254]\n",
      " [0.00918556 0.00495604 0.00256729 0.00943121]\n",
      " [0.00338202 0.00784474 0.00308926 0.0034591 ]\n",
      " [0.00952418 0.00874715 0.00205027 0.0016668 ]\n",
      " [0.00086363 0.00244828 0.00302101 0.0039168 ]\n",
      " [0.00232631 0.00773301 0.00817539 0.00815417]]\n",
      "\n",
      "Initial Decoder Biases:\n",
      "[[0.00720015 0.00305315 0.00593426 0.00850788 0.00258921 0.00068197]\n",
      " [0.00554123 0.00900657 0.00715757 0.00220975 0.00033281 0.00041953]\n",
      " [0.00028299 0.00311497 0.00873493 0.00559643 0.00637345 0.00484705]\n",
      " [0.00976626 0.00128913 0.00317859 0.00884707 0.00669565 0.00664766]]\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "Forward Pass:\n",
      "\n",
      "\n",
      "Encoder layer Z1;\n",
      "[[ 0.00723989  0.01727299  0.00709551 -0.00030023]\n",
      " [ 0.00964387  0.02870231  0.02860897  0.03913843]\n",
      " [-0.0062236  -0.00824599 -0.00325593  0.0006854 ]\n",
      " ...\n",
      " [ 0.02975609  0.03413574  0.01707064  0.0271625 ]\n",
      " [-0.01477027 -0.0026388   0.00384782 -0.00039602]\n",
      " [ 0.01337961  0.00967987  0.00185263  0.002952  ]]\n",
      "\n",
      "Encoder Activation Function (Sigmoid) A1 = sigmoid(Z1):\n",
      "[[0.50180996 0.50431814 0.50177387 0.49992494]\n",
      " [0.50241095 0.50717509 0.50715175 0.50978336]\n",
      " [0.49844411 0.49793851 0.49918602 0.50017135]\n",
      " ...\n",
      " [0.50743847 0.50853311 0.50426756 0.50679021]\n",
      " [0.4963075  0.4993403  0.50096195 0.49990099]\n",
      " [0.50334485 0.50241995 0.50046316 0.500738  ]]\n",
      "\n",
      "Decoder layer output:\n",
      "[[0.01143204 0.00828175 0.01255958 0.01261476 0.00801249 0.00630926]\n",
      " [0.01155    0.00833878 0.01266191 0.0127435  0.00811528 0.00640247]\n",
      " [0.01137413 0.00820628 0.01247213 0.01255972 0.0079868  0.00629338]\n",
      " ...\n",
      " [0.01156367 0.00835352 0.01266676 0.01274665 0.00809032 0.00637259]\n",
      " [0.01136437 0.00821756 0.01248413 0.01255219 0.00799125 0.00629932]\n",
      " [0.01144014 0.00826631 0.01254624 0.01262348 0.00801292 0.00630856]]\n",
      "\n",
      "Decoder error:\n",
      "[[ 1.35268456  1.34953428 -1.32869294 -1.21177568 -1.21637795 -1.21808118]\n",
      " [-1.32970253 -1.33291374 -1.32859061  1.23713394 -1.21627516 -1.21798797]\n",
      " [ 0.4584583  -0.4388779  -0.43461205  1.23695016  0.0079868   0.00629338]\n",
      " ...\n",
      " [-0.4355205  -1.332899   -1.32858577 -1.21164378 -1.21630011  0.00637259]\n",
      " [-1.32988815  0.45530174 -0.43460004  1.23694263  1.23238169  0.00629932]\n",
      " [-0.43564403 -0.43881786  0.45963042 -1.21176695  1.23240336  0.00630856]]\n",
      "Equation: δ_decoder = A_output - X\n",
      "\n",
      "Decoder gradients (Weights):\n",
      "[[ 4.56357036 16.27108713 -1.90977795 -7.59942078  9.23200038  4.13053539]\n",
      " [ 3.41179292 16.87103119 -2.77072403 -7.43386296  8.9319811   3.2973983 ]\n",
      " [ 3.4134127  17.25149863 -1.9412394  -6.33244087  8.9119559   3.25218431]\n",
      " [ 3.46530798 16.08496808 -2.03215617 -6.24388451  8.73193064  3.1703423 ]]\n",
      "Equation: ∂L/∂W_decoder = A1^T . δ_decoder\n",
      "\n",
      "Decoder Biases:\n",
      "[  9.21458959  35.64276129  -2.53821704 -12.12239753  18.99252532\n",
      "   9.24892373]\n",
      "Equation: ∂L/∂b_decoder = sum(δ_decoder)\n",
      "\n",
      "Encoder error:\n",
      "[[-0.00207866  0.00163649 -0.00686435 -0.00405887]\n",
      " [-0.00374548 -0.00676478 -0.00571519 -0.00605345]\n",
      " [ 0.00248265 -0.00044611  0.00049256  0.00339222]\n",
      " ...\n",
      " [-0.00713416 -0.00674983 -0.00759502 -0.0072526 ]\n",
      " [ 0.00073862 -0.00080826  0.00301329  0.00136357]\n",
      " [-0.0022157  -0.0013353   0.00090708 -0.00144656]]\n",
      "Equation: δ_encoder = (δ_decoder . W_decoder^T) * A1 * (1 - A1), where '*' denotes element-wise multiplication and the term A1 * (1 - A1) is the derivative of the sigmoid activation function.\n",
      "\n",
      "Encoder gradients (Weights):\n",
      "[[-1.23317739 -1.03208032 -0.07125068 -1.58069128]\n",
      " [-0.54573637 -1.57065387 -0.61442741 -0.3293315 ]\n",
      " [-1.1323769  -1.33706777 -1.59216272 -0.72121839]\n",
      " [-1.3585996  -0.27818311 -0.93984363 -1.41541038]\n",
      " [-0.56867902 -0.16850107 -1.19781928 -1.26128894]\n",
      " [-0.07892852 -0.1375316  -0.8944968  -1.05181361]]\n",
      "Equation: ∂L/∂W_encoder = X^T . δ_encoder\n",
      "\n",
      "Encoder Biases:\n",
      "[0.0281118  0.08432931 0.04737508 0.05231638]\n",
      "Equation: ∂L/∂b_encoder = sum(δ_encoder)\n",
      "\n",
      "New Decoder Weights:\n",
      "[[ 6.74378882e-03  1.42603667e-03  6.12524019e-03  9.26781720e-03\n",
      "   1.66601118e-03  2.68918086e-04]\n",
      " [ 5.20005161e-03  7.31947166e-03  7.43464034e-03  2.95313393e-03\n",
      "  -5.60386703e-04  8.97920225e-05]\n",
      " [-5.83549278e-05  1.38981781e-03  8.92905875e-03  6.22966917e-03\n",
      "   5.48225646e-03  4.52183626e-03]\n",
      " [ 9.41972588e-03 -3.19368844e-04  3.38181004e-03  9.47145428e-03\n",
      "   5.82245236e-03  6.33062911e-03]]\n",
      "\n",
      "New Decoder Biases:\n",
      "[[-0.00092146 -0.00356428  0.00025382  0.00121224 -0.00189925 -0.00092489]]\n",
      "\n",
      "New Encoder Weights:\n",
      "[[0.00052827 0.00739287 0.00733138 0.00695061]\n",
      " [0.00924014 0.00511311 0.00262873 0.00946414]\n",
      " [0.00349526 0.00797844 0.00324848 0.00353122]\n",
      " [0.00966004 0.00877496 0.00214425 0.00180834]\n",
      " [0.0009205  0.00246513 0.00314079 0.00404293]\n",
      " [0.0023342  0.00774677 0.00826484 0.00825935]]\n",
      "\n",
      "New Encoder Biases:\n",
      "[[-0.00092146 -0.00356428  0.00025382  0.00121224 -0.00189925 -0.00092489]]\n",
      "\n",
      "Output Loss: 0.9894704806225957\n",
      "\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "Forward Pass:\n",
      "\n",
      "\n",
      "Encoder layer Z1;\n",
      "[[ 7.39600088e-03  1.71663320e-02  7.58361157e-03 -8.39376322e-06]\n",
      " [ 9.94448133e-03  2.92257187e-02  2.90508526e-02  3.95960297e-02]\n",
      " [-6.37286209e-03 -8.20462469e-03 -3.28027348e-03  4.83160558e-04]\n",
      " ...\n",
      " [ 3.02694638e-02  3.46181425e-02  1.76267801e-02  2.76965776e-02]\n",
      " [-1.48174224e-02 -2.57394223e-03  3.63461708e-03 -4.99454897e-04]\n",
      " [ 1.35024255e-02  9.74145549e-03  1.77577738e-03  3.01879180e-03]]\n",
      "\n",
      "Encoder Activation Function (Sigmoid) A1 = sigmoid(Z1):\n",
      "[[0.50184899 0.50429148 0.50189589 0.4999979 ]\n",
      " [0.5024861  0.50730591 0.5072622  0.50989771]\n",
      " [0.49840679 0.49794886 0.49917993 0.50012079]\n",
      " ...\n",
      " [0.50756679 0.50865367 0.50440658 0.5069237 ]\n",
      " [0.49629571 0.49935651 0.50090865 0.49987514]\n",
      " [0.50337556 0.50243534 0.50044394 0.5007547 ]]\n",
      "\n",
      "Decoder layer output:\n",
      "[[0.0097658  0.00138039 0.01324935 0.01521488 0.00431697 0.00469014]\n",
      " [0.00987871 0.00140766 0.01335706 0.01535688 0.0044034  0.00477752]\n",
      " [0.00971092 0.00132524 0.01315727 0.01514849 0.00430062 0.00467714]\n",
      " ...\n",
      " [0.00989214 0.00142175 0.01336264 0.01536199 0.00437814 0.00474726]\n",
      " [0.00970159 0.00133501 0.01316941 0.01514152 0.00430436 0.00468296]\n",
      " [0.00977366 0.00136672 0.01323449 0.01522167 0.004317   0.00468861]]\n",
      "\n",
      "Decoder error:\n",
      "[[ 1.35101833  1.34263291 -1.32800318 -1.20917556 -1.22007347 -1.2197003 ]\n",
      " [-1.33137381 -1.33984487 -1.32789547  1.23974732 -1.21998703 -1.21961292]\n",
      " [ 0.4567951  -0.44575894 -0.4339269   1.23953893  0.00430062  0.00467714]\n",
      " ...\n",
      " [-0.43719204 -1.33983078 -1.32788988 -1.20902845 -1.2200123   0.00474726]\n",
      " [-1.33155093  0.44841919 -0.43391476  1.23953196  1.2286948   0.00468296]\n",
      " [-0.43731052 -0.44571746  0.46031867 -1.20916877  1.22870744  0.00468861]]\n",
      "Equation: δ_decoder = A_output - X\n",
      "\n",
      "Decoder gradients (Weights):\n",
      "[[ 3.9672262  13.88153938 -1.69410592 -6.72540973  7.94539885  3.5709579 ]\n",
      " [ 2.81752912 14.4634482  -2.5585799  -6.54114088  7.65231003  2.73504898]\n",
      " [ 2.83676104 14.85975217 -1.73317746 -5.45252602  7.61485501  2.6768605 ]\n",
      " [ 2.86452687 13.69815196 -1.81045942 -5.3710149   7.43382371  2.59481922]]\n",
      "Equation: ∂L/∂W_decoder = A1^T . δ_decoder\n",
      "\n",
      "Decoder Biases:\n",
      "[  8.06467119  30.88316631  -2.0638821  -10.33091576  16.44306493\n",
      "   8.1313155 ]\n",
      "Equation: ∂L/∂b_decoder = sum(δ_decoder)\n",
      "\n",
      "Encoder error:\n",
      "[[-0.00266891  0.00099564 -0.00745175 -0.00461789]\n",
      " [-0.00247334 -0.00559063 -0.00452936 -0.00491957]\n",
      " [ 0.00282077 -0.00011373  0.00081147  0.00369317]\n",
      " ...\n",
      " [-0.00655574 -0.00620788 -0.00697251 -0.00667514]\n",
      " [ 0.00063446 -0.00097389  0.00282638  0.00119259]\n",
      " [-0.00248069 -0.00159324  0.00068521 -0.00167227]]\n",
      "Equation: δ_encoder = (δ_decoder . W_decoder^T) * A1 * (1 - A1), where '*' denotes element-wise multiplication and the term A1 * (1 - A1) is the derivative of the sigmoid activation function.\n",
      "\n",
      "Encoder gradients (Weights):\n",
      "[[-1.14623068 -0.96468319 -0.00366483 -1.51283362]\n",
      " [-0.25691658 -1.2733196  -0.31114767 -0.04600925]\n",
      " [-1.14754259 -1.36879636 -1.60927515 -0.74045465]\n",
      " [-1.49259459 -0.40902913 -1.05238    -1.52601635]\n",
      " [-0.40765934 -0.01306648 -1.04148655 -1.10849959]\n",
      " [ 0.00678637 -0.06466031 -0.82191512 -0.9820765 ]]\n",
      "Equation: ∂L/∂W_encoder = X^T . δ_encoder\n",
      "\n",
      "Encoder Biases:\n",
      "[0.00490459 0.05341129 0.02164545 0.02712103]\n",
      "Equation: ∂L/∂b_encoder = sum(δ_encoder)\n",
      "\n",
      "New Decoder Weights:\n",
      "[[ 6.34706620e-03  3.78827349e-05  6.29465079e-03  9.94035817e-03\n",
      "   8.71471292e-04 -8.81777042e-05]\n",
      " [ 4.91829870e-03  5.87312684e-03  7.69049832e-03  3.60724802e-03\n",
      "  -1.32561771e-03 -1.83712875e-04]\n",
      " [-3.42031032e-04 -9.61574029e-05  9.10237650e-03  6.77492178e-03\n",
      "   4.72077096e-03  4.25415021e-03]\n",
      " [ 9.13327320e-03 -1.68918404e-03  3.56285598e-03  1.00085558e-02\n",
      "   5.07906998e-03  6.07114719e-03]]\n",
      "\n",
      "New Decoder Biases:\n",
      "[[-0.00172793 -0.00665259  0.00046021  0.00224533 -0.00354356 -0.00173802]]\n",
      "\n",
      "New Encoder Weights:\n",
      "[[0.0006429  0.00748934 0.00733174 0.0071019 ]\n",
      " [0.00926583 0.00524044 0.00265985 0.00946874]\n",
      " [0.00361001 0.00811532 0.00340941 0.00360527]\n",
      " [0.0098093  0.00881587 0.00224949 0.00196095]\n",
      " [0.00096127 0.00246644 0.00324494 0.00415378]\n",
      " [0.00233353 0.00775323 0.00834703 0.00835756]]\n",
      "\n",
      "New Encoder Biases:\n",
      "[[-0.00172793 -0.00665259  0.00046021  0.00224533 -0.00354356 -0.00173802]]\n",
      "\n",
      "Output Loss: 0.9893459678105961\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'],verbose=True)\n",
    "losses = autoE.train(X_train, max_epochs=2, lr=best_params_auto['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008333</td>\n",
       "      <td>-0.004565</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.017460</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.003297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008442</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008281</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.003286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008221</td>\n",
       "      <td>-0.004621</td>\n",
       "      <td>0.013668</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008378</td>\n",
       "      <td>-0.004565</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.017516</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.003312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.008342</td>\n",
       "      <td>-0.004585</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.003292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.008277</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.003279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.008455</td>\n",
       "      <td>-0.004550</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.003348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0.008272</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.017376</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.003292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.008341</td>\n",
       "      <td>-0.004577</td>\n",
       "      <td>0.013832</td>\n",
       "      <td>0.017464</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.003294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5\n",
       "0    0.008333 -0.004565  0.013849  0.017460  0.001135  0.003297\n",
       "1    0.008442 -0.004564  0.013961  0.017613  0.001207  0.003379\n",
       "2    0.008281 -0.004602  0.013752  0.017382  0.001127  0.003286\n",
       "3    0.008221 -0.004621  0.013668  0.017296  0.001098  0.003252\n",
       "4    0.008378 -0.004565  0.013882  0.017516  0.001150  0.003312\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "686  0.008342 -0.004585  0.013818  0.017462  0.001134  0.003292\n",
       "687  0.008277 -0.004606  0.013743  0.017378  0.001122  0.003279\n",
       "688  0.008455 -0.004550  0.013968  0.017620  0.001182  0.003348\n",
       "689  0.008272 -0.004593  0.013765  0.017376  0.001130  0.003292\n",
       "690  0.008341 -0.004577  0.013832  0.017464  0.001135  0.003294\n",
       "\n",
       "[691 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_, Output = autoE.forward_pass(X_train)\n",
    "pd.DataFrame(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2        3        4        5\n",
       "0   -1.341253 -1.341253  1.341253  1.22439  1.22439  1.22439\n",
       "1    1.341253  1.341253  1.341253 -1.22439  1.22439  1.22439\n",
       "2   -0.447084  0.447084  0.447084 -1.22439  0.00000  0.00000\n",
       "3   -0.447084  0.447084 -0.447084 -1.22439  0.00000 -1.22439\n",
       "4   -0.447084  0.447084  0.447084  1.22439 -1.22439  1.22439\n",
       "..        ...       ...       ...      ...      ...      ...\n",
       "686  1.341253  1.341253 -1.341253  1.22439 -1.22439 -1.22439\n",
       "687  0.447084  0.447084 -1.341253  0.00000  1.22439 -1.22439\n",
       "688  0.447084  1.341253  1.341253  1.22439  1.22439  0.00000\n",
       "689  1.341253 -0.447084  0.447084 -1.22439 -1.22439  0.00000\n",
       "690  0.447084  0.447084 -0.447084  1.22439 -1.22439  0.00000\n",
       "\n",
       "[691 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Weights (Hidden Layer 1):\n",
      "[[-0.19897223 -0.21660858  0.16426085  0.43285014]\n",
      " [-0.38491301  0.31715251  0.19540782  0.01999713]\n",
      " [ 0.36271531  0.30134397  0.13822726  0.56357072]\n",
      " [ 0.54346778  0.03582247  0.19582056 -0.14137119]\n",
      " [-0.0208248   0.0097908   0.78290707 -0.17904159]\n",
      " [-0.014578    0.62268923 -0.07072667 -0.0681484 ]]\n",
      "\n",
      "Autoencoder Biases (Hidden Layer 1):\n",
      "[[-0.02741079 -0.01100522 -0.01800552 -0.04949936]]\n",
      "\n",
      "Hidden Layer 2 Initial Weights:\n",
      "[[0.00214517 0.00088517 0.00516021 0.00746642 0.00363071]\n",
      " [0.00151387 0.00722286 0.00126583 0.00835262 0.00771288]\n",
      " [0.00919317 0.00858486 0.00137869 0.00024996 0.00582891]\n",
      " [0.00394396 0.00931276 0.00046398 0.00747815 0.00666105]]\n",
      "\n",
      "Hidden Layer 2 Initial Biases:\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "\n",
      "Output Layer Initial Biases:\n",
      "[[0.0076336  0.00915909 0.00521226 0.00088066]\n",
      " [0.00807222 0.00778139 0.00544301 0.00120706]\n",
      " [0.00920974 0.00803674 0.00309442 0.00048366]\n",
      " [0.00674751 0.00548631 0.00661776 0.0015848 ]\n",
      " [0.00133052 0.00356454 0.00765107 0.00735614]]\n",
      "\n",
      "Output Layer Initial Biases:\n",
      "[[0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "\n",
      "Forward Pass:\n",
      "\n",
      "Encoder layer (Hidden 1):\n",
      "[[ 1.86428949  1.07658154  0.79673333 -0.37674184]\n",
      " [-1.03281913  1.25856977  1.2820247   1.18421032]\n",
      " [-0.61379445  0.31849635 -0.1820418   0.19097767]\n",
      " ...\n",
      " [ 0.49377821  0.77756093  1.70126655  0.53442261]\n",
      " [-0.59994907 -0.36444822 -1.02159951  1.16639548]\n",
      " [ 0.24029352 -0.06890682 -0.63782567 -0.05287878]]\n",
      "\n",
      "Sigmoid activation function:\n",
      "[[0.86579614 0.74584653 0.68927528 0.40691297]\n",
      " [0.26253792 0.7787798  0.78279423 0.76570399]\n",
      " [0.35119412 0.57895776 0.45461482 0.54759983]\n",
      " ...\n",
      " [0.62099608 0.6851542  0.84570008 0.63051402]\n",
      " [0.35435535 0.4098832  0.26471595 0.76249286]\n",
      " [0.55978598 0.48278011 0.34573821 0.48678338]]\n",
      "\n",
      "Second hidden layer:\n",
      "[[0.01092787 0.01586034 0.0065509  0.01590942 0.01562427]\n",
      " [0.01195843 0.0197084  0.00377505 0.01438679 0.01662306]\n",
      " [0.0079689  0.01349507 0.00342594 0.01166665 0.01203801]\n",
      " ...\n",
      " [0.01263076 0.0186305  0.00553026 0.01528592 0.01666856]\n",
      " [0.00682148 0.01264766 0.00306613 0.01183757 0.01106995]\n",
      " [0.00702999 0.01148397 0.00420225 0.01193874 0.01101381]]\n",
      "\n",
      "Second hidden layer activation function (tanh):\n",
      "[[0.01092743 0.01585901 0.00655081 0.01590807 0.015623  ]\n",
      " [0.01195786 0.01970585 0.00377504 0.0143858  0.01662153]\n",
      " [0.00796873 0.01349425 0.00342593 0.01166612 0.01203743]\n",
      " ...\n",
      " [0.01263009 0.01862835 0.0055302  0.01528473 0.01666702]\n",
      " [0.00682138 0.01264698 0.00306612 0.01183702 0.0110695 ]\n",
      " [0.00702987 0.01148347 0.00420223 0.01193817 0.01101337]]\n",
      "\n",
      "Classification Output (Softmax):\n",
      "[[0.25001376 0.25001856 0.25001087 0.24995681]\n",
      " [0.25001229 0.25001906 0.25001213 0.24995652]\n",
      " [0.2500093  0.2500134  0.25000901 0.24996829]\n",
      " ...\n",
      " [0.25001404 0.25002037 0.25001153 0.24995406]\n",
      " [0.25000883 0.25001206 0.2500088  0.24997031]\n",
      " [0.25000944 0.25001244 0.25000823 0.24996988]]\n",
      "\n",
      "Output Error:\n",
      "[[ 0.25001376  0.25001856 -0.74998913  0.24995681]\n",
      " [ 0.25001229  0.25001906 -0.74998787  0.24995652]\n",
      " [ 0.2500093   0.2500134  -0.74999099  0.24996829]\n",
      " ...\n",
      " [ 0.25001404 -0.74997963  0.25001153  0.24995406]\n",
      " [ 0.25000883  0.25001206 -0.7499912   0.24997031]\n",
      " [-0.74999056  0.25001244  0.25000823  0.24996988]]\n",
      "Equation: δ_output = A_output - y_true\n",
      "\n",
      "Output Weight Gradients:\n",
      "[[ 8.21386988e-02  1.14437931e+00 -2.39924736e+00  1.17272935e+00]\n",
      " [ 1.15739650e-01  1.76569033e+00 -3.69208564e+00  1.81065566e+00]\n",
      " [-1.65456202e-03  5.93867447e-01 -1.19873753e+00  6.06524645e-01]\n",
      " [ 4.10555123e-02  1.65247340e+00 -3.38610568e+00  1.69257677e+00]\n",
      " [ 5.86047444e-02  1.63856178e+00 -3.37438859e+00  1.67722206e+00]]\n",
      "Equation: ∂L/∂W_output = A2^T . δ_output\n",
      "\n",
      "Output Bias Gradients:\n",
      "[  22.75689567  143.7596007  -315.24415449  148.72765811]\n",
      "Equation: ∂L/∂b_output = sum(δ_output)\n",
      "\n",
      "Hidden Layer 2 Error:\n",
      "[[ 0.00050938  0.00018313  0.00211192 -0.00150809 -0.002675  ]\n",
      " [ 0.00050936  0.0001831   0.00211198 -0.00150816 -0.00267491]\n",
      " [ 0.00050932  0.00018307  0.0021119  -0.00150832 -0.00267522]\n",
      " ...\n",
      " [-0.00343683 -0.00215444 -0.00283021 -0.00037692  0.00141048]\n",
      " [ 0.00050932  0.00018306  0.00211189 -0.00150832 -0.00267527]\n",
      " [-0.0019119  -0.0024458  -0.00400333 -0.00163804  0.0036445 ]]\n",
      "Equation: δ_hidden_2 = (δ_output . W_output^T) * (1 - A2^2), noting the derivative of tanh activation is 1 - tanh^2(x)\n",
      "\n",
      "Hidden Layer 2 Weight Gradients:\n",
      "[[-0.03755226 -0.15019299  0.16484872 -0.46740499 -0.32211477]\n",
      " [-0.04928833 -0.15371753  0.10660458 -0.42398561 -0.24653917]\n",
      " [-0.02201778 -0.12170412  0.17558634 -0.41648292 -0.31673429]\n",
      " [-0.00167776 -0.10182006  0.24526479 -0.43415501 -0.39683825]]\n",
      "Equation: ∂L/∂W_hidden_2 = A1^T . δ_hidden_2\n",
      "\n",
      "Hidden Layer 2 Bias Gradients:\n",
      "[-0.02172689 -0.23396011  0.46137688 -0.90812111 -0.7751029 ]\n",
      "Equation: ∂L/∂b_hidden_2 = sum(δ_hidden_2)\n",
      "\n",
      "Hidden Layer 1 (Encoder) Error:\n",
      "[[-1.02475484e-06 -5.39509920e-06 -1.45696029e-06 -5.88898919e-06]\n",
      " [-1.70752955e-06 -4.90335460e-06 -1.15659547e-06 -4.37773543e-06]\n",
      " [-2.01019036e-06 -6.93888342e-06 -1.68723827e-06 -6.04612428e-06]\n",
      " ...\n",
      " [-5.07842265e-06 -3.58438894e-06 -5.98505738e-06 -6.60574908e-06]\n",
      " [-2.01846664e-06 -6.88530079e-06 -1.32462733e-06 -4.41989530e-06]\n",
      " [-6.38796618e-06 -2.79665055e-06 -5.26116661e-06 -5.03355952e-06]]\n",
      "Equation: δ_hidden_1 = (δ_hidden_2 . W_hidden_2^T) * A1 * (1 - A1), where '*' denotes element-wise multiplication and A1 * (1 - A1) is the derivative of the sigmoid function.\n",
      "\n",
      "Hidden Layer 1 (Encoder) Weight Gradients:\n",
      "[[ 3.52115993e-04  5.20532571e-04  2.36565338e-04  5.13983112e-04]\n",
      " [ 1.88405747e-04  5.21168213e-04  5.82526020e-05  5.10605911e-04]\n",
      " [-1.37324609e-05  7.64236422e-05 -2.46768703e-05  5.09735051e-05]\n",
      " [-2.71966541e-04  4.62597049e-04 -3.12366151e-04  1.89222283e-04]\n",
      " [ 1.91043820e-04  5.56410660e-04  2.08317309e-04  5.31037887e-04]\n",
      " [-6.95727623e-06  9.41884819e-04 -1.89609034e-05  7.86786782e-04]]\n",
      "Equation: ∂L/∂W_hidden_1 = X^T . δ_hidden_1\n",
      "\n",
      "Hidden Layer 1 (Encoder) Bias Gradients:\n",
      "[-0.00164827 -0.0032003  -0.00136121 -0.00305595]\n",
      "Equation: ∂L/∂b_hidden_1 = sum(δ_hidden_1)\n",
      "\n",
      "New Weights/Biases:\n",
      "Output W:\n",
      " [[ 0.00755146  0.00801471  0.00761151 -0.00029207]\n",
      " [ 0.00795648  0.0060157   0.00913509 -0.00060359]\n",
      " [ 0.0092114   0.00744287  0.00429316 -0.00012286]\n",
      " [ 0.00670645  0.00383384  0.01000387 -0.00010777]\n",
      " [ 0.00127191  0.00192598  0.01102546  0.00567892]]\n",
      "\n",
      "Output biases:\n",
      " [[-0.0227569  -0.1437596   0.31524415 -0.14872766]]\n",
      "\n",
      "Hidden 2 W:\n",
      " [[0.00218272 0.00103537 0.00499536 0.00793382 0.00395282]\n",
      " [0.00156316 0.00737658 0.00115923 0.00877661 0.00795942]\n",
      " [0.00921518 0.00870656 0.0012031  0.00066644 0.00614564]\n",
      " [0.00394564 0.00941458 0.00021871 0.00791231 0.00705789]]\n",
      "\n",
      "Hidden 2 biases:\n",
      " [[ 2.17268875e-05  2.33960109e-04 -4.61376877e-04  9.08121111e-04\n",
      "   7.75102901e-04]]\n",
      "\n",
      "Hidden 1 W:\n",
      " [[-0.19897258 -0.2166091   0.16426061  0.43284963]\n",
      " [-0.3849132   0.31715199  0.19540776  0.01999662]\n",
      " [ 0.36271532  0.3013439   0.13822729  0.56357067]\n",
      " [ 0.54346805  0.035822    0.19582087 -0.14137138]\n",
      " [-0.02082499  0.00979025  0.78290686 -0.17904212]\n",
      " [-0.014578    0.62268829 -0.07072665 -0.06814918]]\n",
      "\n",
      "Hidden 1 biases:\n",
      " [[-0.02740914 -0.01100202 -0.01800416 -0.0494963 ]]\n",
      "\n",
      "Classification Loss: 1.1985126269661899\n",
      "\n",
      "\n",
      "Backpass:\n",
      "\n",
      "\n",
      "Forward Pass:\n",
      "\n",
      "Encoder layer (Hidden 1):\n",
      "[[ 1.86429199  1.07658363  0.79673527 -0.37673932]\n",
      " [-1.03281875  1.2585702   1.28202508  1.18421055]\n",
      " [-0.61379305  0.31850008 -0.18204073  0.19098094]\n",
      " ...\n",
      " [ 0.49377957  0.77756185  1.70126789  0.5344238 ]\n",
      " [-0.5999479  -0.36444428 -1.02159855  1.16639893]\n",
      " [ 0.24029549 -0.06890394 -0.63782382 -0.05287574]]\n",
      "\n",
      "Sigmoid activation function:\n",
      "[[0.86579643 0.74584692 0.68927569 0.40691357]\n",
      " [0.262538   0.77877988 0.78279429 0.76570403]\n",
      " [0.35119444 0.57895867 0.45461508 0.54760064]\n",
      " ...\n",
      " [0.6209964  0.6851544  0.84570026 0.6305143 ]\n",
      " [0.35435561 0.40988416 0.26471614 0.76249349]\n",
      " [0.55978647 0.48278083 0.34573863 0.48678414]]\n",
      "\n",
      "Second hidden layer:\n",
      "[[0.01103474 0.01646432 0.00564646 0.01800219 0.01724195]\n",
      " [0.01204692 0.02027474 0.00286213 0.01640627 0.01822653]\n",
      " [0.00804329 0.01398187 0.00263083 0.01341149 0.01343029]\n",
      " ...\n",
      " [0.01272926 0.01923018 0.00459034 0.01740076 0.0183307 ]\n",
      " [0.00688383 0.01310771 0.00226915 0.01352641 0.01244669]\n",
      " [0.00710497 0.01196788 0.00341704 0.01366855 0.01239095]]\n",
      "\n",
      "Second hidden layer activation function (tanh):\n",
      "[[0.01103429 0.01646283 0.0056464  0.01800024 0.01724024]\n",
      " [0.01204633 0.02027196 0.00286212 0.0164048  0.01822452]\n",
      " [0.00804312 0.01398096 0.00263082 0.01341068 0.01342949]\n",
      " ...\n",
      " [0.01272857 0.01922781 0.00459031 0.017399   0.01832864]\n",
      " [0.00688372 0.01310696 0.00226915 0.01352559 0.01244605]\n",
      " [0.00710485 0.01196731 0.00341702 0.0136677  0.01239032]]\n",
      "\n",
      "Classification Output (Softmax):\n",
      "[[0.23982162 0.21247311 0.33633788 0.21136739]\n",
      " [0.2398196  0.21247201 0.33634265 0.21136574]\n",
      " [0.23981948 0.21247534 0.33632123 0.21138396]\n",
      " ...\n",
      " [0.23982117 0.21247265 0.33634303 0.21136314]\n",
      " [0.23981937 0.21247542 0.33631821 0.211387  ]\n",
      " [0.23982014 0.21247608 0.33631692 0.21138685]]\n",
      "\n",
      "Output Error:\n",
      "[[ 0.23982162  0.21247311 -0.66366212  0.21136739]\n",
      " [ 0.2398196   0.21247201 -0.66365735  0.21136574]\n",
      " [ 0.23981948  0.21247534 -0.66367877  0.21138396]\n",
      " ...\n",
      " [ 0.23982117 -0.78752735  0.33634303  0.21136314]\n",
      " [ 0.23981937  0.21247542 -0.66368179  0.211387  ]\n",
      " [-0.76017986  0.21247608  0.33631692  0.21138685]]\n",
      "Equation: δ_output = A_output - y_true\n",
      "\n",
      "Output Weight Gradients:\n",
      "[[ 0.02341599  0.93824969 -1.92246378  0.9607981 ]\n",
      " [ 0.02804776  1.49091546 -3.04721109  1.52824787]\n",
      " [-0.03899596  0.39328273 -0.75424969  0.39996292]\n",
      " [-0.02851852  1.55526428 -3.11979372  1.59304796]\n",
      " [-0.01156883  1.49692908 -3.01753883  1.53217857]]\n",
      "Equation: ∂L/∂W_output = A2^T . δ_output\n",
      "\n",
      "Output Bias Gradients:\n",
      "[  15.71577707  117.82091818 -255.60280403  122.06610878]\n",
      "Equation: ∂L/∂b_output = sum(δ_output)\n",
      "\n",
      "Hidden Layer 2 Error:\n",
      "[[-0.00159909 -0.00300307  0.0009153  -0.00423766 -0.00540098]\n",
      " [-0.00159904 -0.00300263  0.00091531 -0.00423786 -0.00540076]\n",
      " [-0.00159931 -0.00300346  0.00091525 -0.00423844 -0.0054017 ]\n",
      " ...\n",
      " [-0.00200213  0.00011551 -0.00223433  0.00193047  0.00369568]\n",
      " [-0.00159937 -0.00300356  0.00091523 -0.00423846 -0.00540186]\n",
      " [-0.00153932 -0.00182521 -0.00400295 -0.00094165  0.00435017]]\n",
      "Equation: δ_hidden_2 = (δ_output . W_output^T) * (1 - A2^2), noting the derivative of tanh activation is 1 - tanh^2(x)\n",
      "\n",
      "Hidden Layer 2 Weight Gradients:\n",
      "[[-0.47052761 -0.79708575 -0.10448226 -0.99873637 -0.85231418]\n",
      " [-0.41628037 -0.69942211 -0.12893085 -0.86369432 -0.68538753]\n",
      " [-0.41124275 -0.70156814 -0.07028183 -0.88695501 -0.78645418]\n",
      " [-0.43823654 -0.75451637 -0.02364514 -0.97117597 -0.93302264]]\n",
      "Equation: ∂L/∂W_hidden_2 = A1^T . δ_hidden_2\n",
      "\n",
      "Hidden Layer 2 Bias Gradients:\n",
      "[-0.91813648 -1.57456424 -0.09064562 -2.01273146 -1.87776831]\n",
      "Equation: ∂L/∂b_hidden_2 = sum(δ_hidden_2)\n",
      "\n",
      "Hidden Layer 1 (Encoder) Error:\n",
      "[[-6.62269306e-06 -1.96709336e-05 -1.62339710e-05 -2.55890023e-05]\n",
      " [-1.10353747e-05 -1.78774295e-05 -1.28867507e-05 -1.90213386e-05]\n",
      " [-1.29895744e-05 -2.52999013e-05 -1.87959163e-05 -2.62714552e-05]\n",
      " ...\n",
      " [ 3.41568052e-06  8.95029635e-06  5.04534247e-07  7.93419176e-06]\n",
      " [-1.30428721e-05 -2.51046527e-05 -1.47558865e-05 -1.92052342e-05]\n",
      " [-3.82483057e-06  1.46077046e-06 -1.98727466e-06 -2.19907612e-07]]\n",
      "Equation: δ_hidden_1 = (δ_hidden_2 . W_hidden_2^T) * A1 * (1 - A1), where '*' denotes element-wise multiplication and A1 * (1 - A1) is the derivative of the sigmoid function.\n",
      "\n",
      "Hidden Layer 1 (Encoder) Weight Gradients:\n",
      "[[0.001151   0.00208211 0.00119096 0.00189405]\n",
      " [0.00161646 0.00228756 0.0016728  0.00275483]\n",
      " [0.00029646 0.00039408 0.00028583 0.00065782]\n",
      " [0.00123938 0.00283253 0.0017253  0.0027873 ]\n",
      " [0.00116884 0.00210537 0.00113888 0.0021596 ]\n",
      " [0.00235058 0.00418195 0.00309597 0.00475508]]\n",
      "Equation: ∂L/∂W_hidden_1 = X^T . δ_hidden_1\n",
      "\n",
      "Hidden Layer 1 (Encoder) Bias Gradients:\n",
      "[-0.00593062 -0.00993516 -0.00752101 -0.01033957]\n",
      "Equation: ∂L/∂b_hidden_1 = sum(δ_hidden_1)\n",
      "\n",
      "New Weights/Biases:\n",
      "Output W:\n",
      " [[ 0.00752805  0.00707646  0.00953397 -0.00125287]\n",
      " [ 0.00792843  0.00452478  0.0121823  -0.00213184]\n",
      " [ 0.00925039  0.00704959  0.00504741 -0.00052282]\n",
      " [ 0.00673497  0.00227857  0.01312366 -0.00170082]\n",
      " [ 0.00128348  0.00042905  0.014043    0.00414674]]\n",
      "\n",
      "Output biases:\n",
      " [[-0.03847267 -0.26158052  0.57084696 -0.27079377]]\n",
      "\n",
      "Hidden 2 W:\n",
      " [[0.00265325 0.00183245 0.00509984 0.00893256 0.00480514]\n",
      " [0.00197944 0.008076   0.00128816 0.0096403  0.00864481]\n",
      " [0.00962643 0.00940813 0.00127339 0.00155339 0.0069321 ]\n",
      " [0.00438387 0.01016909 0.00024236 0.00888349 0.00799091]]\n",
      "\n",
      "Hidden 2 biases:\n",
      " [[ 0.00093986  0.00180852 -0.00037073  0.00292085  0.00265287]]\n",
      "\n",
      "Hidden 1 W:\n",
      " [[-0.19897373 -0.21661118  0.16425942  0.43284774]\n",
      " [-0.38491482  0.3171497   0.19540609  0.01999387]\n",
      " [ 0.36271503  0.3013435   0.138227    0.56357001]\n",
      " [ 0.54346681  0.03581917  0.19581915 -0.14137416]\n",
      " [-0.02082616  0.00978814  0.78290572 -0.17904428]\n",
      " [-0.01458035  0.62268411 -0.07072975 -0.06815394]]\n",
      "\n",
      "Hidden 1 biases:\n",
      " [[-0.02740321 -0.01099208 -0.01799663 -0.04948596]]\n",
      "\n",
      "Classification Loss: 1.0757682138231675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined = CombinedModel(autoE,n_hidden_2=5,n_output=y_test.shape[1],verbose=True)\n",
    "metrics, val_metrics, final_metric = combined.train(X_train,y_train,X_test,y_test,epochs=2, lr=best_params_combined['lr'],patience=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
