{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from src.cross_validation import CrossValidation\n",
    "from src.evaluation import Evaluation\n",
    "from models.knn import KNN\n",
    "from models.null_model import NullModelClassification\n",
    "from data_configs.configs import *\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "config = breast_cancer_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)\n",
    "knn_model = KNN(config)\n",
    "null_model = NullModelClassification(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "\n",
    "raw_data_2 = raw_data.drop(columns=['Sample code number'])\n",
    "\n",
    "data_1 = data_processor.impute_missing_values(raw_data_2)\n",
    "\n",
    "data_2 = data_processor.encode_nominal_features(data_1)\n",
    "\n",
    "data_3 = data_processor.encode_ordinal_features(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "0                  5                        1                         1   \n",
       "1                  5                        4                         4   \n",
       "2                  3                        1                         1   \n",
       "3                  6                        8                         8   \n",
       "4                  4                        1                         1   \n",
       "..               ...                      ...                       ...   \n",
       "694                3                        1                         1   \n",
       "695                2                        1                         1   \n",
       "696                5                       10                        10   \n",
       "697                4                        8                         6   \n",
       "698                4                        8                         8   \n",
       "\n",
       "     Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei  \\\n",
       "0                    1                            2          1.0   \n",
       "1                    5                            7         10.0   \n",
       "2                    1                            2          2.0   \n",
       "3                    1                            3          4.0   \n",
       "4                    3                            2          1.0   \n",
       "..                 ...                          ...          ...   \n",
       "694                  1                            3          2.0   \n",
       "695                  1                            2          1.0   \n",
       "696                  3                            7          3.0   \n",
       "697                  4                            3          4.0   \n",
       "698                  5                            4          5.0   \n",
       "\n",
       "     Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0                  3                1        1      2  \n",
       "1                  3                2        1      2  \n",
       "2                  3                1        1      2  \n",
       "3                  3                7        1      2  \n",
       "4                  3                1        1      2  \n",
       "..               ...              ...      ...    ...  \n",
       "694                1                1        1      2  \n",
       "695                1                1        1      2  \n",
       "696                8               10        2      4  \n",
       "697               10                6        1      4  \n",
       "698               10                4        1      4  \n",
       "\n",
       "[699 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 Loss score with k=1: 0.05428571428571429\n",
      "Average 0-1 Loss score with k=2: 0.08\n",
      "Average 0-1 Loss score with k=3: 0.03357142857142857\n",
      "Average 0-1 Loss score with k=4: 0.045714285714285714\n",
      "Average 0-1 Loss score with k=5: 0.03142857142857143\n",
      "Average 0-1 Loss score with k=6: 0.03571428571428571\n",
      "Average 0-1 Loss score with k=7: 0.02785714285714285\n",
      "Average 0-1 Loss score with k=8: 0.03\n",
      "Average 0-1 Loss score with k=9: 0.027142857142857142\n",
      "Average 0-1 Loss score with k=10: 0.03\n",
      "Average 0-1 Loss score with k=11: 0.02857142857142857\n",
      "Average 0-1 Loss score with k=12: 0.03142857142857143\n",
      "Average 0-1 Loss score with k=13: 0.029285714285714283\n",
      "Average 0-1 Loss score with k=14: 0.03357142857142857\n",
      "Best k is 9 with the lowest average 0-1 loss score of 0.027142857142857142\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        predictions_1 = knn_model.knn_classifier(data_val, train_set_1, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 Loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.35062724014336916\n",
      "Average null model Precision score: 0.421685374674015\n",
      "Average null model Recall score: 0.6493727598566308\n",
      "Average null model F1 score: 0.5113278649043025\n",
      "Average KNN 0-1 score for k=9: 0.03577956989247312\n",
      "Average Precision score for k=9: 0.9645357411379856\n",
      "Average Recall score for k=9: 0.9642204301075268\n",
      "Average F1 score for k=9: 0.9641369555377652\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Additional lists for null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(test_set, train_set, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(test_set[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(test_set[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(test_set[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(test_set[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Null model predictions and metrics calculation\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "# Calculate averages for all metrics\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "\n",
    "# Additional averages for null model metrics\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "# Print out all average scores\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edited KNN ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 loss score with k=1: 0.034999999999999996\n",
      "Average 0-1 loss score with k=2: 0.04928571428571428\n",
      "Average 0-1 loss score with k=3: 0.029285714285714283\n",
      "Average 0-1 loss score with k=4: 0.03357142857142857\n",
      "Average 0-1 loss score with k=5: 0.029285714285714286\n",
      "Average 0-1 loss score with k=6: 0.02857142857142857\n",
      "Average 0-1 loss score with k=7: 0.027857142857142858\n",
      "Average 0-1 loss score with k=8: 0.02857142857142857\n",
      "Average 0-1 loss score with k=9: 0.02785714285714285\n",
      "Average 0-1 loss score with k=10: 0.03214285714285715\n",
      "Average 0-1 loss score with k=11: 0.03214285714285715\n",
      "Average 0-1 loss score with k=12: 0.03642857142857143\n",
      "Average 0-1 loss score with k=13: 0.03428571428571429\n",
      "Average 0-1 loss score with k=14: 0.037142857142857144\n",
      "Best k is 9 with the lowest average 0-1 loss score of 0.02785714285714285\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        edited_train_set = knn_model.edited_knn_classification(train_set_1, train_set_2)\n",
    "\n",
    "        predictions_1 = knn_model.knn_classifier(data_val, edited_train_set, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.35062724014336916\n",
      "Average null model Precision score: 0.421685374674015\n",
      "Average null model Recall score: 0.6493727598566308\n",
      "Average null model F1 score: 0.5113278649043025\n",
      "Average KNN 0-1 score for k=9: 0.04078981054787506\n",
      "Average Precision score for k=9: 0.959379852134522\n",
      "Average Recall score for k=9: 0.959210189452125\n",
      "Average F1 score for k=9: 0.9590563638775824\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Additional lists for null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    edited_train_set = knn_model.edited_knn_classification(train_set, data_val)\n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(test_set, edited_train_set, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(test_set[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(test_set[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(test_set[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(test_set[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Null model predictions and metrics calculation\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "# Calculate averages for all metrics\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "\n",
    "# Additional averages for null model metrics\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "# Print out all average scores\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensed Knn ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 Loss score with k=1: 0.07642857142857142\n",
      "Average 0-1 Loss score with k=2: 0.15285714285714286\n",
      "Average 0-1 Loss score with k=3: 0.05285714285714286\n",
      "Average 0-1 Loss score with k=4: 0.08785714285714284\n",
      "Average 0-1 Loss score with k=5: 0.049999999999999996\n",
      "Average 0-1 Loss score with k=6: 0.05928571428571429\n",
      "Average 0-1 Loss score with k=7: 0.04285714285714286\n",
      "Average 0-1 Loss score with k=8: 0.045\n",
      "Average 0-1 Loss score with k=9: 0.07999999999999999\n",
      "Average 0-1 Loss score with k=10: 0.05\n",
      "Average 0-1 Loss score with k=11: 0.085\n",
      "Average 0-1 Loss score with k=12: 0.10642857142857143\n",
      "Average 0-1 Loss score with k=13: 0.21500000000000002\n",
      "Average 0-1 Loss score with k=14: 0.09714285714285714\n",
      "Best k is 7 with the lowest average 0-1 loss score of 0.04285714285714286\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        condesed_train_set = knn_model.condensed_knn_classification(train_set_1)\n",
    "        predictions_1 = knn_model.knn_classifier(data_val, condesed_train_set, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 Loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.35062724014336916\n",
      "Average null model Precision score: 0.421685374674015\n",
      "Average null model Recall score: 0.6493727598566308\n",
      "Average null model F1 score: 0.5113278649043025\n",
      "Average KNN 0-1 score for k=7: 0.03899385560675883\n",
      "Average Precision score for k=7: 0.9615396077888876\n",
      "Average Recall score for k=7: 0.9610061443932413\n",
      "Average F1 score for k=7: 0.960945312796736\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Additional lists for null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    condesed_train_set = knn_model.condensed_knn_classification(train_set)\n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(test_set, condesed_train_set, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(test_set[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(test_set[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(test_set[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(test_set[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Null model predictions and metrics calculation\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "# Calculate averages for all metrics\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "\n",
    "# Additional averages for null model metrics\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "# Print out all average scores\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
