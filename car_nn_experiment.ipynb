{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from data_configs.configs import *\n",
    "from models.neural_networks import *\n",
    "from src.cross_validation import CrossValidation\n",
    "import numpy as np\n",
    "\n",
    "config = car_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srikanta\\Documents\\Intro to Machine Learning\\programming_assignment_1\\src\\data_preprocessor.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "data_1 = data_processor.impute_missing_values(raw_data)\n",
    "data_2 = data_processor.encode_ordinal_features(data_1)\n",
    "data_3 = data_processor.standardize_data(data_2,data_2,features=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        buying     maint     doors  persons  lug_boot   safety  Class\n",
       "0    -1.341253 -1.341253 -1.341253 -1.22439  -1.22439 -1.22439  unacc\n",
       "1    -1.341253 -1.341253 -1.341253 -1.22439  -1.22439  0.00000  unacc\n",
       "2    -1.341253 -1.341253 -1.341253 -1.22439  -1.22439  1.22439  unacc\n",
       "3    -1.341253 -1.341253 -1.341253 -1.22439   0.00000 -1.22439  unacc\n",
       "4    -1.341253 -1.341253 -1.341253 -1.22439   0.00000  0.00000  unacc\n",
       "...        ...       ...       ...      ...       ...      ...    ...\n",
       "1723  1.341253  1.341253  1.341253  1.22439   0.00000  0.00000   good\n",
       "1724  1.341253  1.341253  1.341253  1.22439   0.00000  1.22439  vgood\n",
       "1725  1.341253  1.341253  1.341253  1.22439   1.22439 -1.22439  unacc\n",
       "1726  1.341253  1.341253  1.341253  1.22439   1.22439  0.00000   good\n",
       "1727  1.341253  1.341253  1.341253  1.22439   1.22439  1.22439  vgood\n",
       "\n",
       "[1728 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_processor.encode_nominal_features(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class_acc</th>\n",
       "      <th>Class_good</th>\n",
       "      <th>Class_unacc</th>\n",
       "      <th>Class_vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        buying     maint     doors  persons  lug_boot   safety  Class_acc  \\\n",
       "599  -0.447084 -0.447084  0.447084 -1.22439   0.00000  1.22439          0   \n",
       "1201  0.447084  1.341253 -1.341253  0.00000   0.00000  0.00000          1   \n",
       "628  -0.447084 -0.447084  1.341253 -1.22439   1.22439  0.00000          0   \n",
       "1498  1.341253 -0.447084  1.341253  0.00000   0.00000  0.00000          1   \n",
       "1263  0.447084  1.341253  0.447084  1.22439   0.00000 -1.22439          0   \n",
       "...        ...       ...       ...      ...       ...      ...        ...   \n",
       "100  -1.341253 -1.341253  1.341253  1.22439  -1.22439  0.00000          0   \n",
       "274  -1.341253  0.447084  0.447084 -1.22439   0.00000  0.00000          0   \n",
       "1206  0.447084  1.341253 -1.341253  1.22439  -1.22439 -1.22439          0   \n",
       "101  -1.341253 -1.341253  1.341253  1.22439  -1.22439  1.22439          0   \n",
       "1084  0.447084  0.447084 -1.341253 -1.22439   0.00000  0.00000          0   \n",
       "\n",
       "      Class_good  Class_unacc  Class_vgood  \n",
       "599            0            1            0  \n",
       "1201           0            0            0  \n",
       "628            0            1            0  \n",
       "1498           0            0            0  \n",
       "1263           0            1            0  \n",
       "...          ...          ...          ...  \n",
       "100            0            1            0  \n",
       "274            0            1            0  \n",
       "1206           0            1            0  \n",
       "101            0            1            0  \n",
       "1084           0            1            0  \n",
       "\n",
       "[346 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_val.to_numpy()\n",
    "X_val = data_test[:,:-4]\n",
    "y_val = data_test[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.341253</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.341253</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1.341253</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.447084</td>\n",
       "      <td>0.447084</td>\n",
       "      <td>-0.447084</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>-1.22439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        buying     maint     doors  persons  lug_boot   safety  Class\n",
       "107  -1.341253 -1.341253  1.341253  1.22439   1.22439  1.22439  unacc\n",
       "901   0.447084 -1.341253 -0.447084  0.00000  -1.22439  0.00000  unacc\n",
       "1709  1.341253  1.341253  1.341253 -1.22439   1.22439  1.22439  unacc\n",
       "706  -0.447084  0.447084  0.447084 -1.22439   0.00000  0.00000  unacc\n",
       "678  -0.447084  0.447084 -0.447084 -1.22439   0.00000 -1.22439  unacc\n",
       "...        ...       ...       ...      ...       ...      ...    ...\n",
       "1130  0.447084  0.447084 -0.447084  1.22439   0.00000  1.22439  vgood\n",
       "1294  0.447084  1.341253  1.341253  1.22439   1.22439  0.00000   good\n",
       "860  -0.447084  1.341253  1.341253  1.22439   0.00000  1.22439    acc\n",
       "1459  1.341253 -0.447084  0.447084 -1.22439  -1.22439  0.00000  unacc\n",
       "1126  0.447084  0.447084 -0.447084  1.22439  -1.22439  0.00000    acc\n",
       "\n",
       "[1382 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'lr': 0.0001, 'epochs': 15263}\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 15263} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15263}, Score: 0.41081176080693943\n",
      "Testing params: {'lr': 0.0001, 'epochs': 8947}\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 8947} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 8947}, Score: 0.41770325635835276\n",
      "Testing params: {'lr': 0.001, 'epochs': 16842}\n",
      "Skipping params: {'lr': 0.001, 'epochs': 16842} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 16842}, Score: 0.40158658154749993\n",
      "Testing params: {'lr': 0.001, 'epochs': 17631}\n",
      "Skipping params: {'lr': 0.001, 'epochs': 17631} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 17631}, Score: 0.40154822284753333\n",
      "Testing params: {'lr': 1e-06, 'epochs': 11315}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 11315} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 11315}, Score: 0.6253265810638988\n",
      "Testing params: {'lr': 0.001, 'epochs': 8947}\n",
      "Skipping params: {'lr': 0.001, 'epochs': 8947} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 8947}, Score: 0.40234645181159456\n",
      "Testing params: {'lr': 1e-07, 'epochs': 19210}\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 19210} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 19210}, Score: 0.9829650164183109\n",
      "Testing params: {'lr': 0.0001, 'epochs': 12894}\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 12894} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 12894}, Score: 0.4126737537058921\n",
      "Testing params: {'lr': 1e-05, 'epochs': 9736}\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 9736} due to high score.\n",
      "Tested params: {'lr': 1e-05, 'epochs': 9736}, Score: 0.4872324610983023\n",
      "Testing params: {'lr': 1e-06, 'epochs': 13684}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 13684} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 13684}, Score: 0.6067590202320609\n",
      "Testing params: {'lr': 1e-07, 'epochs': 10526}\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 10526} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 10526}, Score: 1.1272579666370333\n",
      "Testing params: {'lr': 0.001, 'epochs': 12894}\n",
      "Skipping params: {'lr': 0.001, 'epochs': 12894} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 12894}, Score: 0.4018505797718201\n",
      "Testing params: {'lr': 1e-07, 'epochs': 14473}\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 14473} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 14473}, Score: 1.0567500404247034\n",
      "Testing params: {'lr': 0.001, 'epochs': 9736}\n",
      "Skipping params: {'lr': 0.001, 'epochs': 9736} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 9736}, Score: 0.40221550747909174\n",
      "Testing params: {'lr': 1e-06, 'epochs': 14473}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 14473} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 14473}, Score: 0.6017026091225701\n",
      "Best parameters: {'lr': 0.001, 'epochs': 17631}, Best score: 0.40154822284753333\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.001,0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.linspace(5000, 20000, num=20).astype(int).tolist()\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_linear = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "        train_set = data_processor.encode_nominal_features(train_set)\n",
    "\n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-4]\n",
    "        y_train = train_data[:,-4:]\n",
    "\n",
    "        linear = LinearNetwork(config)\n",
    "\n",
    "        _, val_losses = linear.logistic_regression(X_train,y_train,X_val,y_val,epochs=params['epochs'],lr=params['lr'],patience=np.inf)\n",
    "\n",
    "        score = val_losses[-1]\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 0.2:\n",
    "            print(f\"Skipping params: {params} due to high score.\")\n",
    "            break  # Exit the current for-loop\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_linear = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_linear}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'lr': 1e-05, 'epochs': 9000, 'n_hidden': 6}\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 9000, 'n_hidden': 6} due to high score.\n",
      "Tested params: {'lr': 1e-05, 'epochs': 9000, 'n_hidden': 6}, Score: 0.8678054958021445\n",
      "Testing params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden': 51}\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden': 51} due to high score.\n",
      "Tested params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden': 51}, Score: 0.45562835589304573\n",
      "Testing params: {'lr': 0.0001, 'epochs': 15000, 'n_hidden': 51}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_hidden': 51}, Score: 0.09680569367472854\n",
      "Testing params: {'lr': 0.0001, 'epochs': 5000, 'n_hidden': 51}\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 5000, 'n_hidden': 51} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 5000, 'n_hidden': 51}, Score: 0.232982181600254\n",
      "Testing params: {'lr': 1e-06, 'epochs': 9000, 'n_hidden': 51}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 9000, 'n_hidden': 51} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 9000, 'n_hidden': 51}, Score: 0.9074982027233979\n",
      "Testing params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden': 36}\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden': 36} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden': 36}, Score: 1.2965034670679596\n",
      "Testing params: {'lr': 1e-07, 'epochs': 7000, 'n_hidden': 51}\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 7000, 'n_hidden': 51} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 7000, 'n_hidden': 51}, Score: 1.2650219761274304\n",
      "Testing params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden': 21}\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden': 21} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden': 21}, Score: 0.16296211988959672\n",
      "Testing params: {'lr': 1e-06, 'epochs': 19000, 'n_hidden': 36}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 19000, 'n_hidden': 36} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 19000, 'n_hidden': 36}, Score: 0.8775258075208221\n",
      "Testing params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden': 21}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden': 21}, Score: 0.108476128590865\n",
      "Testing params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden': 51}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden': 51}, Score: 0.0982083564991221\n",
      "Testing params: {'lr': 1e-06, 'epochs': 7000, 'n_hidden': 51}\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 7000, 'n_hidden': 51} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 7000, 'n_hidden': 51}, Score: 0.9267235624471896\n",
      "Testing params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden': 21}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden': 21}, Score: 0.10865140052925884\n",
      "Testing params: {'lr': 1e-07, 'epochs': 13000, 'n_hidden': 51}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(5000, 20000, 2000).tolist(),\n",
    "    'n_hidden': np.arange(X_val.shape[1], 60, 15)\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_ffn = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "        train_set = data_processor.encode_nominal_features(train_set)\n",
    "\n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-4]\n",
    "        y_train = train_data[:,-4:]\n",
    "\n",
    "        ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=params['n_hidden'],n_hidden_2=params['n_hidden'],n_output=y_train.shape[1])\n",
    "\n",
    "        _, val_losses, _ = ffn.train(X_train,y_train,X_val,y_val,epochs=params['epochs'],lr=params['lr'],patience=500)\n",
    "\n",
    "        score = val_losses[-1]\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 0.2:\n",
    "            print(f\"Skipping params: {params} due to high score.\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_ffn = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_ffn}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping params: {'lr': 1e-05, 'epochs': 19000, 'n_encoder': 2} due to high score: 0.5119282422554161\n",
      "Tested params: {'lr': 1e-05, 'epochs': 19000, 'n_encoder': 2}, Score: 0.5119282422554161\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 15000, 'n_encoder': 2} due to high score: 0.775706004102815\n",
      "Tested params: {'lr': 1e-07, 'epochs': 15000, 'n_encoder': 2}, Score: 0.775706004102815\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 7000, 'n_encoder': 3} due to high score: 0.4089134955182551\n",
      "Tested params: {'lr': 1e-05, 'epochs': 7000, 'n_encoder': 3}, Score: 0.4089134955182551\n",
      "Tested params: {'lr': 0.0001, 'epochs': 5000, 'n_encoder': 5}, Score: 0.14052794815010286\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 11000, 'n_encoder': 5} due to high score: 0.7739698980470101\n",
      "Tested params: {'lr': 1e-06, 'epochs': 11000, 'n_encoder': 5}, Score: 0.7739698980470101\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 1000, 'n_encoder': 2} due to high score: 0.5266316653649191\n",
      "Tested params: {'lr': 0.0001, 'epochs': 1000, 'n_encoder': 2}, Score: 0.5266316653649191\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 17000, 'n_encoder': 3} due to high score: 0.37805733829112914\n",
      "Tested params: {'lr': 0.0001, 'epochs': 17000, 'n_encoder': 3}, Score: 0.37805733829112914\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 5000, 'n_encoder': 3} due to high score: 0.7772636644849409\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_encoder': 3}, Score: 0.7772636644849409\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 17000, 'n_encoder': 6} due to high score: 0.7677656557856941\n",
      "Tested params: {'lr': 1e-06, 'epochs': 17000, 'n_encoder': 6}, Score: 0.7677656557856941\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 11000, 'n_encoder': 2} due to high score: 0.5190275887873937\n",
      "Tested params: {'lr': 1e-05, 'epochs': 11000, 'n_encoder': 2}, Score: 0.5190275887873937\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 9000, 'n_encoder': 4} due to high score: 0.2828258357069964\n",
      "Tested params: {'lr': 1e-05, 'epochs': 9000, 'n_encoder': 4}, Score: 0.2828258357069964\n",
      "Tested params: {'lr': 1e-05, 'epochs': 17000, 'n_encoder': 5}, Score: 0.15142127399831914\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 11000, 'n_encoder': 2} due to high score: 0.7739012583023561\n",
      "Tested params: {'lr': 1e-06, 'epochs': 11000, 'n_encoder': 2}, Score: 0.7739012583023561\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 19000, 'n_encoder': 3} due to high score: 0.775465941730448\n",
      "Tested params: {'lr': 1e-07, 'epochs': 19000, 'n_encoder': 3}, Score: 0.775465941730448\n",
      "Tested params: {'lr': 0.0001, 'epochs': 13000, 'n_encoder': 5}, Score: 0.13543387025726694\n",
      "Best parameters: {'lr': 0.0001, 'epochs': 13000, 'n_encoder': 5}, Best score: 0.13543387025726694\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.001,0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(5000, 20000, 2000).tolist(),\n",
    "    'n_encoder': np.arange(2,X_val.shape[1]-1,1).tolist()\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_auto = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "        train_set = data_processor.encode_nominal_features(train_set)\n",
    "\n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-4]\n",
    "        y_train = train_data[:,-4:]\n",
    "\n",
    "        autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=params['n_encoder'])\n",
    "\n",
    "        losses = autoE.train(X_train, max_epochs=params['epochs'], lr=params['lr'])\n",
    "\n",
    "        score = losses[-1]\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 0.2:\n",
    "            print(f\"Skipping params: {params} due to high score.\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_auto = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_auto}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13856338687548111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'])\n",
    "losses = autoE.train(X_train, max_epochs=best_params_auto['epochs'], lr=best_params_auto['lr'])\n",
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping params: {'lr': 1e-05, 'epochs': 5000, 'n_hidden_2': 2} due to high score: 0.21302964724649376\n",
      "Tested params: {'lr': 1e-05, 'epochs': 5000, 'n_hidden_2': 2}, Score: 0.21302964724649376\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 3000, 'n_hidden_2': 6} due to high score: 0.21100967540221627\n",
      "Tested params: {'lr': 0.0001, 'epochs': 3000, 'n_hidden_2': 6}, Score: 0.21100967540221627\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 19000, 'n_hidden_2': 5} due to high score: 0.2143173905786451\n",
      "Tested params: {'lr': 1e-05, 'epochs': 19000, 'n_hidden_2': 5}, Score: 0.2143173905786451\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 15000, 'n_hidden_2': 4} due to high score: 0.21099434508970913\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_hidden_2': 4}, Score: 0.21099434508970913\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 17000, 'n_hidden_2': 2} due to high score: 0.34852841662211764\n",
      "Tested params: {'lr': 1e-07, 'epochs': 17000, 'n_hidden_2': 2}, Score: 0.34852841662211764\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 3} due to high score: 0.4419344872532241\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 3}, Score: 0.4419344872532241\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 7000, 'n_hidden_2': 5} due to high score: 0.21442186953526793\n",
      "Tested params: {'lr': 1e-05, 'epochs': 7000, 'n_hidden_2': 5}, Score: 0.21442186953526793\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 5000, 'n_hidden_2': 4} due to high score: 0.25652563665773354\n",
      "Tested params: {'lr': 1e-06, 'epochs': 5000, 'n_hidden_2': 4}, Score: 0.25652563665773354\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 3} due to high score: 0.21099399702982347\n",
      "Tested params: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 3}, Score: 0.21099399702982347\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 1000, 'n_hidden_2': 4} due to high score: 0.4906626701298433\n",
      "Tested params: {'lr': 1e-07, 'epochs': 1000, 'n_hidden_2': 4}, Score: 0.4906626701298433\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 2} due to high score: 0.21099320529834226\n",
      "Tested params: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 2}, Score: 0.21099320529834226\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 11000, 'n_hidden_2': 2} due to high score: 0.22149615375190382\n",
      "Tested params: {'lr': 1e-06, 'epochs': 11000, 'n_hidden_2': 2}, Score: 0.22149615375190382\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 17000, 'n_hidden_2': 5} due to high score: 0.21401104936057513\n",
      "Tested params: {'lr': 1e-06, 'epochs': 17000, 'n_hidden_2': 5}, Score: 0.21401104936057513\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden_2': 3} due to high score: 0.21393718647242094\n",
      "Tested params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden_2': 3}, Score: 0.21393718647242094\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 11000, 'n_hidden_2': 5} due to high score: 0.21345766141781855\n",
      "Tested params: {'lr': 1e-05, 'epochs': 11000, 'n_hidden_2': 5}, Score: 0.21345766141781855\n",
      "Best parameters: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 2}, Best score: 0.21099320529834226\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.001,0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(5000, 20000, 2000).tolist(),\n",
    "    'n_hidden_2': np.arange(X_val.shape[1], 60, 15)\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_combined = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "        train_set = data_processor.encode_nominal_features(train_set)\n",
    "\n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-4]\n",
    "        y_train = train_data[:,-4:]\n",
    "\n",
    "        combined = CombinedModel(autoE,n_hidden_2=params['n_hidden_2'],n_output=y_val.shape[1])\n",
    "\n",
    "        _, val_losses, _ = combined.train(X_train,y_train,X_val,y_val,epochs=params['epochs'], lr=params['lr'],patience=500)\n",
    "\n",
    "\n",
    "        score = val_losses[-1]\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 0.2:\n",
    "            print(f\"Skipping params: {params} due to high score: {score}\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_combined = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_combined}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Tested params: {'lr': 1e-05, 'epochs': 16000}, Average Score: 0.04989911176833896\n",
      "FFN Model Tested params: {'lr': 0.0001, 'epochs': 9000, 'n_hidden': 13}, Average Score: 0.03156401749087871\n",
      "Combined Model Tested params: {'lr': 0.0001, 'epochs': 17000, 'n_hidden_2': 2}, Average Score: 0.18956194648423397\n",
      "Linear Model Scores: [0.0505000841400078, 0.04918110999398384, 0.05035961379445303, 0.04952270884214627, 0.047728709073474665, 0.051374267862787815, 0.051798825129258506, 0.04851693151371093, 0.05040374914517882, 0.049605118188387975]\n",
      "FFN Model Scores: [0.03359497771019102, 0.032634738953724624, 0.025770116157577162, 0.033536566622630326, 0.03010756680708861, 0.03351205924653568, 0.032073554484799295, 0.0302916960663417, 0.033118007017676795, 0.031000891842221913]\n",
      "Combined Model Scores: [0.18992116083816254, 0.189201816211392, 0.18992146398954116, 0.18920090264527847, 0.18992062012505473, 0.1892022910473092, 0.1899204139983393, 0.18920491250373522, 0.18992265491993404, 0.18920322856359292]\n"
     ]
    }
   ],
   "source": [
    "linear_scores = []\n",
    "ffn_scores = []\n",
    "combined_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "\n",
    "    train_set = data_processor.encode_nominal_features(train_set)\n",
    "    test_set = data_processor.encode_nominal_features(test_set)\n",
    "\n",
    "    train_data = train_set.to_numpy()\n",
    "    X_train = train_data[:,:-4]\n",
    "    y_train = train_data[:,-4:]\n",
    "\n",
    "    test_data = test_set.to_numpy()\n",
    "    X_test = test_data[:,:-4]\n",
    "    y_test = test_data[:,-4:]\n",
    "\n",
    "    linear = LinearNetwork(config)\n",
    "    _, linear_val_losses = linear.logistic_regression(X_train,y_train,X_test,y_test,epochs=best_params_linear['epochs'],lr=best_params_linear['lr'],patience=np.inf)\n",
    "\n",
    "    ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=best_params_ffn['n_hidden'],n_hidden_2=best_params_ffn['n_hidden'],n_output=y_train.shape[1])\n",
    "    _, ffn_val_losses, _ = ffn.train(X_train,y_train,X_test,y_test,epochs=best_params_ffn['epochs'],lr=best_params_ffn['lr'],patience=np.inf)\n",
    "\n",
    "    autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'])\n",
    "    losses = autoE.train(X_train, max_epochs=best_params_auto['epochs'], lr=best_params_auto['lr'])\n",
    "    combined = CombinedModel(autoE,n_hidden_2=best_params_combined['n_hidden_2'],n_output=y_test.shape[1])\n",
    "    _, combined_val_losses, _ = combined.train(X_train,y_train,X_test,y_test,epochs=best_params_combined['epochs'], lr=best_params_combined['lr'],patience=np.inf)\n",
    "\n",
    "\n",
    "    linear_score = np.min(linear_val_losses)\n",
    "    ffn_score = np.min(ffn_val_losses)\n",
    "    combined_score = np.min(combined_val_losses)\n",
    "    \n",
    "    linear_scores.append(linear_score)\n",
    "    ffn_scores.append(ffn_score)\n",
    "    combined_scores.append(combined_score)\n",
    "\n",
    "avg_score_linear = np.mean(linear_scores)\n",
    "avg_score_ffn = np.mean(ffn_scores)\n",
    "avg_score_combined = np.mean(combined_scores)\n",
    "\n",
    "print(f\"Linear Model Tested params: {best_params_linear}, Average Score: {avg_score_linear}\")\n",
    "print(f\"FFN Model Tested params: {best_params_ffn}, Average Score: {avg_score_ffn}\")\n",
    "print(f\"Combined Model Tested params: {best_params_combined}, Average Score: {avg_score_combined}\")\n",
    "\n",
    "print(f\"Linear Model Scores: {linear_scores}\")\n",
    "print(f\"FFN Model Scores: {ffn_scores}\")\n",
    "print(f\"Combined Model Scores: {combined_scores}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model vs. FFN Model: t-statistic = 21.213127344778, p-value = 3.4769130932319286e-14\n",
      "Linear Model vs. Combined Model: t-statistic = -339.07928011744707, p-value = 1.0464626991282104e-35\n",
      "FFN Model vs. Combined Model: t-statistic = -202.94519565836433, p-value = 1.0746998437377001e-31\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(linear_scores, ffn_scores)\n",
    "print(f\"Linear Model vs. FFN Model: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Comparing Linear Model vs. Combined Model\n",
    "t_stat, p_val = stats.ttest_ind(linear_scores, combined_scores)\n",
    "print(f\"Linear Model vs. Combined Model: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Comparing FFN Model vs. Combined Model\n",
    "t_stat, p_val = stats.ttest_ind(ffn_scores, combined_scores)\n",
    "print(f\"FFN Model vs. Combined Model: t-statistic = {t_stat}, p-value = {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = data_processor.encode_nominal_features(data_train)\n",
    "# # data_val = data_processor.encode_nominal_features(data_val)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_train.to_numpy()\n",
    "# X_train = data[:,:-4]\n",
    "# y_train = data[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = data_val.to_numpy()\n",
    "# X_val = data_test[:,:-4]\n",
    "# y_val = data_test[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1382, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9944572214658505,\n",
       " 0.9944211142740377,\n",
       " 0.9944010212794319,\n",
       " 0.9943892307272889,\n",
       " 0.9943817251075924,\n",
       " 0.9943764093822699,\n",
       " 0.9943721889208345,\n",
       " 0.9943684896224162,\n",
       " 0.9943650081583324,\n",
       " 0.9943615819749549,\n",
       " 0.9943581216231622,\n",
       " 0.9943545755242066,\n",
       " 0.9943509116161269,\n",
       " 0.9943471077848369,\n",
       " 0.9943431468664367,\n",
       " 0.9943390140278618,\n",
       " 0.9943346953845406,\n",
       " 0.9943301772610029,\n",
       " 0.994325445785209,\n",
       " 0.9943204866555801,\n",
       " 0.994315284996864,\n",
       " 0.9943098252611032,\n",
       " 0.9943040911508638,\n",
       " 0.994298065552749,\n",
       " 0.9942917304748742,\n",
       " 0.9942850669849118,\n",
       " 0.9942780551468415,\n",
       " 0.9942706739553282,\n",
       " 0.9942629012670494,\n",
       " 0.9942547137285048,\n",
       " 0.9942460866999336,\n",
       " 0.9942369941750139,\n",
       " 0.9942274086960318,\n",
       " 0.9942173012642117,\n",
       " 0.9942066412448854,\n",
       " 0.9941953962671677,\n",
       " 0.9941835321177918,\n",
       " 0.9941710126287319,\n",
       " 0.9941577995582284,\n",
       " 0.994143852464801,\n",
       " 0.9941291285738212,\n",
       " 0.9941135826361754,\n",
       " 0.9940971667785446,\n",
       " 0.9940798303447775,\n",
       " 0.9940615197278191,\n",
       " 0.9940421781916188,\n",
       " 0.9940217456824118,\n",
       " 0.9940001586287326,\n",
       " 0.9939773497294817,\n",
       " 0.9939532477293331,\n",
       " 0.9939277771807222,\n",
       " 0.9939008581916244,\n",
       " 0.993872406158273,\n",
       " 0.9938423314819393,\n",
       " 0.9938105392688379,\n",
       " 0.9937769290121687,\n",
       " 0.9937413942552646,\n",
       " 0.9937038222347551,\n",
       " 0.9936640935025983,\n",
       " 0.9936220815257818,\n",
       " 0.9935776522624316,\n",
       " 0.9935306637130027,\n",
       " 0.9934809654451742,\n",
       " 0.9934283980909977,\n",
       " 0.9933727928147891,\n",
       " 0.9933139707501888,\n",
       " 0.9932517424047494,\n",
       " 0.9931859070303473,\n",
       " 0.9931162519576511,\n",
       " 0.9930425518928134,\n",
       " 0.9929645681745016,\n",
       " 0.9928820479893209,\n",
       " 0.9927947235436292,\n",
       " 0.9927023111897079,\n",
       " 0.9926045105042006,\n",
       " 0.9925010033167118,\n",
       " 0.9923914526864288,\n",
       " 0.9922755018246355,\n",
       " 0.9921527729609765,\n",
       " 0.9920228661513767,\n",
       " 0.9918853580255492,\n",
       " 0.9917398004721044,\n",
       " 0.9915857192593743,\n",
       " 0.9914226125901852,\n",
       " 0.9912499495889943,\n",
       " 0.991067168720001,\n",
       " 0.9908736761351159,\n",
       " 0.9906688439509688,\n",
       " 0.9904520084545241,\n",
       " 0.9902224682373022,\n",
       " 0.9899794822587349,\n",
       " 0.9897222678397833,\n",
       " 0.9894499985886488,\n",
       " 0.9891618022612155,\n",
       " 0.9888567585597837,\n",
       " 0.98853389687471,\n",
       " 0.9881921939747483,\n",
       " 0.9878305716532383,\n",
       " 0.9874478943387817,\n",
       " 0.987042966680725,\n",
       " 0.9866145311216347,\n",
       " 0.9861612654709947,\n",
       " 0.9856817804966304,\n",
       " 0.9851746175528244,\n",
       " 0.9846382462667811,\n",
       " 0.9840710623079967,\n",
       " 0.9834713852682068,\n",
       " 0.9828374566829046,\n",
       " 0.9821674382289329,\n",
       " 0.9814594101363266,\n",
       " 0.9807113698564025,\n",
       " 0.9799212310319975,\n",
       " 0.9790868228197174,\n",
       " 0.9782058896179816,\n",
       " 0.9772760912584835,\n",
       " 0.9762950037223146,\n",
       " 0.9752601204453286,\n",
       " 0.97416885428019,\n",
       " 0.9730185401848647,\n",
       " 0.9718064387088438,\n",
       " 0.9705297403490197,\n",
       " 0.9691855708466229,\n",
       " 0.9677709974948081,\n",
       " 0.9662830365231122,\n",
       " 0.964718661619912,\n",
       " 0.9630748136469607,\n",
       " 0.961348411590923,\n",
       " 0.9595363647853742,\n",
       " 0.9576355864228832,\n",
       " 0.9556430083604902,\n",
       " 0.9535555972031002,\n",
       " 0.9513703716281456,\n",
       " 0.9490844208914719,\n",
       " 0.9466949244290263,\n",
       " 0.9441991724419859,\n",
       " 0.9415945873248933,\n",
       " 0.9388787457678023,\n",
       " 0.936049401335061,\n",
       " 0.933104507295964,\n",
       " 0.9300422394569767,\n",
       " 0.9268610187224603,\n",
       " 0.923559533091767,\n",
       " 0.920136758786115,\n",
       " 0.9165919801896506,\n",
       " 0.9129248082862931,\n",
       " 0.9091351972779339,\n",
       " 0.9052234590807108,\n",
       " 0.9011902754145961,\n",
       " 0.8970367072273451,\n",
       " 0.8927642012266243,\n",
       " 0.8883745933332838,\n",
       " 0.8838701089134314,\n",
       " 0.87925335969611,\n",
       " 0.8745273373357676,\n",
       " 0.8696954036329226,\n",
       " 0.864761277481008,\n",
       " 0.8597290186608504,\n",
       " 0.8546030086551316,\n",
       " 0.84938792870219,\n",
       " 0.8440887353504092,\n",
       " 0.8387106338102684,\n",
       " 0.8332590494301219,\n",
       " 0.8277395976434091,\n",
       " 0.8221580527490883,\n",
       " 0.8165203158935772,\n",
       " 0.8108323826216894,\n",
       " 0.8051003103563881,\n",
       " 0.7993301861532993,\n",
       " 0.7935280950566003,\n",
       " 0.7877000893589827,\n",
       " 0.7818521590408163,\n",
       " 0.7759902036333147,\n",
       " 0.7701200057183779,\n",
       " 0.7642472062446622,\n",
       " 0.7583772818061539,\n",
       " 0.7525155239967355,\n",
       " 0.7466670209225728,\n",
       " 0.7408366409240719,\n",
       " 0.7350290185310655,\n",
       " 0.7292485426490543,\n",
       " 0.7234993469509425,\n",
       " 0.7177853024278952,\n",
       " 0.7121100120346953,\n",
       " 0.7064768073493277,\n",
       " 0.7008887471533293,\n",
       " 0.695348617828651,\n",
       " 0.6898589354582085,\n",
       " 0.6844219495108147,\n",
       " 0.679039647986589,\n",
       " 0.673713763896076,\n",
       " 0.6684457829449755,\n",
       " 0.6632369522964222,\n",
       " 0.6580882902839721,\n",
       " 0.6530005969507126,\n",
       " 0.6479744652930243,\n",
       " 0.6430102930913782,\n",
       " 0.638108295214984,\n",
       " 0.6332685162920184,\n",
       " 0.6284908436424248,\n",
       " 0.623775020375802,\n",
       " 0.6191206585625965,\n",
       " 0.6145272523925897,\n",
       " 0.6099941912404957,\n",
       " 0.6055207725642545,\n",
       " 0.6011062145673135,\n",
       " 0.5967496685617804,\n",
       " 0.5924502309747687,\n",
       " 0.5882069549455519,\n",
       " 0.5840188614662406,\n",
       " 0.579884950023638,\n",
       " 0.5758042087046686,\n",
       " 0.5717756237323619,\n",
       " 0.5677981884037785,\n",
       " 0.5638709114055325,\n",
       " 0.5599928244866857,\n",
       " 0.556162989472794,\n",
       " 0.5523805046087831,\n",
       " 0.5486445102221432,\n",
       " 0.5449541937016588,\n",
       " 0.5413087937905593,\n",
       " 0.5377076041965836,\n",
       " 0.5341499765250083,\n",
       " 0.5306353225441894,\n",
       " 0.5271631157966196,\n",
       " 0.5237328925718758,\n",
       " 0.5203442522611482,\n",
       " 0.5169968571162425,\n",
       " 0.5136904314390565,\n",
       " 0.5104247602304831,\n",
       " 0.5071996873305018,\n",
       " 0.5040151130838203,\n",
       " 0.5008709915678224,\n",
       " 0.4977673274217179,\n",
       " 0.49470417231765296,\n",
       " 0.4916816211161014,\n",
       " 0.48869980774908445,\n",
       " 0.48575890087565254,\n",
       " 0.4828590993545731,\n",
       " 0.480000627579304,\n",
       " 0.4771837307200793,\n",
       " 0.47440866991729264,\n",
       " 0.4716757174693321,\n",
       " 0.4689851520566214,\n",
       " 0.46633725404185716,\n",
       " 0.4637323008843282,\n",
       " 0.4611705627037925,\n",
       " 0.45865229802668805,\n",
       " 0.45617774974451725,\n",
       " 0.45374714131109645,\n",
       " 0.45136067320205203,\n",
       " 0.44901851965651407,\n",
       " 0.4467208257174493,\n",
       " 0.4444677045835414,\n",
       " 0.44225923528199795,\n",
       " 0.44009546066819744,\n",
       " 0.437976385754718,\n",
       " 0.43590197636905104,\n",
       " 0.4338721581362377,\n",
       " 0.431886815779791,\n",
       " 0.42994579273162875,\n",
       " 0.42804889103933685,\n",
       " 0.4261958715569534,\n",
       " 0.42438645440359435,\n",
       " 0.4226203196726734,\n",
       " 0.4208971083731597,\n",
       " 0.41921642358331457,\n",
       " 0.4175778317966052,\n",
       " 0.4159808644390236,\n",
       " 0.4144250195368256,\n",
       " 0.412909763513722,\n",
       " 0.41143453309680045,\n",
       " 0.40999873731089725,\n",
       " 0.4086017595417639,\n",
       " 0.40724295964914975,\n",
       " 0.4059216761118452,\n",
       " 0.40463722818775355,\n",
       " 0.4033889180731837,\n",
       " 0.4021760330467464,\n",
       " 0.40099784758447526,\n",
       " 0.399853625434067,\n",
       " 0.39874262163741414,\n",
       " 0.3976640844918875,\n",
       " 0.3966172574420817,\n",
       " 0.3956013808949695,\n",
       " 0.39461569395259655,\n",
       " 0.3936594360575834,\n",
       " 0.39273184854777704,\n",
       " 0.39183217611740384,\n",
       " 0.39095966818301464,\n",
       " 0.3901135801533791,\n",
       " 0.3892931746032756,\n",
       " 0.3884977223518355,\n",
       " 0.3877265034467416,\n",
       " 0.3869788080561396,\n",
       " 0.38625393727061386,\n",
       " 0.38555120381799973,\n",
       " 0.3848699326941556,\n",
       " 0.3842094617131114,\n",
       " 0.38356914198024133,\n",
       " 0.3829483382922832,\n",
       " 0.3823464294681578,\n",
       " 0.38176280861461703,\n",
       " 0.38119688333079604,\n",
       " 0.38064807585573807,\n",
       " 0.38011582316293885,\n",
       " 0.37959957700589164,\n",
       " 0.3790988039185348,\n",
       " 0.3786129851743946,\n",
       " 0.3781416167080964,\n",
       " 0.3776842090027792,\n",
       " 0.3772402869468003,\n",
       " 0.37680938966295735,\n",
       " 0.3763910703132973,\n",
       " 0.3759848958824056,\n",
       " 0.3755904469419082,\n",
       " 0.3752073173987427,\n",
       " 0.37483511422958754,\n",
       " 0.3744734572036735,\n",
       " 0.37412197859603596,\n",
       " 0.3737803228931093,\n",
       " 0.3734481464924104,\n",
       " 0.37312511739791293,\n",
       " 0.3728109149125683,\n",
       " 0.3725052293292996,\n",
       " 0.37220776162166225,\n",
       " 0.3719182231352489,\n",
       " 0.3716363352807969,\n",
       " 0.3713618292298548,\n",
       " 0.3710944456137629,\n",
       " 0.37083393422660815,\n",
       " 0.3705800537327303,\n",
       " 0.37033257137927517,\n",
       " 0.37009126271421744,\n",
       " 0.36985591131020745,\n",
       " 0.36962630849453737,\n",
       " 0.369402253085462,\n",
       " 0.36918355113506024,\n",
       " 0.36897001567877896,\n",
       " 0.3687614664917553,\n",
       " 0.368557729851979,\n",
       " 0.3683586383103218,\n",
       " 0.36816403046743196,\n",
       " 0.36797375075746347,\n",
       " 0.3677876492385906,\n",
       " 0.3676055813902327,\n",
       " 0.3674274079169027,\n",
       " 0.36725299455857163,\n",
       " 0.3670822119074341,\n",
       " 0.36691493523094487,\n",
       " 0.3667510443009905,\n",
       " 0.36659042322905055,\n",
       " 0.366432960307199,\n",
       " 0.36627854785479097,\n",
       " 0.36612708207067723,\n",
       " 0.36597846289078656,\n",
       " 0.36583259385091493,\n",
       " 0.3656893819545601,\n",
       " 0.36554873754564204,\n",
       " 0.365410574185947,\n",
       " 0.3652748085371404,\n",
       " 0.3651413602471886,\n",
       " 0.3650101518410406,\n",
       " 0.3648811086154146,\n",
       " 0.36475415853754667,\n",
       " 0.3646292321477526,\n",
       " 0.3645062624656668,\n",
       " 0.36438518490001875,\n",
       " 0.36426593716181616,\n",
       " 0.3641484591808039,\n",
       " 0.3640326930250761,\n",
       " 0.3639185828237178,\n",
       " 0.36380607469236187,\n",
       " 0.3636951166615462,\n",
       " 0.3635856586077636,\n",
       " 0.3634776521870975,\n",
       " 0.36337105077134474,\n",
       " 0.36326580938652464,\n",
       " 0.3631618846536839,\n",
       " 0.36305923473190355,\n",
       " 0.36295781926342535,\n",
       " 0.3628575993208088,\n",
       " 0.3627585373560448,\n",
       " 0.36266059715154364,\n",
       " 0.36256374377292755,\n",
       " 0.3624679435235541,\n",
       " 0.36237316390070495,\n",
       " 0.36227937355337275,\n",
       " 0.3621865422415854,\n",
       " 0.36209464079720627,\n",
       " 0.36200364108615496,\n",
       " 0.36191351597199106,\n",
       " 0.3618242392808111,\n",
       " 0.36173578576740534,\n",
       " 0.3616481310826277,\n",
       " 0.36156125174193293,\n",
       " 0.36147512509503393,\n",
       " 0.3613897292966399,\n",
       " 0.3613050432782319,\n",
       " 0.36122104672083805,\n",
       " 0.3611377200287706,\n",
       " 0.3610550443042878,\n",
       " 0.36097300132314836,\n",
       " 0.36089157351102263,\n",
       " 0.3608107439207307,\n",
       " 0.360730496210277,\n",
       " 0.3606508146216503,\n",
       " 0.3605716839603633,\n",
       " 0.36049308957570375,\n",
       " 0.36041501734167114,\n",
       " 0.36033745363857483,\n",
       " 0.36026038533526966,\n",
       " 0.3601837997720053,\n",
       " 0.3601076847438682,\n",
       " 0.3600320284847957,\n",
       " 0.3599568196521402,\n",
       " 0.3598820473117649,\n",
       " 0.35980770092365294,\n",
       " 0.35973377032801074,\n",
       " 0.3596602457318486,\n",
       " 0.3595871176960225,\n",
       " 0.3595143771227201,\n",
       " 0.35944201524337593,\n",
       " 0.3593700236070008,\n",
       " 0.35929839406891173,\n",
       " 0.35922711877984664,\n",
       " 0.3591561901754538,\n",
       " 0.35908560096613923,\n",
       " 0.3590153441272637,\n",
       " 0.35894541288967413,\n",
       " 0.35887580073056,\n",
       " 0.3588065013646231,\n",
       " 0.35873750873554927,\n",
       " 0.3586688170077731,\n",
       " 0.35860042055852515,\n",
       " 0.35853231397015134,\n",
       " 0.35846449202269665,\n",
       " 0.35839694968674324,\n",
       " 0.3583296821164949,\n",
       " 0.35826268464309907,\n",
       " 0.358195952768199,\n",
       " 0.35812948215770785,\n",
       " 0.3580632686357974,\n",
       " 0.35799730817909436,\n",
       " 0.3579315969110771,\n",
       " 0.3578661310966661,\n",
       " 0.3578009071370014,\n",
       " 0.3577359215644016,\n",
       " 0.3576711710374973,\n",
       " 0.3576066523365334,\n",
       " 0.35754236235883563,\n",
       " 0.35747829811443427,\n",
       " 0.3574144567218404,\n",
       " 0.35735083540397045,\n",
       " 0.3572874314842124,\n",
       " 0.3572242423826301,\n",
       " 0.3571612656123016,\n",
       " 0.35709849877578453,\n",
       " 0.3570359395617073,\n",
       " 0.35697358574147964,\n",
       " 0.35691143516612023,\n",
       " 0.3568494857631954,\n",
       " 0.3567877355338675,\n",
       " 0.3567261825500473,\n",
       " 0.35666482495164814,\n",
       " 0.3566036609439382,\n",
       " 0.3565426887949864,\n",
       " 0.3564819068332011,\n",
       " 0.35642131344495603,\n",
       " 0.35636090707230206,\n",
       " 0.35630068621076166,\n",
       " 0.3562406494072023,\n",
       " 0.3561807952577874,\n",
       " 0.35612112240600186,\n",
       " 0.35606162954074855,\n",
       " 0.35600231539451516,\n",
       " 0.35594317874160736,\n",
       " 0.3558842183964471,\n",
       " 0.3558254332119333,\n",
       " 0.35576682207786303,\n",
       " 0.35570838391941084,\n",
       " 0.35565011769566474,\n",
       " 0.3555920223982161,\n",
       " 0.3555340970498027,\n",
       " 0.35547634070300194,\n",
       " 0.35541875243897303,\n",
       " 0.3553613313662473,\n",
       " 0.35530407661956287,\n",
       " 0.3552469873587443,\n",
       " 0.35519006276762466,\n",
       " 0.355133302053008,\n",
       " 0.35507670444367273,\n",
       " 0.35502026918941165,\n",
       " 0.3549639955601101,\n",
       " 0.3549078828448593,\n",
       " 0.3548519303511038,\n",
       " 0.3547961374038229,\n",
       " 0.3547405033447427,\n",
       " 0.3546850275315805,\n",
       " 0.35462970933731797,\n",
       " 0.35457454814950334,\n",
       " 0.35451954336958114,\n",
       " 0.35446469441224904,\n",
       " 0.35441000070484013,\n",
       " 0.3543554616867298,\n",
       " 0.3543010768087671,\n",
       " 0.35424684553272867,\n",
       " 0.35419276733079563,\n",
       " 0.35413884168505083,\n",
       " 0.35408506808699747,\n",
       " 0.35403144603709824,\n",
       " 0.353977975044332,\n",
       " 0.3539246546257705,\n",
       " 0.35387148430617216,\n",
       " 0.3538184636175932,\n",
       " 0.35376559209901537,\n",
       " 0.3537128692959891,\n",
       " 0.3536602947602928,\n",
       " 0.3536078680496066,\n",
       " 0.3535555887271997,\n",
       " 0.35350345636163244,\n",
       " 0.3534514705264707,\n",
       " 0.3533996308000134,\n",
       " 0.3533479367650321,\n",
       " 0.3532963880085222,\n",
       " 0.3532449841214659,\n",
       " 0.35319372469860494,\n",
       " 0.3531426093382255,\n",
       " 0.3530916376419514,\n",
       " 0.353040809214548,\n",
       " 0.3529901236637351,\n",
       " 0.3529395806000087,\n",
       " 0.3528891796364715,\n",
       " 0.3528389203886714,\n",
       " 0.3527888024744482,\n",
       " 0.3527388255137876,\n",
       " 0.35268898912868296,\n",
       " 0.3526392929430033,\n",
       " 0.35258973658236903,\n",
       " 0.3525403196740331,\n",
       " 0.35249104184676944,\n",
       " 0.3524419027307665,\n",
       " 0.3523929019575265,\n",
       " 0.3523440391597708,\n",
       " 0.35229531397135033,\n",
       " 0.3522467260271601,\n",
       " 0.35219827496306033,\n",
       " 0.3521499604158003,\n",
       " 0.35210178202294806,\n",
       " 0.3520537394228234,\n",
       " 0.3520058322544357,\n",
       " 0.35195806015742503,\n",
       " 0.3519104227720073,\n",
       " 0.3518629197389226,\n",
       " 0.3518155506993877,\n",
       " 0.3517683152950508,\n",
       " 0.3517212131679504,\n",
       " 0.35167424396047603,\n",
       " 0.351627407315333,\n",
       " 0.35158070287550885,\n",
       " 0.3515341302842431,\n",
       " 0.351487689184999,\n",
       " 0.35144137922143787,\n",
       " 0.3513952000373956,\n",
       " 0.3513491512768613,\n",
       " 0.35130323258395785,\n",
       " 0.3512574436029249,\n",
       " 0.3512117839781028,\n",
       " 0.3511662533539193,\n",
       " 0.351120851374877,\n",
       " 0.351075577685543,\n",
       " 0.35103043193054,\n",
       " 0.3509854137545384,\n",
       " 0.35094052280225013,\n",
       " 0.3508957587184237,\n",
       " 0.3508511211478404,\n",
       " 0.3508066097353116,\n",
       " 0.35076222412567687,\n",
       " 0.35071796396380467,\n",
       " 0.3506738288945909,\n",
       " 0.3506298185629618,\n",
       " 0.3505859326138756,\n",
       " 0.3505421706923253,\n",
       " 0.3504985324433425,\n",
       " 0.3504550175120023,\n",
       " 0.35041162554342786,\n",
       " 0.35036835618279666,\n",
       " 0.35032520907534637,\n",
       " 0.3502821838663818,\n",
       " 0.350239280201283,\n",
       " 0.350196497725512,\n",
       " 0.35015383608462153,\n",
       " 0.35011129492426446,\n",
       " 0.35006887389020164,\n",
       " 0.35002657262831227,\n",
       " 0.34998439078460286,\n",
       " 0.3499423280052179,\n",
       " 0.3499003839364497,\n",
       " 0.349858558224749,\n",
       " 0.34981685051673567,\n",
       " 0.34977526045920987,\n",
       " 0.3497337876991626,\n",
       " 0.3496924318837871,\n",
       " 0.3496511926604908,\n",
       " 0.34961006967690567,\n",
       " 0.3495690625809008,\n",
       " 0.3495281710205933,\n",
       " 0.34948739464436024,\n",
       " 0.34944673310085067,\n",
       " 0.34940618603899687,\n",
       " 0.3493657531080263,\n",
       " 0.3493254339574735,\n",
       " 0.3492852282371921,\n",
       " 0.34924513559736564,\n",
       " 0.3492051556885205,\n",
       " 0.3491652881615366,\n",
       " 0.34912553266765967,\n",
       " 0.34908588885851255,\n",
       " 0.3490463563861067,\n",
       " 0.34900693490285367,\n",
       " 0.3489676240615768,\n",
       " 0.34892842351552167,\n",
       " 0.348889332918368,\n",
       " 0.3488503519242407,\n",
       " 0.34881148018772024,\n",
       " 0.34877271736385423,\n",
       " 0.3487340631081677,\n",
       " 0.34869551707667384,\n",
       " 0.3486570789258846,\n",
       " 0.34861874831282086,\n",
       " 0.34858052489502267,\n",
       " 0.34854240833055977,\n",
       " 0.3485043982780408,\n",
       " 0.3484664943966239,\n",
       " 0.3484286963460258,\n",
       " 0.34839100378653176,\n",
       " 0.34835341637900435,\n",
       " 0.3483159337848936,\n",
       " 0.3482785556662452,\n",
       " 0.34824128168571006,\n",
       " 0.34820411150655256,\n",
       " 0.3481670447926597,\n",
       " 0.3481300812085489,\n",
       " 0.34809322041937724,\n",
       " 0.3480564620909486,\n",
       " 0.3480198058897225,\n",
       " 0.34798325148282133,\n",
       " 0.34794679853803856,\n",
       " 0.3479104467238456,\n",
       " 0.34787419570940004,\n",
       " 0.34783804516455213,\n",
       " 0.34780199475985224,\n",
       " 0.3477660441665576,\n",
       " 0.34773019305663905,\n",
       " 0.347694441102788,\n",
       " 0.3476587879784223,\n",
       " 0.347623233357693,\n",
       " 0.34758777691549037,\n",
       " 0.3475524183274496,\n",
       " 0.34751715726995736,\n",
       " 0.3474819934201569,\n",
       " 0.34744692645595365,\n",
       " 0.34741195605602104,\n",
       " 0.34737708189980543,\n",
       " 0.34734230366753127,\n",
       " 0.34730762104020646,\n",
       " 0.3472730336996266,\n",
       " 0.3472385413283801,\n",
       " 0.347204143609853,\n",
       " 0.3471698402282327,\n",
       " 0.3471356308685132,\n",
       " 0.34710151521649824,\n",
       " 0.3470674929588063,\n",
       " 0.347033563782874,\n",
       " 0.34699972737696033,\n",
       " 0.34696598343014956,\n",
       " 0.34693233163235576,\n",
       " 0.3468987716743257,\n",
       " 0.3468653032476419,\n",
       " 0.3468319260447266,\n",
       " 0.3467986397588443,\n",
       " 0.3467654440841048,\n",
       " 0.3467323387154661,\n",
       " 0.34669932334873726,\n",
       " 0.346666397680581,\n",
       " 0.346633561408516,\n",
       " 0.34660081423091954,\n",
       " 0.34656815584702994,\n",
       " 0.34653558595694856,\n",
       " 0.346503104261642,\n",
       " 0.34647071046294386,\n",
       " 0.3464384042635573,\n",
       " 0.34640618536705636,\n",
       " 0.34637405347788747,\n",
       " 0.34634200830137185,\n",
       " 0.3463100495437065,\n",
       " 0.34627817691196566,\n",
       " 0.34624639011410235,\n",
       " 0.34621468885894974,\n",
       " 0.34618307285622213,\n",
       " 0.346151541816516,\n",
       " 0.34612009545131156,\n",
       " 0.34608873347297286,\n",
       " 0.3460574555947496,\n",
       " 0.3460262615307771,\n",
       " 0.34599515099607764,\n",
       " 0.34596412370656054,\n",
       " 0.3459331793790235,\n",
       " 0.3459023177311521,\n",
       " 0.34587153848152113,\n",
       " 0.34584084134959453,\n",
       " 0.34581022605572553,\n",
       " 0.3457796923211573,\n",
       " 0.34574923986802275,\n",
       " 0.34571886841934485,\n",
       " 0.34568857769903655,\n",
       " 0.3456583674319007,\n",
       " 0.34562823734363013,\n",
       " 0.34559818716080737,\n",
       " 0.3455682166109045,\n",
       " 0.34553832542228285,\n",
       " 0.3455085133241926,\n",
       " 0.3454787800467726,\n",
       " 0.34544912532105,\n",
       " 0.34541954887893944,\n",
       " 0.34539005045324267,\n",
       " 0.34536062977764814,\n",
       " 0.34533128658673007,\n",
       " 0.34530202061594806,\n",
       " 0.34527283160164635,\n",
       " 0.3452437192810526,\n",
       " 0.3452146833922778,\n",
       " 0.345185723674315,\n",
       " 0.34515683986703816,\n",
       " 0.34512803171120165,\n",
       " 0.34509929894843927,\n",
       " 0.34507064132126286,\n",
       " 0.3450420585730614,\n",
       " 0.3450135504481003,\n",
       " 0.34498511669151954,\n",
       " 0.3449567570493331,\n",
       " 0.34492847126842735,\n",
       " 0.34490025909656014,\n",
       " 0.34487212028235925,\n",
       " 0.3448440545753212,\n",
       " 0.34481606172581,\n",
       " 0.3447881414850553,\n",
       " 0.3447602936051519,\n",
       " 0.34473251783905734,\n",
       " 0.34470481394059094,\n",
       " 0.344677181664432,\n",
       " 0.34464962076611905,\n",
       " 0.3446221310020471,\n",
       " 0.3445947121294671,\n",
       " 0.3445673639064835,\n",
       " 0.34454008609205367,\n",
       " 0.34451287844598494,\n",
       " 0.344485740728934,\n",
       " 0.3444586727024047,\n",
       " 0.34443167412874615,\n",
       " 0.34440474477115174,\n",
       " 0.3443778843936564,\n",
       " 0.34435109276113546,\n",
       " 0.3443243696393026,\n",
       " 0.34429771479470805,\n",
       " 0.3442711279947367,\n",
       " 0.34424460900760634,\n",
       " 0.34421815760236557,\n",
       " 0.3441917735488923,\n",
       " 0.3441654566178912,\n",
       " 0.34413920658089237,\n",
       " 0.34411302321024884,\n",
       " 0.34408690627913513,\n",
       " 0.3440608555615447,\n",
       " 0.34403487083228856,\n",
       " 0.34400895186699265,\n",
       " 0.3439830984420962,\n",
       " 0.34395731033484955,\n",
       " 0.343931587323312,\n",
       " 0.34390592918635005,\n",
       " 0.3438803357036347,\n",
       " 0.34385480665564,\n",
       " 0.34382934182364083,\n",
       " 0.34380394098971007,\n",
       " 0.34377860393671766,\n",
       " 0.3437533304483274,\n",
       " 0.3437281203089952,\n",
       " 0.34370297330396715,\n",
       " 0.3436778892192767,\n",
       " 0.3436528678417435,\n",
       " 0.34362790895896994,\n",
       " 0.34360301235934,\n",
       " 0.34357817783201655,\n",
       " 0.343553405166939,\n",
       " 0.3435286941548216,\n",
       " 0.34350404458715084,\n",
       " 0.3434794562561831,\n",
       " 0.3434549289549425,\n",
       " 0.3434304624772191,\n",
       " 0.3434060566175658,\n",
       " 0.3433817111712968,\n",
       " 0.343357425934485,\n",
       " 0.34333320070395984,\n",
       " 0.3433090352773047,\n",
       " 0.3432849294528552,\n",
       " 0.34326088302969626,\n",
       " 0.3432368958076604,\n",
       " 0.34321296758732506,\n",
       " 0.34318909817001014,\n",
       " 0.3431652873577764,\n",
       " 0.34314153495342226,\n",
       " 0.3431178407604823,\n",
       " 0.3430942045832244,\n",
       " 0.3430706262266475,\n",
       " 0.34304710549647943,\n",
       " 0.3430236421991746,\n",
       " 0.34300023614191133,\n",
       " 0.34297688713259017,\n",
       " 0.34295359497983097,\n",
       " 0.3429303594929706,\n",
       " 0.34290718048206104,\n",
       " 0.34288405775786684,\n",
       " 0.34286099113186225,\n",
       " 0.3428379804162298,\n",
       " 0.3428150254238575,\n",
       " 0.3427921259683363,\n",
       " 0.34276928186395805,\n",
       " 0.3427464929257133,\n",
       " 0.3427237589692884,\n",
       " 0.34270107981106385,\n",
       " 0.34267845526811125,\n",
       " 0.3426558851581916,\n",
       " 0.34263336929975274,\n",
       " 0.34261090751192685,\n",
       " 0.34258849961452814,\n",
       " 0.34256614542805075,\n",
       " 0.34254384477366623,\n",
       " 0.34252159747322153,\n",
       " 0.34249940334923595,\n",
       " 0.3424772622248994,\n",
       " 0.34245517392407,\n",
       " 0.3424331382712718,\n",
       " 0.3424111550916922,\n",
       " 0.3423892242111796,\n",
       " 0.3423673454562417,\n",
       " 0.3423455186540423,\n",
       " 0.3423237436323997,\n",
       " 0.3423020202197838,\n",
       " 0.34228034824531467,\n",
       " 0.34225872753875886,\n",
       " 0.3422371579305286,\n",
       " 0.3422156392516786,\n",
       " 0.3421941713339037,\n",
       " 0.3421727540095371,\n",
       " 0.34215138711154786,\n",
       " 0.3421300704735383,\n",
       " 0.34210880392974224,\n",
       " 0.342087587315022,\n",
       " 0.3420664204648671,\n",
       " 0.34204530321539117,\n",
       " 0.34202423540332993,\n",
       " 0.342003216866039,\n",
       " 0.34198224744149164,\n",
       " 0.34196132696827625,\n",
       " 0.34194045528559464,\n",
       " 0.34191963223325894,\n",
       " 0.34189885765169037,\n",
       " 0.3418781313819161,\n",
       " 0.3418574532655676,\n",
       " 0.34183682314487807,\n",
       " 0.3418162408626803,\n",
       " 0.3417957062624045,\n",
       " 0.3417752191880763,\n",
       " 0.3417547794843138,\n",
       " 0.3417343869963262,\n",
       " 0.34171404156991103,\n",
       " 0.3416937430514525,\n",
       " 0.3416734912879183,\n",
       " 0.3416532861268588,\n",
       " 0.34163312741640345,\n",
       " 0.34161301500525976,\n",
       " 0.3415929487427103,\n",
       " 0.3415729284786109,\n",
       " 0.34155295406338865,\n",
       " 0.3415330253480393,\n",
       " 0.3415131421841253,\n",
       " 0.34149330442377374,\n",
       " 0.34147351191967407,\n",
       " 0.34145376452507614,\n",
       " 0.34143406209378774,\n",
       " 0.3414144044801727,\n",
       " 0.3413947915391488,\n",
       " 0.34137522312618557,\n",
       " 0.3413556990973022,\n",
       " 0.34133621930906505,\n",
       " 0.3413167836185865,\n",
       " 0.34129739188352176,\n",
       " 0.34127804396206735,\n",
       " 0.34125873971295917,\n",
       " 0.3412394789954698,\n",
       " 0.3412202616694072,\n",
       " 0.3412010875951119,\n",
       " 0.3411819566334555,\n",
       " 0.34116286864583806,\n",
       " 0.3411438234941867,\n",
       " 0.3411248210409531,\n",
       " 0.34110586114911146,\n",
       " 0.34108694368215675,\n",
       " 0.3410680685041023,\n",
       " 0.34104923547947835,\n",
       " 0.34103044447332914,\n",
       " 0.3410116953512118,\n",
       " 0.34099298797919386,\n",
       " 0.34097432222385127,\n",
       " 0.34095569795226666,\n",
       " 0.34093711503202695,\n",
       " 0.3409185733312218,\n",
       " 0.34090007271844114,\n",
       " 0.340881613062774,\n",
       " 0.3408631942338054,\n",
       " 0.3408448161016155,\n",
       " 0.3408264785367769,\n",
       " 0.34080818141035324,\n",
       " 0.3407899245938964,\n",
       " 0.34077170795944606,\n",
       " 0.34075353137952585,\n",
       " 0.3407353947271433,\n",
       " 0.3407172978757866,\n",
       " 0.3406992406994231,\n",
       " 0.3406812230724976,\n",
       " 0.3406632448699305,\n",
       " 0.34064530596711534,\n",
       " 0.34062740623991766,\n",
       " 0.3406095455646724,\n",
       " 0.3405917238181826,\n",
       " 0.34057394087771736,\n",
       " 0.34055619662100983,\n",
       " 0.3405384909262553,\n",
       " 0.34052082367211,\n",
       " 0.3405031947376883,\n",
       " 0.3404856040025615,\n",
       " 0.3404680513467559,\n",
       " 0.340450536650751,\n",
       " 0.3404330597954774,\n",
       " 0.3404156206623153,\n",
       " 0.3403982191330926,\n",
       " 0.3403808550900832,\n",
       " 0.340363528416005,\n",
       " 0.34034623899401806,\n",
       " 0.3403289867077234,\n",
       " 0.34031177144116026,\n",
       " 0.3402945930788055,\n",
       " 0.34027745150557076,\n",
       " 0.3402603466068014,\n",
       " 0.34024327826827416,\n",
       " 0.34022624637619653,\n",
       " 0.3402092508172033,\n",
       " 0.34019229147835667,\n",
       " 0.340175368247143,\n",
       " 0.3401584810114721,\n",
       " 0.34014162965967515,\n",
       " 0.3401248140805028,\n",
       " 0.34010803416312396,\n",
       " 0.34009128979712355,\n",
       " 0.3400745808725013,\n",
       " 0.3400579072796697,\n",
       " 0.34004126890945263,\n",
       " 0.34002466565308365,\n",
       " 0.3400080974022038,\n",
       " 0.3399915640488608,\n",
       " 0.33997506548550693,\n",
       " 0.33995860160499725,\n",
       " 0.3399421723005884,\n",
       " 0.33992577746593644,\n",
       " 0.33990941699509586,\n",
       " 0.33989309078251734,\n",
       " 0.33987679872304644,\n",
       " 0.3398605407119221,\n",
       " 0.3398443166447748,\n",
       " 0.3398281264176251,\n",
       " 0.339811969926882,\n",
       " 0.3397958470693415,\n",
       " 0.3397797577421846,\n",
       " 0.33976370184297633,\n",
       " 0.33974767926966387,\n",
       " 0.3397316899205748,\n",
       " 0.3397157336944159,\n",
       " 0.3396998104902715,\n",
       " 0.33968392020760174,\n",
       " 0.33966806274624156,\n",
       " 0.3396522380063982,\n",
       " 0.33963644588865094,\n",
       " 0.3396206862939484,\n",
       " 0.339604959123608,\n",
       " 0.3395892642793137,\n",
       " 0.3395736016631148,\n",
       " 0.3395579711774248,\n",
       " 0.3395423727250192,\n",
       " 0.3395268062090347,\n",
       " 0.33951127153296695,\n",
       " 0.3394957686006702,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=4)\n",
    "\n",
    "# autoE.train(X_train, max_epochs=20000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = CombinedModel(autoE,n_hidden_2=20,n_output=y_val.shape[1])\n",
    "\n",
    "# loss, val_metrics, final_loss = combined.train(X_train,y_train,X_val,y_val,epochs=15000,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3475558496346913,\n",
       " 1.3116443215896023,\n",
       " 1.2783731312208897,\n",
       " 1.247614222229498,\n",
       " 1.2192328261038254,\n",
       " 1.1930899599902345,\n",
       " 1.1690447973089322,\n",
       " 1.1469568196977793,\n",
       " 1.1266876905917806,\n",
       " 1.108102819424844,\n",
       " 1.091072609448097,\n",
       " 1.0754734007186426,\n",
       " 1.0611881328818586,\n",
       " 1.0481067604432903,\n",
       " 1.036126457116361,\n",
       " 1.0251516464817423,\n",
       " 1.0150938945312487,\n",
       " 1.0058716965164032,\n",
       " 0.9974101865520197,\n",
       " 0.9896407941495597,\n",
       " 0.9825008676344882,\n",
       " 0.9759332804696392,\n",
       " 0.9698860329952316,\n",
       " 0.9643118590637956,\n",
       " 0.9591678445016673,\n",
       " 0.954415062241786,\n",
       " 0.9500182273005653,\n",
       " 0.9459453734611617,\n",
       " 0.9421675525213723,\n",
       " 0.9386585562139085,\n",
       " 0.9353946603620018,\n",
       " 0.9323543904523456,\n",
       " 0.9295183075545672,\n",
       " 0.9268688133621221,\n",
       " 0.9243899730494605,\n",
       " 0.9220673546151141,\n",
       " 0.9198878833946125,\n",
       " 0.9178397104688333,\n",
       " 0.915912093753376,\n",
       " 0.91409529062579,\n",
       " 0.9123804610248201,\n",
       " 0.9107595800354694,\n",
       " 0.9092253590529004,\n",
       " 0.9077711746951281,\n",
       " 0.9063910047078881,\n",
       " 0.905079370174189,\n",
       " 0.9038312834054917,\n",
       " 0.9026422009510291,\n",
       " 0.9015079812164806,\n",
       " 0.9004248462332234,\n",
       " 0.8993893471648743,\n",
       " 0.8983983331791047,\n",
       " 0.8974489233500398,\n",
       " 0.896538481290245,\n",
       " 0.8956645922416681,\n",
       " 0.8948250423822199,\n",
       " 0.8940178001292539,\n",
       " 0.8932409992432477,\n",
       " 0.8924929235548316,\n",
       " 0.8917719931560578,\n",
       " 0.8910767519127913,\n",
       " 0.8904058561694029,\n",
       " 0.8897580645298006,\n",
       " 0.8891322286103747,\n",
       " 0.8885272846707746,\n",
       " 0.8879422460377404,\n",
       " 0.8873761962455635,\n",
       " 0.8868282828242509,\n",
       " 0.8862977116732197,\n",
       " 0.8857837419644106,\n",
       " 0.8852856815241745,\n",
       " 0.8848028826481958,\n",
       " 0.884334738308142,\n",
       " 0.8838806787127172,\n",
       " 0.8834401681893902,\n",
       " 0.8830127023563061,\n",
       " 0.8825978055568172,\n",
       " 0.8821950285316994,\n",
       " 0.8818039463065105,\n",
       " 0.881424156273689,\n",
       " 0.8810552764509375,\n",
       " 0.8806969438991948,\n",
       " 0.8803488132850764,\n",
       " 0.8800105555741091,\n",
       " 0.8796818568423802,\n",
       " 0.8793624171953874,\n",
       " 0.8790519497839528,\n",
       " 0.8787501799080105,\n",
       " 0.8784568441999564,\n",
       " 0.878171689880038,\n",
       " 0.8778944740769652,\n",
       " 0.8776249632075797,\n",
       " 0.8773629324100009,\n",
       " 0.877108165025194,\n",
       " 0.8768604521223903,\n",
       " 0.8766195920642191,\n",
       " 0.8763853901078114,\n",
       " 0.8761576580384806,\n",
       " 0.8759362138329237,\n",
       " 0.8757208813491638,\n",
       " 0.8755114900407296,\n",
       " 0.8753078746928026,\n",
       " 0.8751098751782834,\n",
       " 0.8749173362319218,\n",
       " 0.8747301072408326,\n",
       " 0.8745480420498885,\n",
       " 0.8743709987806143,\n",
       " 0.8741988396623506,\n",
       " 0.8740314308745663,\n",
       " 0.8738686423993118,\n",
       " 0.8737103478829029,\n",
       " 0.8735564245060075,\n",
       " 0.8734067528613968,\n",
       " 0.8732612168386863,\n",
       " 0.8731197035154619,\n",
       " 0.8729821030542434,\n",
       " 0.8728483086047935,\n",
       " 0.8727182162113231,\n",
       " 0.8725917247241944,\n",
       " 0.8724687357157563,\n",
       " 0.8723491533999838,\n",
       " 0.8722328845556285,\n",
       " 0.8721198384526093,\n",
       " 0.8720099267814045,\n",
       " 0.8719030635852272,\n",
       " 0.8717991651947841,\n",
       " 0.8716981501654445,\n",
       " 0.8715999392166557,\n",
       " 0.8715044551734574,\n",
       " 0.8714116229099685,\n",
       " 0.87132136929472,\n",
       " 0.8712336231377339,\n",
       " 0.8711483151392406,\n",
       " 0.871065377839955,\n",
       " 0.870984745572821,\n",
       " 0.870906354416159,\n",
       " 0.8708301421481415,\n",
       " 0.8707560482025409,\n",
       " 0.8706840136256908,\n",
       " 0.8706139810346077,\n",
       " 0.8705458945762297,\n",
       " 0.8704796998877241,\n",
       " 0.8704153440578272,\n",
       " 0.8703527755891778,\n",
       " 0.8702919443616096,\n",
       " 0.8702328015963697,\n",
       " 0.8701752998212371,\n",
       " 0.8701193928365076,\n",
       " 0.8700650356818249,\n",
       " 0.8700121846038286,\n",
       " 0.8699607970245992,\n",
       " 0.8699108315108758,\n",
       " 0.8698622477440291,\n",
       " 0.8698150064907655,\n",
       " 0.869769069574548,\n",
       " 0.8697243998477137,\n",
       " 0.8696809611642699,\n",
       " 0.8696387183533555,\n",
       " 0.8695976371933495,\n",
       " 0.8695576843866101,\n",
       " 0.8695188275348356,\n",
       " 0.869481035115023,\n",
       " 0.8694442764560212,\n",
       " 0.869408521715658,\n",
       " 0.8693737418584311,\n",
       " 0.8693399086337478,\n",
       " 0.8693069945547055,\n",
       " 0.8692749728773955,\n",
       " 0.8692438175807232,\n",
       " 0.8692135033467286,\n",
       " 0.8691840055414004,\n",
       " 0.869155300195969,\n",
       " 0.8691273639886684,\n",
       " 0.8691001742269588,\n",
       " 0.869073708830196,\n",
       " 0.8690479463127393,\n",
       " 0.8690228657674884,\n",
       " 0.8689984468498375,\n",
       " 0.8689746697620392,\n",
       " 0.8689515152379671,\n",
       " 0.8689289645282681,\n",
       " 0.868906999385897,\n",
       " 0.8688856020520208,\n",
       " 0.8688647552422892,\n",
       " 0.868844442133456,\n",
       " 0.8688246463503494,\n",
       " 0.8688053519531803,\n",
       " 0.868786543425177,\n",
       " 0.8687682056605461,\n",
       " 0.8687503239527433,\n",
       " 0.8687328839830526,\n",
       " 0.8687158718094641,\n",
       " 0.8686992738558414,\n",
       " 0.8686830769013749,\n",
       " 0.868667268070311,\n",
       " 0.8686518348219507,\n",
       " 0.8686367649409134,\n",
       " 0.8686220465276537,\n",
       " 0.8686076679892322,\n",
       " 0.8685936180303254,\n",
       " 0.8685798856444767,\n",
       " 0.8685664601055759,\n",
       " 0.8685533309595638,\n",
       " 0.8685404880163581,\n",
       " 0.8685279213419892,\n",
       " 0.8685156212509474,\n",
       " 0.868503578298728,\n",
       " 0.8684917832745782,\n",
       " 0.8684802271944312,\n",
       " 0.8684689012940274,\n",
       " 0.8684577970222206,\n",
       " 0.8684469060344538,\n",
       " 0.868436220186413,\n",
       " 0.8684257315278439,\n",
       " 0.8684154322965323,\n",
       " 0.8684053149124406,\n",
       " 0.868395371972,\n",
       " 0.8683855962425492,\n",
       " 0.8683759806569193,\n",
       " 0.8683665183081591,\n",
       " 0.8683572024443963,\n",
       " 0.8683480264638325,\n",
       " 0.8683389839098651,\n",
       " 0.8683300684663383,\n",
       " 0.8683212739529103,\n",
       " 0.8683125943205426,\n",
       " 0.8683040236471024,\n",
       " 0.8682955561330756,\n",
       " 0.8682871860973871,\n",
       " 0.8682789079733283,\n",
       " 0.8682707163045829,\n",
       " 0.868262605741352,\n",
       " 0.8682545710365753,\n",
       " 0.8682466070422444,\n",
       " 0.8682387087058037,\n",
       " 0.868230871066641,\n",
       " 0.8682230892526593,\n",
       " 0.8682153584769324,\n",
       " 0.8682076740344364,\n",
       " 0.8682000312988596,\n",
       " 0.8681924257194852,\n",
       " 0.868184852818145,\n",
       " 0.8681773081862418,\n",
       " 0.8681697874818404,\n",
       " 0.8681622864268185,\n",
       " 0.8681548008040846,\n",
       " 0.868147326454853,\n",
       " 0.8681398592759765,\n",
       " 0.8681323952173362,\n",
       " 0.8681249302792843,\n",
       " 0.8681174605101386,\n",
       " 0.868109982003727,\n",
       " 0.8681024908969793,\n",
       " 0.868094983367567,\n",
       " 0.8680874556315854,\n",
       " 0.8680799039412782,\n",
       " 0.8680723245828051,\n",
       " 0.8680647138740463,\n",
       " 0.8680570681624444,\n",
       " 0.8680493838228851,\n",
       " 0.8680416572556077,\n",
       " 0.8680338848841509,\n",
       " 0.8680260631533314,\n",
       " 0.8680181885272472,\n",
       " 0.8680102574873138,\n",
       " 0.868002266530324,\n",
       " 0.8679942121665346,\n",
       " 0.867986090917777,\n",
       " 0.867977899315588,\n",
       " 0.8679696338993651,\n",
       " 0.8679612912145387,\n",
       " 0.8679528678107644,\n",
       " 0.867944360240131,\n",
       " 0.8679357650553859,\n",
       " 0.8679270788081742,\n",
       " 0.8679182980472903,\n",
       " 0.8679094193169432,\n",
       " 0.8679004391550316,\n",
       " 0.8678913540914284,\n",
       " 0.8678821606462744,\n",
       " 0.8678728553282782,\n",
       " 0.8678634346330226,\n",
       " 0.8678538950412764,\n",
       " 0.8678442330173084,\n",
       " 0.8678344450072051,\n",
       " 0.8678245274371884,\n",
       " 0.8678144767119361,\n",
       " 0.8678042892128972,\n",
       " 0.8677939612966088,\n",
       " 0.8677834892930079,\n",
       " 0.8677728695037384,\n",
       " 0.867762098200454,\n",
       " 0.8677511716231119,\n",
       " 0.867740085978261,\n",
       " 0.8677288374373202,\n",
       " 0.8677174221348446,\n",
       " 0.8677058361667823,\n",
       " 0.8676940755887188,\n",
       " 0.8676821364141054,\n",
       " 0.8676700146124743,\n",
       " 0.8676577061076369,\n",
       " 0.8676452067758644,\n",
       " 0.8676325124440497,\n",
       " 0.8676196188878503,\n",
       " 0.8676065218298081,\n",
       " 0.8675932169374488,\n",
       " 0.867579699821355,\n",
       " 0.8675659660332176,\n",
       " 0.8675520110638566,\n",
       " 0.8675378303412177,\n",
       " 0.867523419228338,\n",
       " 0.8675087730212807,\n",
       " 0.8674938869470397,\n",
       " 0.867478756161408,\n",
       " 0.8674633757468145,\n",
       " 0.8674477407101212,\n",
       " 0.8674318459803843,\n",
       " 0.8674156864065755,\n",
       " 0.8673992567552625,\n",
       " 0.8673825517082447,\n",
       " 0.8673655658601485,\n",
       " 0.8673482937159712,\n",
       " 0.8673307296885828,\n",
       " 0.8673128680961736,\n",
       " 0.8672947031596518,\n",
       " 0.8672762289999898,\n",
       " 0.8672574396355112,\n",
       " 0.867238328979125,\n",
       " 0.8672188908354964,\n",
       " 0.8671991188981605,\n",
       " 0.8671790067465682,\n",
       " 0.8671585478430711,\n",
       " 0.8671377355298338,\n",
       " 0.8671165630256807,\n",
       " 0.8670950234228663,\n",
       " 0.8670731096837743,\n",
       " 0.8670508146375354,\n",
       " 0.8670281309765691,\n",
       " 0.86700505125304,\n",
       " 0.8669815678752294,\n",
       " 0.8669576731038208,\n",
       " 0.8669333590480903,\n",
       " 0.8669086176620087,\n",
       " 0.8668834407402409,\n",
       " 0.8668578199140494,\n",
       " 0.8668317466470923,\n",
       " 0.8668052122311147,\n",
       " 0.866778207781533,\n",
       " 0.8667507242328991,\n",
       " 0.8667227523342554,\n",
       " 0.8666942826443622,\n",
       " 0.8666653055268037,\n",
       " 0.8666358111449637,\n",
       " 0.8666057894568694,\n",
       " 0.8665752302098967,\n",
       " 0.8665441229353348,\n",
       " 0.8665124569428039,\n",
       " 0.8664802213145236,\n",
       " 0.8664474048994225,\n",
       " 0.8664139963070916,\n",
       " 0.8663799839015675,\n",
       " 0.866345355794948,\n",
       " 0.8663100998408293,\n",
       " 0.866274203627562,\n",
       " 0.8662376544713181,\n",
       " 0.8662004394089651,\n",
       " 0.8661625451907394,\n",
       " 0.8661239582727133,\n",
       " 0.8660846648090478,\n",
       " 0.8660446506440269,\n",
       " 0.8660039013038637,\n",
       " 0.8659624019882719,\n",
       " 0.8659201375617955,\n",
       " 0.8658770925448893,\n",
       " 0.8658332511047402,\n",
       " 0.8657885970458237,\n",
       " 0.8657431138001842,\n",
       " 0.8656967844174325,\n",
       " 0.8656495915544502,\n",
       " 0.8656015174647917,\n",
       " 0.8655525439877735,\n",
       " 0.8655026525372416,\n",
       " 0.8654518240900059,\n",
       " 0.8654000391739313,\n",
       " 0.8653472778556728,\n",
       " 0.8652935197280456,\n",
       " 0.8652387438970154,\n",
       " 0.8651829289682991,\n",
       " 0.86512605303356,\n",
       " 0.8650680936561895,\n",
       " 0.865009027856655,\n",
       " 0.8649488320974043,\n",
       " 0.8648874822673117,\n",
       " 0.8648249536656503,\n",
       " 0.8647612209855708,\n",
       " 0.8646962582970801,\n",
       " 0.8646300390294921,\n",
       " 0.864562535953342,\n",
       " 0.8644937211617416,\n",
       " 0.8644235660511598,\n",
       " 0.864352041301607,\n",
       " 0.8642791168562065,\n",
       " 0.8642047619001297,\n",
       " 0.8641289448388774,\n",
       " 0.8640516332758822,\n",
       " 0.8639727939894125,\n",
       " 0.863892392908755,\n",
       " 0.8638103950896476,\n",
       " 0.863726764688946,\n",
       " 0.8636414649384901,\n",
       " 0.8635544581181505,\n",
       " 0.8634657055280249,\n",
       " 0.8633751674597582,\n",
       " 0.8632828031669546,\n",
       " 0.8631885708346575,\n",
       " 0.86309242754786,\n",
       " 0.8629943292590218,\n",
       " 0.8628942307545512,\n",
       " 0.8627920856202291,\n",
       " 0.8626878462055315,\n",
       " 0.8625814635868196,\n",
       " 0.8624728875293618,\n",
       " 0.8623620664481462,\n",
       " 0.8622489473674481,\n",
       " 0.8621334758791128,\n",
       " 0.8620155960995097,\n",
       " 0.8618952506251205,\n",
       " 0.8617723804867137,\n",
       " 0.8616469251020639,\n",
       " 0.8615188222271711,\n",
       " 0.8613880079059316,\n",
       " 0.861254416418212,\n",
       " 0.8611179802262824,\n",
       " 0.8609786299195528,\n",
       " 0.860836294157565,\n",
       " 0.8606908996111897,\n",
       " 0.8605423709019687,\n",
       " 0.8603906305395573,\n",
       " 0.8602355988572039,\n",
       " 0.8600771939452148,\n",
       " 0.8599153315823448,\n",
       " 0.8597499251650597,\n",
       " 0.8595808856346052,\n",
       " 0.8594081214018311,\n",
       " 0.8592315382697033,\n",
       " 0.8590510393534498,\n",
       " 0.8588665249982749,\n",
       " 0.8586778926945838,\n",
       " 0.8584850369906577,\n",
       " 0.8582878494027147,\n",
       " 0.8580862183223006,\n",
       " 0.8578800289209494,\n",
       " 0.8576691630520513,\n",
       " 0.8574534991498769,\n",
       " 0.8572329121256961,\n",
       " 0.8570072732609415,\n",
       " 0.8567764500973636,\n",
       " 0.8565403063241275,\n",
       " 0.856298701661808,\n",
       " 0.8560514917432398,\n",
       " 0.8557985279911837,\n",
       " 0.8555396574927799,\n",
       " 0.8552747228707583,\n",
       " 0.8550035621513862,\n",
       " 0.8547260086291403,\n",
       " 0.8544418907280993,\n",
       " 0.8541510318600583,\n",
       " 0.8538532502793816,\n",
       " 0.8535483589346208,\n",
       " 0.8532361653169337,\n",
       " 0.8529164713053586,\n",
       " 0.8525890730090152,\n",
       " 0.852253760606311,\n",
       " 0.8519103181812662,\n",
       " 0.8515585235570754,\n",
       " 0.851198148127061,\n",
       " 0.8508289566831887,\n",
       " 0.8504507072423488,\n",
       " 0.8500631508706321,\n",
       " 0.8496660315058716,\n",
       " 0.8492590857787434,\n",
       " 0.8488420428327733,\n",
       " 0.8484146241436269,\n",
       " 0.8479765433381163,\n",
       " 0.8475275060133975,\n",
       " 0.8470672095568946,\n",
       " 0.8465953429675422,\n",
       " 0.846111586678999,\n",
       " 0.8456156123855527,\n",
       " 0.8451070828715189,\n",
       " 0.8445856518449988,\n",
       " 0.8440509637769603,\n",
       " 0.8435026537466874,\n",
       " 0.8429403472947435,\n",
       " 0.8423636602846946,\n",
       " 0.8417721987749487,\n",
       " 0.8411655589021847,\n",
       " 0.8405433267779636,\n",
       " 0.8399050784002491,\n",
       " 0.8392503795817003,\n",
       " 0.8385787858967385,\n",
       " 0.8378898426495541,\n",
       " 0.8371830848653602,\n",
       " 0.8364580373073824,\n",
       " 0.8357142145222303,\n",
       " 0.8349511209164832,\n",
       " 0.8341682508675007,\n",
       " 0.8333650888716538,\n",
       " 0.8325411097333637,\n",
       " 0.8316957787985291,\n",
       " 0.8308285522361081,\n",
       " 0.8299388773718197,\n",
       " 0.8290261930781111,\n",
       " 0.8280899302247218,\n",
       " 0.8271295121943495,\n",
       " 0.826144355468081,\n",
       " 0.8251338702853996,\n",
       " 0.8240974613837116,\n",
       " 0.8230345288224273,\n",
       " 0.8219444688967212,\n",
       " 0.8208266751461204,\n",
       " 0.8196805394630863,\n",
       " 0.8185054533066948,\n",
       " 0.8173008090264285,\n",
       " 0.8160660013009272,\n",
       " 0.8148004286963162,\n",
       " 0.8135034953484223,\n",
       " 0.8121746127727852,\n",
       " 0.8108132018058969,\n",
       " 0.8094186946804883,\n",
       " 0.8079905372369929,\n",
       " 0.8065281912724729,\n",
       " 0.8050311370273462,\n",
       " 0.8034988758091476,\n",
       " 0.8019309327513248,\n",
       " 0.8003268597036801,\n",
       " 0.7986862382495277,\n",
       " 0.7970086828429513,\n",
       " 0.7952938440577014,\n",
       " 0.7935414119372882,\n",
       " 0.791751119433703,\n",
       " 0.789922745919952,\n",
       " 0.7880561207592354,\n",
       " 0.7861511269111632,\n",
       " 0.7842077045528982,\n",
       " 0.7822258546906002,\n",
       " 0.7802056427340265,\n",
       " 0.778147202004695,\n",
       " 0.7760507371456683,\n",
       " 0.7739165273988273,\n",
       " 0.771744929713541,\n",
       " 0.7695363816489524,\n",
       " 0.7672914040307753,\n",
       " 0.7650106033225696,\n",
       " 0.7626946736710414,\n",
       " 0.7603443985850203,\n",
       " 0.757960652208495,\n",
       " 0.7555444001494666,\n",
       " 0.7530966998284727,\n",
       " 0.7506187003134565,\n",
       " 0.7481116416112371,\n",
       " 0.7455768533901759,\n",
       " 0.7430157531137027,\n",
       " 0.740429843570156,\n",
       " 0.737820709790801,\n",
       " 0.7351900153548823,\n",
       " 0.7325394980880163,\n",
       " 0.7298709651680176,\n",
       " 0.7271862876602456,\n",
       " 0.7244873945126018,\n",
       " 0.721776266048227,\n",
       " 0.7190549270015861,\n",
       " 0.7163254391507873,\n",
       " 0.7135898936055195,\n",
       " 0.7108504028157271,\n",
       " 0.7081090923709182,\n",
       " 0.7053680926637266,\n",
       " 0.7026295304938884,\n",
       " 0.6998955206900828,\n",
       " 0.697168157827094,\n",
       " 0.6944495081144656,\n",
       " 0.6917416015302505,\n",
       " 0.6890464242696942,\n",
       " 0.6863659115738048,\n",
       " 0.6837019409968708,\n",
       " 0.6810563261652545,\n",
       " 0.67843081107235,\n",
       " 0.6758270649466526,\n",
       " 0.6732466777215926,\n",
       " 0.670691156127366,\n",
       " 0.6681619204165772,\n",
       " 0.6656603017273127,\n",
       " 0.663187540079413,\n",
       " 0.6607447829923465,\n",
       " 0.6583330847063441,\n",
       " 0.6559534059824016,\n",
       " 0.6536066144514739,\n",
       " 0.651293485478727,\n",
       " 0.6490147035050786,\n",
       " 0.6467708638254751,\n",
       " 0.6445624747613782,\n",
       " 0.6423899601837446,\n",
       " 0.6402536623423277,\n",
       " 0.6381538449573398,\n",
       " 0.6360906965303266,\n",
       " 0.6340643338324448,\n",
       " 0.6320748055301108,\n",
       " 0.6301220959101447,\n",
       " 0.6282061286689681,\n",
       " 0.6263267707330719,\n",
       " 0.6244838360807733,\n",
       " 0.6226770895381672,\n",
       " 0.6209062505250899,\n",
       " 0.6191709967298055,\n",
       " 0.6174709676939517,\n",
       " 0.6158057682920055,\n",
       " 0.6141749720921277,\n",
       " 0.6125781245876871,\n",
       " 0.6110147462910491,\n",
       " 0.6094843356833016,\n",
       " 0.6079863720155156,\n",
       " 0.6065203179588577,\n",
       " 0.6050856221024188,\n",
       " 0.6036817212989801,\n",
       " 0.6023080428601297,\n",
       " 0.6009640066031591,\n",
       " 0.5996490267530424,\n",
       " 0.5983625137035188,\n",
       " 0.5971038756418865,\n",
       " 0.5958725200425911,\n",
       " 0.5946678550350424,\n",
       " 0.5934892906513582,\n",
       " 0.5923362399599081,\n",
       " 0.5912081200906208,\n",
       " 0.5901043531580625,\n",
       " 0.5890243670882535,\n",
       " 0.5879675963551272,\n",
       " 0.5869334826324181,\n",
       " 0.5859214753666138,\n",
       " 0.5849310322764417,\n",
       " 0.5839616197841544,\n",
       " 0.5830127133836775,\n",
       " 0.5820837979504538,\n",
       " 0.5811743679975911,\n",
       " 0.5802839278826886,\n",
       " 0.5794119919694763,\n",
       " 0.5785580847481747,\n",
       " 0.577721740918243,\n",
       " 0.576902505436965,\n",
       " 0.5760999335370957,\n",
       " 0.5753135907165824,\n",
       " 0.574543052703166,\n",
       " 0.5737879053964723,\n",
       " 0.5730477447900151,\n",
       " 0.5723221768753531,\n",
       " 0.5716108175304753,\n",
       " 0.5709132923943236,\n",
       " 0.5702292367292183,\n",
       " 0.5695582952728008,\n",
       " 0.5689001220809806,\n",
       " 0.5682543803632466,\n",
       " 0.5676207423115842,\n",
       " 0.5669988889241325,\n",
       " 0.5663885098246147,\n",
       " 0.5657893030784775,\n",
       " 0.56520097500659,\n",
       " 0.5646232399972723,\n",
       " 0.5640558203173452,\n",
       " 0.5634984459228265,\n",
       " 0.5629508542698309,\n",
       " 0.5624127901261795,\n",
       " 0.5618840053841576,\n",
       " 0.5613642588748244,\n",
       " 0.5608533161842183,\n",
       " 0.5603509494717691,\n",
       " 0.5598569372911861,\n",
       " 0.5593710644140535,\n",
       " 0.5588931216563365,\n",
       " 0.55842290570797,\n",
       " 0.5579602189656728,\n",
       " 0.5575048693691104,\n",
       " 0.5570566702405013,\n",
       " 0.5566154401277488,\n",
       " 0.5561810026511531,\n",
       " 0.5557531863537523,\n",
       " 0.5553318245553139,\n",
       " 0.5549167552099985,\n",
       " 0.5545078207676926,\n",
       " 0.5541048680390054,\n",
       " 0.5537077480639108,\n",
       " 0.5533163159840084,\n",
       " 0.5529304309183692,\n",
       " 0.5525499558429251,\n",
       " 0.5521747574733559,\n",
       " 0.5518047061514221,\n",
       " 0.5514396757346853,\n",
       " 0.5510795434895596,\n",
       " 0.550724189987626,\n",
       " 0.5503734990051478,\n",
       " 0.5500273574257151,\n",
       " 0.5496856551459516,\n",
       " 0.5493482849842061,\n",
       " 0.549015142592164,\n",
       " 0.5486861263692967,\n",
       " 0.5483611373800809,\n",
       " 0.5480400792739095,\n",
       " 0.5477228582076231,\n",
       " 0.5474093827705842,\n",
       " 0.5470995639122231,\n",
       " 0.5467933148719825,\n",
       " 0.5464905511115854,\n",
       " 0.5461911902495594,\n",
       " 0.5458951519979418,\n",
       " 0.5456023581011014,\n",
       " 0.5453127322766029,\n",
       " 0.5450262001580539,\n",
       " 0.5447426892398621,\n",
       " 0.5444621288238427,\n",
       " 0.5441844499676128,\n",
       " 0.5439095854347097,\n",
       " 0.5436374696463757,\n",
       " 0.5433680386349492,\n",
       " 0.5431012299988062,\n",
       " 0.5428369828587978,\n",
       " 0.542575237816129,\n",
       " 0.5423159369116275,\n",
       " 0.5420590235863516,\n",
       " 0.541804442643489,\n",
       " 0.5415521402114992,\n",
       " 0.5413020637084515,\n",
       " 0.5410541618075183,\n",
       " 0.5408083844035777,\n",
       " 0.5405646825808846,\n",
       " 0.5403230085817713,\n",
       " 0.5400833157763384,\n",
       " 0.5398455586330989,\n",
       " 0.5396096926905407,\n",
       " 0.5393756745295704,\n",
       " 0.5391434617468082,\n",
       " 0.5389130129287009,\n",
       " 0.538684287626421,\n",
       " 0.5384572463315243,\n",
       " 0.538231850452336,\n",
       " 0.53800806229104,\n",
       " 0.5377858450214421,\n",
       " 0.5375651626673859,\n",
       " 0.5373459800817931,\n",
       " 0.5371282629263099,\n",
       " 0.5369119776515305,\n",
       " 0.5366970914777852,\n",
       " 0.5364835723764643,\n",
       " 0.5362713890518634,\n",
       " 0.5360605109235302,\n",
       " 0.5358509081090942,\n",
       " 0.5356425514075615,\n",
       " 0.535435412283061,\n",
       " 0.5352294628490238,\n",
       " 0.5350246758527801,\n",
       " 0.5348210246605626,\n",
       " 0.5346184832428991,\n",
       " 0.5344170261603824,\n",
       " 0.5342166285498059,\n",
       " 0.5340172661106501,\n",
       " 0.5338189150919126,\n",
       " 0.5336215522792641,\n",
       " 0.5334251549825282,\n",
       " 0.5332297010234669,\n",
       " 0.5330351687238662,\n",
       " 0.5328415368939137,\n",
       " 0.5326487848208536,\n",
       " 0.5324568922579179,\n",
       " 0.5322658394135203,\n",
       " 0.5320756069407059,\n",
       " 0.5318861759268512,\n",
       " 0.5316975278836056,\n",
       " 0.5315096447370676,\n",
       " 0.5313225088181887,\n",
       " 0.5311361028533998,\n",
       " 0.5309504099554535,\n",
       " 0.5307654136144754,\n",
       " 0.5305810976892203,\n",
       " 0.5303974463985276,\n",
       " 0.5302144443129682,\n",
       " 0.5300320763466829,\n",
       " 0.5298503277494021,\n",
       " 0.529669184098646,\n",
       " 0.5294886312920983,\n",
       " 0.5293086555401508,\n",
       " 0.5291292433586138,\n",
       " 0.5289503815615891,\n",
       " 0.5287720572544998,\n",
       " 0.5285942578272761,\n",
       " 0.5284169709476905,\n",
       " 0.5282401845548415,\n",
       " 0.528063886852779,\n",
       " 0.5278880663042739,\n",
       " 0.52771271162472,\n",
       " 0.5275378117761755,\n",
       " 0.5273633559615314,\n",
       " 0.5271893336188102,\n",
       " 0.5270157344155907,\n",
       " 0.5268425482435537,\n",
       " 0.5266697652131489,\n",
       " 0.5264973756483793,\n",
       " 0.5263253700817,\n",
       " 0.52615373924903,\n",
       " 0.5259824740848732,\n",
       " 0.5258115657175478,\n",
       " 0.5256410054645216,\n",
       " 0.5254707848278489,\n",
       " 0.5253008954897099,\n",
       " 0.5251313293080483,\n",
       " 0.5249620783123057,\n",
       " 0.5247931346992515,\n",
       " 0.5246244908289045,\n",
       " 0.5244561392205461,\n",
       " 0.5242880725488225,\n",
       " 0.5241202836399337,\n",
       " 0.5239527654679085,\n",
       " 0.5237855111509613,\n",
       " 0.5236185139479338,\n",
       " 0.5234517672548132,\n",
       " 0.5232852646013323,\n",
       " 0.523118999647643,\n",
       " 0.5229529661810683,\n",
       " 0.5227871581129261,\n",
       " 0.5226215694754244,\n",
       " 0.5224561944186298,\n",
       " 0.5222910272075023,\n",
       " 0.5221260622189998,\n",
       " 0.5219612939392475,\n",
       " 0.5217967169607731,\n",
       " 0.5216323259798058,\n",
       " 0.5214681157936356,\n",
       " 0.5213040812980358,\n",
       " 0.5211402174847439,\n",
       " 0.5209765194390007,\n",
       " 0.5208129823371472,\n",
       " 0.5206496014442767,\n",
       " 0.520486372111942,\n",
       " 0.5203232897759157,\n",
       " 0.5201603499540034,\n",
       " 0.5199975482439081,\n",
       " 0.519834880321144,\n",
       " 0.5196723419370002,\n",
       " 0.519509928916552,\n",
       " 0.5193476371567193,\n",
       " 0.519185462624371,\n",
       " 0.5190234013544739,\n",
       " 0.5188614494482862,\n",
       " 0.5186996030715934,\n",
       " 0.5185378584529868,\n",
       " 0.5183762118821829,\n",
       " 0.5182146597083822,\n",
       " 0.5180531983386694,\n",
       " 0.5178918242364493,\n",
       " 0.5177305339199233,\n",
       " 0.5175693239605998,\n",
       " 0.5174081909818425,\n",
       " 0.5172471316574534,\n",
       " 0.5170861427102891,\n",
       " 0.5169252209109128,\n",
       " 0.5167643630762768,\n",
       " 0.5166035660684389,\n",
       " 0.516442826793309,\n",
       " 0.5162821421994269,\n",
       " 0.51612150927677,\n",
       " 0.5159609250555902,\n",
       " 0.5158003866052794,\n",
       " 0.5156398910332627,\n",
       " 0.5154794354839201,\n",
       " 0.5153190171375326,\n",
       " 0.515158633209257,\n",
       " 0.5149982809481246,\n",
       " 0.5148379576360644,\n",
       " 0.5146776605869527,\n",
       " 0.5145173871456834,\n",
       " 0.5143571346872652,\n",
       " 0.5141969006159376,\n",
       " 0.5140366823643124,\n",
       " 0.5138764773925347,\n",
       " 0.5137162831874655,\n",
       " 0.5135560972618858,\n",
       " 0.5133959171537196,\n",
       " 0.5132357404252774,\n",
       " 0.513075564662519,\n",
       " 0.5129153874743347,\n",
       " 0.5127552064918445,\n",
       " 0.5125950193677162,\n",
       " 0.5124348237755004,\n",
       " 0.5122746174089825,\n",
       " 0.5121143979815518,\n",
       " 0.5119541632255863,\n",
       " 0.5117939108918542,\n",
       " 0.5116336387489303,\n",
       " 0.5114733445826278,\n",
       " 0.5113130261954453,\n",
       " 0.5111526814060272,\n",
       " 0.5109923080486398,\n",
       " 0.5108319039726601,\n",
       " 0.5106714670420781,\n",
       " 0.5105109951350132,\n",
       " 0.5103504861432417,\n",
       " 0.5101899379717398,\n",
       " 0.5100293485382354,\n",
       " 0.5098687157727748,\n",
       " 0.5097080376172988,\n",
       " 0.5095473120252323,\n",
       " 0.5093865369610842,\n",
       " 0.509225710400057,\n",
       " 0.5090648303276697,\n",
       " 0.5089038947393888,\n",
       " 0.5087429016402705,\n",
       " 0.5085818490446125,\n",
       " 0.5084207349756164,\n",
       " 0.5082595574650581,\n",
       " 0.508098314552969,\n",
       " 0.5079370042873256,\n",
       " 0.5077756247237468,\n",
       " 0.5076141739252028,\n",
       " 0.5074526499617292,\n",
       " 0.5072910509101518,\n",
       " 0.5071293748538184,\n",
       " 0.5069676198823383,\n",
       " 0.5068057840913303,\n",
       " 0.5066438655821779,\n",
       " 0.5064818624617919,\n",
       " 0.5063197728423804,\n",
       " 0.5061575948412251,\n",
       " 0.5059953265804663,\n",
       " 0.5058329661868923,\n",
       " 0.5056705117917373,\n",
       " 0.5055079615304856,\n",
       " 0.5053453135426806,\n",
       " 0.5051825659717422,\n",
       " 0.5050197169647884,\n",
       " 0.5048567646724642,\n",
       " 0.5046937072487755,\n",
       " 0.504530542850929,\n",
       " 0.5043672696391779,\n",
       " 0.5042038857766723,\n",
       " 0.5040403894293164,\n",
       " 0.503876778765629,\n",
       " 0.503713051956611,\n",
       " 0.5035492071756169,\n",
       " 0.5033852425982313,\n",
       " 0.5032211564021506,\n",
       " 0.5030569467670692,\n",
       " 0.5028926118745708,\n",
       " 0.5027281499080231,\n",
       " 0.5025635590524781,\n",
       " 0.502398837494577,\n",
       " 0.5022339834224581,\n",
       " 0.50206899502567,\n",
       " 0.5019038704950887,\n",
       " 0.5017386080228383,\n",
       " 0.5015732058022165,\n",
       " 0.5014076620276237,\n",
       " 0.5012419748944951,\n",
       " 0.501076142599238,\n",
       " 0.5009101633391723,\n",
       " 0.5007440353124732,\n",
       " 0.5005777567181211,\n",
       " 0.5004113257558498,\n",
       " 0.5002447406261035,\n",
       " 0.5000779995299933,\n",
       " 0.4999111006692594,\n",
       " 0.49974404224623453,\n",
       " 0.499576822463813,\n",
       " 0.4994094395254207,\n",
       " 0.49924189163498994,\n",
       " 0.499074176996936,\n",
       " 0.4989062938161378,\n",
       " 0.49873824029792074,\n",
       " 0.49857001464804307,\n",
       " 0.49840161507268466,\n",
       " 0.4982330397784392,\n",
       " 0.4980642869723079,\n",
       " 0.4978953548616983,\n",
       " 0.4977262416544228,\n",
       " 0.49755694555870267,\n",
       " 0.49738746478317286,\n",
       " 0.4972177975368899,\n",
       " 0.49704794202934216,\n",
       " 0.49687789647046404,\n",
       " 0.4967076590706501,\n",
       " 0.49653722804077366,\n",
       " 0.4963666015922072,\n",
       " 0.49619577793684483,\n",
       " 0.4960247552871277,\n",
       " 0.4958535318560707,\n",
       " 0.49568210585729283,\n",
       " 0.49551047550504873,\n",
       " 0.49533863901426245,\n",
       " 0.49516659460056445,\n",
       " 0.49499434048032914,\n",
       " 0.49482187487071616,\n",
       " 0.49464919598971235,\n",
       " 0.49447630205617654,\n",
       " 0.49430319128988687,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b1ee3aae50>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGgCAYAAAB45mdaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBAUlEQVR4nO3deXxU1f3/8fedPXvYEgiGxRUVBATBuLRao1QtrV/bapUKta396hd/VenXha8Va6tibbVYi1pt1Vq1oNZdKkUUVyqyREVBQFaBhDWZrDOTmfP74yZDAglkst0k83o+Hrcz99x7Zz4nAnn33HPvtYwxRgAAAA5xOV0AAABIboQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOCohMPIO++8o4kTJyovL0+WZenFF19s8bHvv/++PB6PRo0alejXAgCAHsqT6AGVlZUaOXKkfvzjH+vCCy9s8XGlpaWaPHmyzjrrLJWUlCT0nbFYTNu2bVNGRoYsy0q0ZAAA4ABjjMrLy5WXlyeXq/nxD6stD8qzLEsvvPCCLrjggkPu+4Mf/EBHHXWU3G63XnzxRRUVFbX4e7766ivl5+e3tkwAAOCgLVu26LDDDmt2e8IjI63x2GOPaf369XryySd1++23H3L/UCikUCgUX6/PS1u2bFFmZmaH1QkAANpPMBhUfn6+MjIyDrpfh4eRtWvX6qabbtK7774rj6dlXzdz5kzddtttB7RnZmYSRgAA6GYONcWiQ6+miUajuvTSS3Xbbbfp6KOPbvFx06dPV1lZWXzZsmVLB1YJAACc1KEjI+Xl5Vq6dKlWrFihq6++WpI9GdUYI4/Ho3//+9/6xje+ccBxfr9ffr+/I0sDAABdRIeGkczMTH366aeN2h544AG9+eabeu655zR06NCO/HoAANANJBxGKioqtG7duvj6hg0bVFRUpN69e2vQoEGaPn26tm7dqieeeEIul0vDhw9vdHxOTo4CgcAB7QAAIDklHEaWLl2qM888M74+bdo0SdKUKVP0+OOPa/v27dq8eXP7VQgAAHq0Nt1npLMEg0FlZWWprKyMq2kAAOgmWvr7m2fTAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4qlOe2ttVhd79k1S6Ua4xU+TNG+F0OQAAJKWkHhnZ+Pbf5V/2iFau/NjpUgAASFpJHUaMDv5IYwAA0PGSO4xYdWHExJwtBACAJJbUYUQijAAA4LSkDiPx0zRd//E8AAD0WEkdRupHRogiAAA4J6nDiKmfv8rICAAAjknuMFLXfSPmjAAA4JSkDiPxCawxRkYAAHBKcoeR+PxVRkYAAHBKUoeR+qtpLKawAgDgmKQOI/GraZjACgCAY5I6jMTvwMrICAAAjknuMMJNzwAAcFxShxFuBw8AgPOSOoyY+JwRhwsBACCJJXUYEU/tBQDAcckdRsQEVgAAnEYYkThPAwCAg5I6jNRf2st9RgAAcE5yh5H4aRrmjAAA4JSkDiP7TtM4WwUAAMmMMCJxNQ0AAA5K6jBirPg7J8sAACCpJXUYETc9AwDAcUkdRoxld9/iNA0AAI5J6jAirqYBAMBxhBGJKSMAADgoucNI/G7wjIwAAOCUpA4jpq773IEVAADnJHUYAQAAzkvqMFJ/NQ2naQAAcE5Sh5EGk0YcrQIAgGSW3GHEqr+ahjACAIBTkjuMxBFGAABwSsJh5J133tHEiROVl5cny7L04osvHnT/559/Xmeffbb69eunzMxMFRQUaP78+a2tt10ZRkYAAHBcwmGksrJSI0eO1OzZs1u0/zvvvKOzzz5b8+bN07Jly3TmmWdq4sSJWrFiRcLFtj/mjAAA4DRPogece+65Ovfcc1u8/6xZsxqt33nnnXrppZf0yiuvaPTo0Yl+fTurv5qGMAIAgFM6fc5ILBZTeXm5evfu3dlffSCr/qm9XNoLAIBTEh4Zaavf//73qqio0EUXXdTsPqFQSKFQKL4eDAY7qBqr7n8ZGQEAwCmdOjLy9NNP67bbbtMzzzyjnJycZvebOXOmsrKy4kt+fn6H1MMEVgAAnNdpYWTOnDn66U9/qmeeeUaFhYUH3Xf69OkqKyuLL1u2bOnY4ggjAAA4plNO0/zjH//Qj3/8Y82ZM0fnn3/+Iff3+/3y+/0dX5jF1TQAADgt4TBSUVGhdevWxdc3bNigoqIi9e7dW4MGDdL06dO1detWPfHEE5LsUzNTpkzRfffdp/Hjx6u4uFiSlJKSoqysrHbqRuuY+NU0jpYBAEBSS/g0zdKlSzV69Oj4ZbnTpk3T6NGjNWPGDEnS9u3btXnz5vj+Dz/8sGprazV16lQNGDAgvlxzzTXt1IW2qB8ZiTpaBQAAySzhkZEzzjhD5iBzLB5//PFG64sWLUr0KzpPfAKrs2UAAJDMkvzZNMwZAQDAackdRuJZhDACAIBTkjyMcDt4AACcltRhxMSHRrgdPAAATknqMCLuwAoAgOOSO4wwgRUAAMcRRiRGRgAAcFByhxFuBw8AgOOSOozUT2C1GBkBAMAxSR1G4pf2MjICAIBjkjuM1GNkBAAAxyR3GGFkBAAAxyV1GLG4HTwAAI5L6jDCfUYAAHBeUocRw7NpAABwXFKHkfqREYuREQAAHJPcYST+bBoelAcAgFOSO4zE54wAAACnJHcY4XbwAAA4LrnDCLeDBwDAcckdRrjpGQAAjkvyMMIEVgAAnJbcYSSOkREAAJyS3GHEYs4IAABOS+owYllJ3X0AALqEpP5tbJp4BwAAOldSh5H6q2ksJrACAOCY5A4j4tJeAACcltRhxLjckiSXiTpcCQAAySu5wwinaQAAcFxShxFZjIwAAOC0JA8jdSMjYmQEAACnJHUYqZ8zwmkaAACck9RhxLLqwwinaQAAcEpShxExMgIAgOMII2JkBAAAJyV1GLHqwwgTWAEAcExyhxEu7QUAwHFJHUbk4qZnAAA4LanDCKdpAABwHmFEnKYBAMBJSR5GPPar4am9AAA4JeEw8s4772jixInKy8uTZVl68cUXD3nMokWLdOKJJ8rv9+vII4/U448/3opSO0D8NA0jIwAAOCXhMFJZWamRI0dq9uzZLdp/w4YNOv/883XmmWeqqKhI1157rX76059q/vz5CRfb3lx1E1hdTGAFAMAxnkQPOPfcc3Xuuee2eP+HHnpIQ4cO1T333CNJOvbYY/Xee+/pD3/4gyZMmJDo17er+GkaJrACAOCYDp8zsnjxYhUWFjZqmzBhghYvXtzRX31o9RNYCSMAADgm4ZGRRBUXFys3N7dRW25uroLBoKqrq5WSknLAMaFQSKFQKL4eDAY7pDYXV9MAAOC4Lnk1zcyZM5WVlRVf8vPzO+R7LLedxRgZAQDAOR0eRvr376+SkpJGbSUlJcrMzGxyVESSpk+frrKysviyZcuWDqnN4qm9AAA4rsNP0xQUFGjevHmN2hYsWKCCgoJmj/H7/fL7/R1d2r6bnjEyAgCAYxIeGamoqFBRUZGKiook2ZfuFhUVafPmzZLsUY3JkyfH97/yyiu1fv163XDDDVq9erUeeOABPfPMM7ruuuvapwdtQBgBAMB5CYeRpUuXavTo0Ro9erQkadq0aRo9erRmzJghSdq+fXs8mEjS0KFD9dprr2nBggUaOXKk7rnnHv3lL39x/LJeSXLVzxnhNA0AAI5J+DTNGWecIXOQ26c3dXfVM844QytWrEj0qzrcvpERrqYBAMApXfJqms5SH0bcnKYBAMAxSR1GXG7mjAAA4LSkDiOW2ydJ8nCaBgAAxyR1GHF5A5LqwkiMQAIAgBOSOozI0+BeJrWh5vcDAAAdJqnDiMvj27cSJYwAAOCE5A4jbq+ixrJXGBkBAMARyR1GXC6F5bVXCCMAADgiqcOI22UpXH/fN8IIAACOSOow4nJZCqlu3ghzRgAAcERShxGve9/IiGFkBAAARyR1GPG73QoZe85INFztcDUAACSnpA4jXo8Vn8BaG6lxuBoAAJJTUocRn9ulUH0YqWFkBAAAJyR1GPG4XSpXqiQpWh10uBoAAJJTUocRSapQmiQpVlPqbCEAACSppA8jlS47jJjqUmcLAQAgSSV9GKmy7DCiGk7TAADgBMKIK91+U13mbCEAACSppA8jla4MSZKrepfDlQAAkJySPozs8eRIkjyV2x2uBACA5JT0YWSvN1eS5K/c5nAlAAAkp6QPI6V1YcQb2iuFKhyuBgCA5JP0YcSdmqUdJtteKVnpaC0AACSjpA8j6X6PPo4dbq9sXe5sMQAAJCHCiN+j5bGj7ZV1bzhbDAAASYgwEvBoXmycvbL+LWn3l84WBABAkkn6MJIR8GqT6a+1GeMlE5Pm/lBau0AKbpOiEafLAwCgx/M4XYDTMvz2j2BuzjX6Zexaacfn0lPf27dDWj8pc6DUa4h0+BnS8AulQJYTpQIA0CMl/chIZoodRr6s7Sf9bJE09id28LDc9g6VO6XtRdLnL0qvXivdN1Ja/ZozxQIA0AMl/chITmZAklQSDElZA6Vv3WtviMWk6j326ZrgVqn4U+mTZ6Tda+1TOZe9KB3+decKBwCgh0j6kZEBWXYYKQ7WNN7gcklpfaUBJ0jHnCt9/Qbpf/4jjfi+PbfklWukaK0DFQMA0LMQRjJTJEl7KsOqiUQPvrPbI31rlpTSW9q7QfpiXscXCABAD5f0YSQzxaP0ukmsG3dXHvoAf7o0epL9ftXLHVgZAADJIenDiGVZOi4vU5L06VdlLTto2Lfs13ULJWM6qDIAAJJD0ocRSRox0L5Ud8WW0pYdkDdacnntCa6lmzquMAAAkgBhRNJpR/WVJP37s2JForFDH+DxS/2H2++3FXVcYQAAJAHCiKTTjuyrvul+7aoIa/Zb62Racuql3zD7dfe6ji0OAIAeLunvMyJJXrdL1084Wjf+81PNemOtFnxeorGDeym/d6oG90nT4f3SNKh3qrzuBtmt11D7de9GR2oGAKCnIIzUuWhsvvZURvSHN9bos21BfbYt2Gi7z+3SaUf11eSCwTrjmBz7Lq0SYQQAgDYijNSxLEtXnXGEvjfmML2zZqfW7CjXV3uqtWFXpTbsqlR1JKo3V+/Qm6t36NLxg3T76EH2Oa7SzU6XDgBAt0YY2U+/DL++O+awRm2xmNG6nRWa+9EWPfb+Bj394WYdH0jTJMl+dg0AAGg1JrC2gMtl6ejcDN3yreN0+wUjJEn3flBqb4xUSaEK54oDAKCba1UYmT17toYMGaJAIKDx48dryZIlB91/1qxZOuaYY5SSkqL8/Hxdd911qqmpOegxXdUl4/J13IBM7Y54FXHZz7VR5Q5niwIAoBtLOIzMnTtX06ZN06233qrly5dr5MiRmjBhgnbsaPoX8tNPP62bbrpJt956q1atWqW//vWvmjt3rv7v//6vzcU7wbIsXTTWPo2zx8q2Gys4VQMAQGslHEbuvfdeXXHFFbr88st13HHH6aGHHlJqaqoeffTRJvf/4IMPdOqpp+rSSy/VkCFDdM455+iSSy455GhKV/aNYbmSpK2RDLuBkREAAFotoTASDoe1bNkyFRYW7vsAl0uFhYVavHhxk8eccsopWrZsWTx8rF+/XvPmzdN5553X7PeEQiEFg8FGS1eS3ztFfdJ8KjVpdkNNC59pAwAADpDQ1TS7du1SNBpVbm5uo/bc3FytXr26yWMuvfRS7dq1S6eddpqMMaqtrdWVV1550NM0M2fO1G233ZZIaZ3KsiwdPzBL5RtS7YaarhWWAADoTjr8appFixbpzjvv1AMPPKDly5fr+eef12uvvabf/OY3zR4zffp0lZWVxZctW7Z0dJkJOyonXeUmxV5hZAQAgFZLaGSkb9++crvdKikpadReUlKi/v37N3nMLbfcossuu0w//elPJUkjRoxQZWWlfvazn+nmm2+Wy3VgHvL7/fL7/YmU1unye6UoqLrTNCFGRgAAaK2ERkZ8Pp/GjBmjhQsXxttisZgWLlyogoKCJo+pqqo6IHC43W5JatkD6bqo/N6pKjf1p2kYGQEAoLUSvgPrtGnTNGXKFI0dO1bjxo3TrFmzVFlZqcsvv1ySNHnyZA0cOFAzZ86UJE2cOFH33nuvRo8erfHjx2vdunW65ZZbNHHixHgo6Y4O65Wqt8RpGgAA2irhMHLxxRdr586dmjFjhoqLizVq1Ci9/vrr8UmtmzdvbjQS8stf/lKWZemXv/yltm7dqn79+mnixIm644472q8XDsjJ8CtYdzVNrLqMW9kCANBKlukG50qCwaCysrJUVlamzMxMp8uRZD+v5opb7tRfvXcrknOCvP/zrtMlAQDQpbT09zf/h76VXC5LVsD+wcY4TQMAQKsRRtrAl1J3B9ZwlbOFAADQjRFG2iCQZocRVy1hBACA1iKMtEFKun2axl1bLXX9qTcAAHRJhJE28KWkS5Jcikm1IYerAQCgeyKMtIG/LoxIkiKcqgEAoDUII22QnhJQyNTdqiVc6WwxAAB0U4SRNsgIeFWtumfoMDICAECrEEbaIN3vUVV9GGFkBACAViGMtEFGwKNqw8gIAABtQRhpg4yAt8HICGEEAIDWIIy0QUbAoyoF7JUIp2kAAGgNwkgbpPs9qjKMjAAA0BaEkTZI9bnjp2lMuMLhagAA6J4II22Q4nOruu40TW0Np2kAAGgNwkgbpHjd8dM0tTWMjAAA0BqEkTbwuF0KWXUjIyHCCAAArUEYaaOIO0WSFOM0DQAArUIYaaPa+jDCHVgBAGgVwkgbRT12GDEhwggAAK1BGGmjmDfVfsPICAAArUIYaaOYJ81+w7NpAABoFcJIW9WNjLhqCSMAALQGYaStvPbIiIuREQAAWoUw0kaW3x4ZcUerHa4EAIDuiTDSRpY/XZLkIYwAANAqhJE28vjt0zTeaLVkjMPVAADQ/RBG2sgdsEdGXIpJtTUOVwMAQPdDGGkjb0r6vpUwk1gBAEgUYaSNUvw+1RivvRLhxmcAACSKMNJGKV63quS3VxgZAQAgYYSRNkrze1SlgL3CLeEBAEgYYaSNUnxuVZm6kRFO0wAAkDDCSBul+TycpgEAoA0II22U6nOr2tSdpmFkBACAhBFG2ijV51ZlfGSEMAIAQKIII22U6vOoui6MGMIIAAAJI4y0Uarfraq60zTREGEEAIBEEUbaKLXBfUZqayocrgYAgO6HMNJGHrdLNa4USVJtddDhagAA6H4II+2g2p0hSYpV7XW4EgAAuh/CSDuocWfabwgjAAAkrFVhZPbs2RoyZIgCgYDGjx+vJUuWHHT/0tJSTZ06VQMGDJDf79fRRx+tefPmtargrqjGmyVJsmpKnS0EAIBuyJPoAXPnztW0adP00EMPafz48Zo1a5YmTJigL774Qjk5OQfsHw6HdfbZZysnJ0fPPfecBg4cqE2bNik7O7s96u8SIr4sqVpyhUqdLgUAgG4n4TBy77336oorrtDll18uSXrooYf02muv6dFHH9VNN910wP6PPvqo9uzZow8++EBer1eSNGTIkLZV3cVEfPbIiIcwAgBAwhI6TRMOh7Vs2TIVFhbu+wCXS4WFhVq8eHGTx7z88ssqKCjQ1KlTlZubq+HDh+vOO+9UNBpt9ntCoZCCwWCjpSsz/mxJkjdcJsVizhYDAEA3k1AY2bVrl6LRqHJzcxu15+bmqri4uMlj1q9fr+eee07RaFTz5s3TLbfconvuuUe33357s98zc+ZMZWVlxZf8/PxEyux0sUC2JMmlmBQud7YYAAC6mQ6/miYWiyknJ0cPP/ywxowZo4svvlg333yzHnrooWaPmT59usrKyuLLli1bOrrMNvEGUlVtfPZK1R5niwEAoJtJaM5I37595Xa7VVJS0qi9pKRE/fv3b/KYAQMGyOv1yu12x9uOPfZYFRcXKxwOy+fzHXCM3++X3+9PpDRHpfo82mWylG/tlCp2SL2HOl0SAADdRkIjIz6fT2PGjNHChQvjbbFYTAsXLlRBQUGTx5x66qlat26dYg3mUqxZs0YDBgxoMoh0R+l+j7art70S3OpsMQAAdDMJn6aZNm2aHnnkEf3tb3/TqlWrdNVVV6mysjJ+dc3kyZM1ffr0+P5XXXWV9uzZo2uuuUZr1qzRa6+9pjvvvFNTp05tv144LCvFq2JTH0a2OVsMAADdTMKX9l588cXauXOnZsyYoeLiYo0aNUqvv/56fFLr5s2b5XLtyzj5+fmaP3++rrvuOp1wwgkaOHCgrrnmGt14443t1wuHZaV6tc30sVcYGQEAICGWMcY4XcShBINBZWVlqaysTJmZmU6Xc4B/f1asD56+Q7/yPiEd9x3poiecLgkAAMe19Pc3z6ZpB9mpvn0jI3s3OVsMAADdDGGkHWSnerXODLRXdq3hxmcAACSAMNIOslO82mRyFTIeKVIllTI6AgBASxFG2kFmildRubXe5NkNO1c7WxAAAN0IYaQdBLxupfrc+twMthu+WupsQQAAdCOEkXaSmxnQktgwe2XTB84WAwBAN0IYaSf9MwP6sD6MbF0qRaqdLQgAgG6CMNJOBmQHtNH0V7m/vxQNS+vecLokAAC6BcJIOxmQFZBk6ZPMM+yGz15wshwAALoNwkg76Z+VIkl6y3Oa3fDFv6TqvQ5WBABA90AYaSdH9EuTJP27dKCUO9y+38jyvztcFQAAXR9hpJ0ck5shSdq8t1qhMVfYjUselqIRB6sCAKDrI4y0kz7pfvVN90uSVvf7ppSWI5VtkVY86XBlAAB0bYSRdjR8oP1EwqVbq6Wv/a/d+PbdXOYLAMBBEEba0SlH2E/u/WDdLmnMj6SsfKl8m/TRX5wtDACALoww0o5OOaKvJGnx+t2qirmlr99ob3j3HqmmzMHKAADouggj7ej4vEwN6p2qqnBUr68slkZeIvU9xr7E9/37nC4PAIAuiTDSjizL0vfGHCZJemLxJhmXWyq81d64+AEpuN3B6gAA6JoII+3sB+PyFfC6VLSlVG+v2Skdc56UP16qrZbevsvp8gAA6HIII+0sJyOgy04eLEm647VVCkeNVHibvXH536WdaxysDgCArocw0gGmnnmk+qT5tHZHhR5+50tpcIE9QmKi0pu/dro8AAC6FMJIB8hO9emWbx0nSfrjm+u0bkeFdNYMyXJJq16RtnzkcIUAAHQdhJEO8p1Refra0f0Uro3phuc+VrTvMGnUpfbGBTMkY5wtEACALoIw0kEsy9LMC0co3e/R8s2levyDjdIZ0yVPQNr8gbT2306XCABAl0AY6UADs1M0/bxhkqTfzV+tjZFe0vj/tje+8SspFnWuOAAAugjCSAe7dNwgnXJEH9VEYrrhn58odsp1UiBL2vG59MkzTpcHAIDjCCMdzLIs3XXhCUrxurVkwx499UmZdNp19sZ372F0BACQ9AgjnWBQn1Td+M1jJEkz/7VaXx05yR4d2b1WWv2qw9UBAOAswkgnmVwwROOG9FZVOKqbXt0gc9IV9oZ37+XKGgBAUiOMdBKXy9Jvv3eC/B6X3lu3Sy/4JkqeFGl7kbT+LafLAwDAMYSRTjS0b5r+9xz7dM2MN0pUNWKSveHdex2sCgAAZxFGOtmPTxuqEw7LUkWoVn+s/qbk8kgb35W2f+J0aQAAOIIw0sncLksz6m4V//DHYQUPP8/esOTPDlYFAIBzCCMOGDukt84b0V8xI80Knmk3fvqcVLnb2cIAAHAAYcQhN35zmLxuS49uzlF57+FSbY20/G9OlwUAQKcjjDhkcJ80TS4YIsnSXyMT7MaP/iJFa50sCwCATkcYcdBVZxyhFK9bD+w8QWF/Hym4VfriNafLAgCgUxFGHNQ33a/JBYMVllcvus6yG5dxqgYAkFwIIw772dcOV6rPrftLC+yGL9+USjc7WxQAAJ2IMOKwPul+TS4Yoi0mVx97R0ky0oqnnC4LAIBOQxjpAn5y2lD5PC79pfI0u2HFkzzNFwCQNAgjXUC/DL++e+Jhmh87SRWuDCn4lX26BgCAJNCqMDJ79mwNGTJEgUBA48eP15IlS1p03Jw5c2RZli644ILWfG2PdsXpQxWxvJobrhsd4Z4jAIAkkXAYmTt3rqZNm6Zbb71Vy5cv18iRIzVhwgTt2LHjoMdt3LhR//u//6vTTz+91cX2ZIf3S9fZx+ZqbvQMu+GLf0kVOx2tCQCAzpBwGLn33nt1xRVX6PLLL9dxxx2nhx56SKmpqXr00UebPSYajWrSpEm67bbbdPjhh7ep4J7sZ187XGtMvopiR0qxWumTuU6XBABAh0sojITDYS1btkyFhYX7PsDlUmFhoRYvXtzscb/+9a+Vk5Ojn/zkJ62vNAmMHdJbo/Kz9Uz063bDir9LxjhbFAAAHSyhMLJr1y5Fo1Hl5uY2as/NzVVxcXGTx7z33nv661//qkceeaTF3xMKhRQMBhstyWJywWC9Ei1QSD5p52pp63KnSwIAoEN16NU05eXluuyyy/TII4+ob9++LT5u5syZysrKii/5+fkdWGXXct6IAfKlZeu16Di7YcXfnS0IAIAOllAY6du3r9xut0pKShq1l5SUqH///gfs/+WXX2rjxo2aOHGiPB6PPB6PnnjiCb388svyeDz68ssvm/ye6dOnq6ysLL5s2bIlkTK7tYDXrYtPytez9adqVv5TClc5WxQAAB0ooTDi8/k0ZswYLVy4MN4Wi8W0cOFCFRQUHLD/sGHD9Omnn6qoqCi+fPvb39aZZ56poqKiZkc8/H6/MjMzGy3JZNLJg7XEHKstsX5SKCitftXpkgAA6DCeRA+YNm2apkyZorFjx2rcuHGaNWuWKisrdfnll0uSJk+erIEDB2rmzJkKBAIaPnx4o+Ozs7Ml6YB27DMwO0VnHTtAz37xdU1zPWefqjnhIqfLAgCgQyQcRi6++GLt3LlTM2bMUHFxsUaNGqXXX389Pql18+bNcrm4sWtbTS4Yohs/P13Xev4p14Z3pL0bpV5DnC4LAIB2ZxnT9a8dDQaDysrKUllZWdKcsjHG6Kx739avSn+pr7k/lb5+o3Tm/zldFgAALdbS398MYXRRlmVp8smD9VzdRFZT9JQUizlcFQAA7Y8w0oVdOOYwvesZrzKTKqvsK2nD206XBABAuyOMdGGZAa/OGz1UL0VPtRtWPOlsQQAAdADCSBc3uWBI/J4jZtUrUvVehysCAKB9EUa6uGP6Zyh18BitiuXLiobsm6ABANCDEEa6gcmnDNWz0TMkSbHlnKoBAPQshJFu4Jzjc/V+6pkKG7dc21dIxSudLgkAgHZDGOkGvG6Xzht/ghbGTrQbip5ytiAAANoRYaSbuGRcvv4ZO0OSVFs0R6oNO1sQAADthDDSTeRkBpR63ASVmGx5avZIa153uiQAANoFYaQbmXLaEXo+erokqeajvzlcDQAA7YMw0o2MGdxbqwd8W5Lk2/CmFNzucEUAALQdYaSbufDsM/VR7Gi5FFP14oedLgcAgDYjjHQzXzuqr97IvFCSZH30FylU4XBFAAC0DWGkm7EsS6POuUzrY/0VqA2q6j+POl0SAABtQhjphiYMH6iX078vSap9749c5gsA6NYII92Qy2XpxIlXqsRkKzOyU3s+4MoaAED3RRjppk4fNlD/zr5YkmS981spUuNwRQAAtA5hpJuyLEsnXvgLbTO91at2pzbNv9/pkgAAaBXCSDd2/OBcLc6/QpKUteyPilSVOlsQAACtQBjp5s76wbXaoDxlm6A+mfsbp8sBACBhhJFuLjs9VdvH3iBJGr7xb9ry5ecOVwQAQGIIIz1AwXlT9Kl/tPxWRNvmXqfaaMzpkgAAaDHCSA9guVzKufg+RYxb48P/0bwX/u50SQAAtBhhpIfIPXykNhx5mSRp5Kd3aOXGYocrAgCgZQgjPchR3/+19nj6abBVos+evF5l1RGnSwIA4JAIIz2IFciS7zv3SZK+H3lFD/z9aRljHK4KAICDI4z0MOkjzteeo74rl2X0/a/u0qOLVjldEgAAB0UY6YF6/9fvVe3vqyNd2+R+8zZ98OUup0sCAKBZhJGeKLW3AhfOliT9yP265j75Z23cVelwUQAANI0w0kNZx3xTteOukiT9KvaAbnpsHhNaAQBdEmGkB/Occ5siuSPVy6rQtPLf6ZqnlnBDNABAl0MY6ck8fnkvekxRb7rGub5Q4cZ79OtXP+cKGwBAl0IY6en6HCH39/4qI0s/9CxUbMlf9MCiL52uCgCAOMJIMjjmm7IKb5Uk/crzNy1e8JzmfrTZ4aIAALARRpLFqddKIy6Sx4rpz9579cwL/9SCz0ucrgoAAMJI0rAs6Tt/kjniG0qzQnrUe7fuf/oFvfXFDqcrAwAkOcJIMvH4ZV38pMxh45RlVekJ96/10N+fZoQEAOAowkiy8aXJmvSsYoedpGyrUo+779BzT/1ZLxVtdboyAECSIowko5RsuSa/pNiRhUqxwvqz9x599dxN+sO/P1csxmW/AIDORRhJVr40uS6Zo9hJV0iSpnpe1pnv/VC3//UZ7a0MO1wcACCZEEaSmdsr1/m/l773mMKedI1yfan/++pKLbhnshZ//JnT1QEAkkSrwsjs2bM1ZMgQBQIBjR8/XkuWLGl230ceeUSnn366evXqpV69eqmwsPCg+8MBwy+U7+cfqWzoefJYMV0U+5dGP/91vfWHH6l4/SdOVwcA6OESDiNz587VtGnTdOutt2r58uUaOXKkJkyYoB07mr5EdNGiRbrkkkv01ltvafHixcrPz9c555yjrVuZMNmlZOYpa8o/FLrkBW1OG6GAFdGZZS+o/xOna/09Z2nvh09LoQqnqwQA9ECWSfBBJePHj9dJJ52kP/3pT5KkWCym/Px8/b//9/900003HfL4aDSqXr166U9/+pMmT57cou8MBoPKyspSWVmZMjMzEykXrWGMNi99TTve+KNG1yyR27L/iIQtv6qGFCr7pIulI74h+TMcLhQA0JW19Pe3J5EPDYfDWrZsmaZPnx5vc7lcKiws1OLFi1v0GVVVVYpEIurdu3ciX43OZFkadNK3lD/2fC0t+kSbFj6kMcGFGuoqkW/Da9KG1xS1PDL5BfIcc4501NlSv2H2jdUAAEhQQmFk165dikajys3NbdSem5ur1atXt+gzbrzxRuXl5amwsLDZfUKhkEKhUHw9GAwmUibaiWVZOmn0SJ00+kEt3bBbz729QNlfvqqzrSUa4iqRNr9rLwtuUSy9v1xDTpUGnyINPpVwAgBosYTCSFvdddddmjNnjhYtWqRAINDsfjNnztRtt93WiZXhUMYO7aOxQ3+g3RX/peeWfaUPl36kQXve15muIp3sWiV/RbG08p/2IkmpfaT8k6WBJ0p5o+0lldEwAMCBEpozEg6HlZqaqueee04XXHBBvH3KlCkqLS3VSy+91Oyxv//973X77bfrjTfe0NixYw/6PU2NjOTn5zNnpItZW1KuVz7Zrn8XbVD23k80zlqt8a5VOtG1VilWE/cq6TXUDiUDT5T6j5ByjpfS+3V+4QCATtHSOSOtmsA6btw43X///ZLsCayDBg3S1Vdf3ewE1rvvvlt33HGH5s+fr5NPPjmRr5PEBNauzhijL3dW6s3VJXpz9Q59vHGnjjVf6kTXWp3gWq+RrvUabDXz/Ju0flLOsXYwyT1OyjlO6ncMk2MBoAfosDAyd+5cTZkyRX/+8581btw4zZo1S88884xWr16t3NxcTZ48WQMHDtTMmTMlSb/97W81Y8YMPf300zr11FPjn5Oenq709PR27Qy6hmBNRO+u2aW31+zQ4vW7tWVPtbJUoRGuDTrB+lKj3Bs0wrtV/aPbZamZP37puVLvI6Q+dUv9+96HS96Uzu0QAKBVOuRqGkm6+OKLtXPnTs2YMUPFxcUaNWqUXn/99fik1s2bN8vl2nf7kgcffFDhcFjf+973Gn3Orbfeql/96leJfj26gcyAV+efMEDnnzBAkrRlT5UWf7lbH3x5tJ77crceKA9JYSlFNTrK2qpjXFt0UkqxRvq3alBko1LCu6WKEnvZ/MGBX5CeK2XlS1mH1S37vU/tzeRZAOhGEh4ZcQIjIz2HMUbrd1Vq2aa9WrF5r5Zt2qs1JY1vppapSh3pKdHJWaUanbpLR3h2KDeyVakVG2XVlB36SzwBO7Ck50rpOVJG/wbrDdrS+klubwf1FADQYadpnEAY6dnKqiMq2lIaDyifbi1TaVXkgP1cltHI3jGN61Wp49PKNNRXqoHWLmVFSuQObpXKvrJHUxKR0su+8ie1j5Ta1x5VSe0jpfVt0N5g8Wcw6gIALUQYQbdljNHW0mqt3BrUZ9vKtHJrmT7bFtSO8lCT+7ssaUifNB2Zk66j+/o0LK1cg/1VOsxTpqzoXrkq6075VOyQyovt18odUqw28eLcvn3BJKVXgyV7v/VeUqBBmy+NEAMg6RBG0OPsCNZoVXG51paUa92OCq3dUaE1JeUqr2k+VPg9Lg3qnarBfVI1uE+aBvdJVX6vVOVl+TXQX6X02jKpanfdsqvudY9UuatBe90SqWp98S5vy0LL/uEmkCW53K3/XgBwEGEEScEYox3lIa0tsYPJhl2V2rSnSpt3V2rL3mpFYwf/450R8CgvK0V52QHlZacoLztFA+teB2QFlJPpl99TFwbCVY2DS3WpVL23wWuDpaaurWqPFDvwlFNCfBl2OAlk2cElkNVgvam2BtsYkQHgIMIIkl5tNKZtpTXauNsOKJvqgsrWvdXaVlbd5LyUpvRK9Sonww4mORkB5Wb6lZPhV27mvrZ+GX4FvE2MYBhjj6gcKrQ0Wsrs13B5238ILk+CIWa/Nib4AmgDwghwCJWhWm0vq9a20hptK63WttJqba1/X1at7aU1CkdjLf687FSvcjLscNIn3ac+aX71zfCpb5rfXk/3q0+aT33T/UrxteDUSzQi1ZTZS3WpHVxqyva9Vu+3vn9ba+bE7M+blthITMP9fBlSg8v8ASQfwgjQRsYYlVZFtKM8pJJgTfx1537rO8pDCte2PLRIUprPbYeT+tCS7lOfdDuo9En3q2+aHV56pXnVK9UnrzvBX+rxEZnSfUGlpSGmpkwKtcfDKS376iN/phTIPMRrVtPt/gzmzADdGGEE6CTGGAWra1VSXqMdwZB2lNdod0VYuypD2l0R1u6KkHZXhrW7IqydFYkHF8me29Ir1adeaT71TvXue5/ms9+nehutZ6d6Ew8wDUVr7UDSZIhpQbCJNn3lU6v4MloYZpoJPv5Myd2pzwQFUIcwAnRBxhhVhGrtkFIZ0q6KcKPAsrMiZL+vCGt3ZVh7q8Jq7d/QjIDnwLBSF2J6pfrUO82r7Lrgkp1ivzY576U1IjV1YSYohcrqXoPNvDazvT0DjTet6VGXhoHlUO0eP5OBgQQRRoAeIBozClZHtLfKDiZ7KiPaWxdS9lSFtbfSbittsF5aHWl1gPF7XMpO9SorxQ4oWaleZad47cCS6rPbG2zPTvUqK9WrDL9HVnv/oq4N7QsnzQaZ4L7TSk1tr61uv3pc3v0CS1YT4eUQAYd5NEgyHfZsGgCdx+2y7JGMNF+Lj6kPMPvCSlilVY3X91bVBZzKsMqqIyqtjigaMwrVxlQSDKkkmNiohNtl1QUUrzLrw0tK4wBTPwLTcHtWilee5k4nefxSej97aa3asBQqbzz6EipvOuQ0at/vvYx9iXb9pd1t4WsuvNS/zzp0wPH421YD0MUQRoAeplGAaeHv8frTR6VVEZVV20tpVUSl1eF4W2lVON7ecHtNJKZozGhPXdBJVJrPray6EJPV1FI3ElO/PTOwb5vPc4hRBo9P8vSR0vokXFdcLCaFKxoEl/IGIzLlTbTvH3Tq2qN1P5twed1l21tbX5Pb30RgOdhITRMBx5fOKA26DMIIAFmWpYyAVxkBr/ITPLYmEm0cYKrsU0VlDcJMaXVEwf0CTv2dcyvDUVWGo9pWVpNw3Sled6PgcmCg8ewLM4HG+7V4fozLZf8SD2RKWQmXuE+kZr8g09RoTHPtde/DdQ+VjIakqpB9871Ws5oJL1kHWbIbr3taPmIHHAxhBECbBLxuBbxu5WYGEjquNhpTsKY2HmTK6gJLU+/3X+qDTHUkqupIVMXBxIOM3+NqciSmqRGa/dsCXlfic2S8AXtpy2mnWDSx0Zj4+7LG7bFaScZuD5VJrb2S25t6iPByiEDDTfVQhzACwBEet0u96y5HTlQ0ZlRe0zC41DYZWpoKNMEae4JvqDamHeWhZh/AeDA+t6suoHgOGmiaCjapPnfrJ/u63HXPLcpu3fFS3T1oqg8SZOov6T7IUn8fmkiVvZRvb10t9TfVa1WgySTM9CCEEQDdjttl1V2WnHiQicWMykO18aBysBGYA0ZramoVjRmFozHtqghpV0XiQcbrtuKnjJqbJ5NZF3L2357eHlctWZbkS7WXjNzWfUaj+9C0Yql/1EGk0l7Kt7WuDm/afncHTiDMcP+ZLoX/EgCSiqvuyp+slMTnx9RP9A3W1KqsqmWnlBpuq40ZRaLGvgleKyb7ul2WMgOehE4p1bdl+D1yudrp8mu3R0rtbS+tEQ8zpa0MM3VzZ+rDTLCVk4F96U3PhWl2ybSP8daFOV86ozPthDACAC3UcKLvwOyUhI41xqi6wWTfhmHm4IHGHsUJR+2rluzLshN/ErTLkjICBzul1PQpp6wUu7/u9goyUjuEmUjdxN7SJu4O3IIlUml/TrjCXlobZiT7/jO+tH2Lty6k+FIbv/el2SM5vrR9QaY+1HhTJU9A8qbUvaba84s8KUlzxRNhBAA6gWVZSvV5lOrzaEBWYkFGanzVUlNhpqlAE6ybV1MTiSlmFG9PvHYp3d98WMk82CmngKf5e8m0ltvb9jBT09zITFNt9UvQDjLhyn0PooxF6o4pbZ++7c/ts0OJN7BfYGkiuDS7T92rx1+31O3n9jVoD9g/T4dGeggjANANtPaqJckOMsGaJkZfquyRl/0DTbDB5OCqcFTGSOU1tSqvqdVXexO/q219kMlM8Soj4FGG32O/BrxKD+x736g9/t6jdH87Bxq31773TFvuP1MbtkdVIlV2OAlXNn4fX6+QwlWN34cr94Wa+m21Nfbl37XV++5JI9nvo2H7qqeO9uP50qCTO/57mkAYAYAerj7I5GQkHmTCtbFG4aTRCMz+p5pq9p1WKquOqCJkjx5UhGpVEarV1tLW354/1eduEFC88aCS4d8XaNL9HqX5PUr1uZXm8yjVb7+m+d1K8XmU5nMr1ec59M3yWsLjkzy9JbVydOZgYtHG4SRSt9TWtOw1Ul13XM2+tmjYfl9bYz9qIf4a2rfudu6+MYQRAECzfB6X+qb71Tc98VvQN3UvmYqaWpXX2PeKKQ/Z7yvqRl3KQ/veB2tqVRGyTzFJUlU4qqpwtFWXYu/P67ZPmaX53Er17wspaf79Xuu2p3jdSvG65fe67Pc+O9yleN0KeF3xsJdS99rm+TUu9745KJ3F4cfUEUYAAB2iLfeSqReujakyVB9Q7NGW8rpAs+/9vnU7tNSqMrTfaziqcK0dbCJR0+r5My3hc7viISXFVx9k3Eqpb6sLLQ0Djd/jls/jkt/j2u/VDkF+t0t+r0s+t7vu1V5veJzHZbX+0m+Hn0hNGAEAdFk+j0s+T2IPi2xOJBo7aFipCu33GrZPL9VEoqqJxFQdjqqmNqrqcFShWnu9OhJVTcRerxeOxhSuGxXqTJZl31nYDiruRoGmPrDsW+w2r9uyf8Zuty4/dYjye6d2as31CCMAgKTgdbuUlWI/BqC9xeqeel0fTqoj9aElqupwLN5WE19i8ccZ1ETsUZtQ3RKujda9xhq8Nt0Wie47vWKMVBOJ2ae2WhGEvjVyAGEEAIDuyuWy7FMyvhY+gLGdxOruCByKxBSKRhWKxOLr9ut+IaZun/q2cHTf64CsxCc4txfCCAAA3ZTLZSngctc9hbr73g02OW7tBgAAuizCCAAAcBRhBAAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACO6hZP7TXGSJKCwaDDlQAAgJaq/71d/3u8Od0ijJSXl0uS8vPzHa4EAAAkqry8XFlZWc1ut8yh4koXEIvFtG3bNmVkZMiyrHb73GAwqPz8fG3ZskWZmZnt9rldVbL1V0q+PtPfno3+9mw9sb/GGJWXlysvL08uV/MzQ7rFyIjL5dJhhx3WYZ+fmZnZY/7Dt0Sy9VdKvj7T356N/vZsPa2/BxsRqccEVgAA4CjCCAAAcFRShxG/369bb71Vfr/f6VI6RbL1V0q+PtPfno3+9mzJ1t+GusUEVgAA0HMl9cgIAABwHmEEAAA4ijACAAAcRRgBAACOSuowMnv2bA0ZMkSBQEDjx4/XkiVLnC7pkGbOnKmTTjpJGRkZysnJ0QUXXKAvvvii0T41NTWaOnWq+vTpo/T0dH33u99VSUlJo302b96s888/X6mpqcrJydH111+v2traRvssWrRIJ554ovx+v4488kg9/vjjHd29Q7rrrrtkWZauvfbaeFtP6+/WrVv1wx/+UH369FFKSopGjBihpUuXxrcbYzRjxgwNGDBAKSkpKiws1Nq1axt9xp49ezRp0iRlZmYqOztbP/nJT1RRUdFon08++USnn366AoGA8vPzdffdd3dK/xqKRqO65ZZbNHToUKWkpOiII47Qb37zm0bPseju/X3nnXc0ceJE5eXlybIsvfjii422d2b/nn32WQ0bNkyBQEAjRozQvHnzOrW/kUhEN954o0aMGKG0tDTl5eVp8uTJ2rZtW4/s7/6uvPJKWZalWbNmNWrvTv3tMCZJzZkzx/h8PvPoo4+azz77zFxxxRUmOzvblJSUOF3aQU2YMME89thjZuXKlaaoqMicd955ZtCgQaaioiK+z5VXXmny8/PNwoULzdKlS83JJ59sTjnllPj22tpaM3z4cFNYWGhWrFhh5s2bZ/r27WumT58e32f9+vUmNTXVTJs2zXz++efm/vvvN26327z++uud2t+GlixZYoYMGWJOOOEEc80118Tbe1J/9+zZYwYPHmx+9KMfmQ8//NCsX7/ezJ8/36xbty6+z1133WWysrLMiy++aD7++GPz7W9/2wwdOtRUV1fH9/nmN79pRo4caf7zn/+Yd9991xx55JHmkksuiW8vKyszubm5ZtKkSWblypXmH//4h0lJSTF//vOfO7W/d9xxh+nTp4959dVXzYYNG8yzzz5r0tPTzX333ddj+jtv3jxz8803m+eff95IMi+88EKj7Z3Vv/fff9+43W5z9913m88//9z88pe/NF6v13z66aed1t/S0lJTWFho5s6da1avXm0WL15sxo0bZ8aMGdPoM3pKfxt6/vnnzciRI01eXp75wx/+0G3721GSNoyMGzfOTJ06Nb4ejUZNXl6emTlzpoNVJW7Hjh1Gknn77beNMfZfdq/Xa5599tn4PqtWrTKSzOLFi40x9l8el8tliouL4/s8+OCDJjMz04RCIWOMMTfccIM5/vjjG33XxRdfbCZMmNDRXWpSeXm5Oeqoo8yCBQvM17/+9XgY6Wn9vfHGG81pp53W7PZYLGb69+9vfve738XbSktLjd/vN//4xz+MMcZ8/vnnRpL56KOP4vv861//MpZlma1btxpjjHnggQdMr1694v2v/+5jjjmmvbt0UOeff7758Y9/3KjtwgsvNJMmTTLG9Lz+7v/LqjP7d9FFF5nzzz+/UT3jx483//3f/92ufWzoYL+c6y1ZssRIMps2bTLG9Mz+fvXVV2bgwIFm5cqVZvDgwY3CSHfub3tKytM04XBYy5YtU2FhYbzN5XKpsLBQixcvdrCyxJWVlUmSevfuLUlatmyZIpFIo74NGzZMgwYNivdt8eLFGjFihHJzc+P7TJgwQcFgUJ999ll8n4afUb+PUz+fqVOn6vzzzz+gpp7W35dfflljx47V97//feXk5Gj06NF65JFH4ts3bNig4uLiRrVmZWVp/PjxjfqbnZ2tsWPHxvcpLCyUy+XShx9+GN/na1/7mnw+X3yfCRMm6IsvvtDevXs7uptxp5xyihYuXKg1a9ZIkj7++GO99957OvfccyX1vP7urzP711X+jO+vrKxMlmUpOztbUs/rbywW02WXXabrr79exx9//AHbe1p/Wyspw8iuXbsUjUYb/XKSpNzcXBUXFztUVeJisZiuvfZanXrqqRo+fLgkqbi4WD6fL/4Xu17DvhUXFzfZ9/ptB9snGAyqurq6I7rTrDlz5mj58uWaOXPmAdt6Wn/Xr1+vBx98UEcddZTmz5+vq666Sj//+c/1t7/9rVG9B/uzW1xcrJycnEbbPR6PevfundDPpDPcdNNN+sEPfqBhw4bJ6/Vq9OjRuvbaazVp0qRGtfSU/u6vM/vX3D5O9r+mpkY33nijLrnkkviD4Xpaf3/729/K4/Ho5z//eZPbe1p/W6tbPLUXTZs6dapWrlyp9957z+lSOsyWLVt0zTXXaMGCBQoEAk6X0+FisZjGjh2rO++8U5I0evRorVy5Ug899JCmTJnicHXt75lnntFTTz2lp59+Wscff7yKiop07bXXKi8vr0f2F/tEIhFddNFFMsbowQcfdLqcDrFs2TLdd999Wr58uSzLcrqcLi0pR0b69u0rt9t9wBUXJSUl6t+/v0NVJebqq6/Wq6++qrfeekuHHXZYvL1///4Kh8MqLS1ttH/DvvXv37/JvtdvO9g+mZmZSklJae/uNGvZsmXasWOHTjzxRHk8Hnk8Hr399tv64x//KI/Ho9zc3B7V3wEDBui4445r1Hbsscdq8+bN8Trra2to//7u2LGj0fba2lrt2bMnoZ9JZ7j++uvjoyMjRozQZZddpuuuuy4+CtbT+ru/zuxfc/s40f/6ILJp0yYtWLAgPioi9az+vvvuu9qxY4cGDRoU//dr06ZN+sUvfqEhQ4bE6+wp/W2LpAwjPp9PY8aM0cKFC+NtsVhMCxcuVEFBgYOVHZoxRldffbVeeOEFvfnmmxo6dGij7WPGjJHX623Uty+++EKbN2+O962goECffvppo78A9f8g1P8iLCgoaPQZ9ft09s/nrLPO0qeffqqioqL4MnbsWE2aNCn+vif199RTTz3gUu01a9Zo8ODBkqShQ4eqf//+jWoNBoP68MMPG/W3tLRUy5Yti+/z5ptvKhaLafz48fF93nnnHUUikfg+CxYs0DHHHKNevXp1WP/2V1VVJZer8T9DbrdbsVhMUs/r7/46s39d5c94fRBZu3at3njjDfXp06fR9p7U38suu0yffPJJo3+/8vLydP3112v+/PnxOntKf9vE6Rm0TpkzZ47x+/3m8ccfN59//rn52c9+ZrKzsxtdcdEVXXXVVSYrK8ssWrTIbN++Pb5UVVXF97nyyivNoEGDzJtvvmmWLl1qCgoKTEFBQXx7/aWu55xzjikqKjKvv/666devX5OXul5//fVm1apVZvbs2Y5f2luv4dU0xvSs/i5ZssR4PB5zxx13mLVr15qnnnrKpKammieffDK+z1133WWys7PNSy+9ZD755BPzne98p8lLQUePHm0+/PBD895775mjjjqq0aWCpaWlJjc311x22WVm5cqVZs6cOSY1NbXTL+2dMmWKGThwYPzS3ueff9707dvX3HDDDT2mv+Xl5WbFihVmxYoVRpK59957zYoVK+JXj3RW/95//33j8XjM73//e7Nq1Spz6623dsilnwfrbzgcNt/+9rfNYYcdZoqKihr9G9bwSpGe0t+m7H81TXfrb0dJ2jBijDH333+/GTRokPH5fGbcuHHmP//5j9MlHZKkJpfHHnssvk91dbX5n//5H9OrVy+Tmppq/uu//sts37690eds3LjRnHvuuSYlJcX07dvX/OIXvzCRSKTRPm+99ZYZNWqU8fl85vDDD2/0HU7aP4z0tP6+8sorZvjw4cbv95thw4aZhx9+uNH2WCxmbrnlFpObm2v8fr8566yzzBdffNFon927d5tLLrnEpKenm8zMTHP55Zeb8vLyRvt8/PHH5rTTTjN+v98MHDjQ3HXXXR3et/0Fg0FzzTXXmEGDBplAIGAOP/xwc/PNNzf6xdTd+/vWW281+Xd2ypQpnd6/Z555xhx99NHG5/OZ448/3rz22mud2t8NGzY0+2/YW2+91eP625Smwkh36m9HsYxpcKtDAACATpaUc0YAAEDXQRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAowggAAHAUYQQAADiKMAIAABxFGAEAAI4ijAAAAEcRRgAAgKP+P41KmrDAYTOmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(loss)\n",
    "# plt.plot(val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Train Loss: 1.3458067933510955, Val Loss: 1.3476974457477864\n",
      "Epoch 100/10000, Train Loss: 0.837086020706496, Val Loss: 0.8762529219730306\n",
      "Epoch 200/10000, Train Loss: 0.8284487759716663, Val Loss: 0.8679327823855624\n",
      "Epoch 300/10000, Train Loss: 0.8221351516511948, Val Loss: 0.8618082195843896\n",
      "Epoch 400/10000, Train Loss: 0.5304145608867049, Val Loss: 0.5635707278766301\n",
      "Epoch 500/10000, Train Loss: 0.49511618918103684, Val Loss: 0.517703817617728\n",
      "Epoch 600/10000, Train Loss: 0.48302550638144104, Val Loss: 0.4990279972825324\n",
      "Epoch 700/10000, Train Loss: 0.4747626363111055, Val Loss: 0.48521071516565834\n",
      "Epoch 800/10000, Train Loss: 0.46928266635909455, Val Loss: 0.47527904819147704\n",
      "Epoch 900/10000, Train Loss: 0.46565317341847773, Val Loss: 0.4682582261362202\n",
      "Epoch 1000/10000, Train Loss: 0.46313841880902873, Val Loss: 0.46325217926458523\n",
      "Epoch 1100/10000, Train Loss: 0.46122160917585747, Val Loss: 0.4595471129435496\n",
      "Epoch 1200/10000, Train Loss: 0.4594944979402648, Val Loss: 0.45655412786742616\n",
      "Epoch 1300/10000, Train Loss: 0.45736485162503554, Val Loss: 0.4535350066129692\n",
      "Epoch 1400/10000, Train Loss: 0.45298212409030847, Val Loss: 0.4485320586323376\n",
      "Epoch 1500/10000, Train Loss: 0.439109417986624, Val Loss: 0.4344953331397372\n",
      "Epoch 1600/10000, Train Loss: 0.40268119959762116, Val Loss: 0.4017957003574464\n",
      "Epoch 1700/10000, Train Loss: 0.3673023659974768, Val Loss: 0.37626230129070387\n",
      "Epoch 1800/10000, Train Loss: 0.34124872379614946, Val Loss: 0.3560315790888359\n",
      "Epoch 1900/10000, Train Loss: 0.2974741477918933, Val Loss: 0.3135092947222727\n",
      "Epoch 2000/10000, Train Loss: 0.24631624580814782, Val Loss: 0.2668381830798428\n",
      "Epoch 2100/10000, Train Loss: 0.22085844673138935, Val Loss: 0.24531111803950462\n",
      "Epoch 2200/10000, Train Loss: 0.20475571091395026, Val Loss: 0.23259894514103843\n",
      "Epoch 2300/10000, Train Loss: 0.1918318549942129, Val Loss: 0.22453590738570894\n",
      "Epoch 2400/10000, Train Loss: 0.1796809868482949, Val Loss: 0.21693392593071495\n",
      "Epoch 2500/10000, Train Loss: 0.167570903378352, Val Loss: 0.20898860084143583\n",
      "Epoch 2600/10000, Train Loss: 0.15559414427162438, Val Loss: 0.20169093277651381\n",
      "Epoch 2700/10000, Train Loss: 0.14408407924411293, Val Loss: 0.19442306322482059\n",
      "Epoch 2800/10000, Train Loss: 0.13318091559101827, Val Loss: 0.18595668179832942\n",
      "Epoch 2900/10000, Train Loss: 0.12301485279048603, Val Loss: 0.17668076393722623\n",
      "Epoch 3000/10000, Train Loss: 0.11361392381104729, Val Loss: 0.16751985016547802\n",
      "Epoch 3100/10000, Train Loss: 0.10485499594066276, Val Loss: 0.15859336133303115\n",
      "Epoch 3200/10000, Train Loss: 0.09631450764253188, Val Loss: 0.14912790177238766\n",
      "Epoch 3300/10000, Train Loss: 0.08742569519324439, Val Loss: 0.1383584603273623\n",
      "Epoch 3400/10000, Train Loss: 0.07837882499682529, Val Loss: 0.1267350537853916\n",
      "Epoch 3500/10000, Train Loss: 0.07010577055176949, Val Loss: 0.11553802262911667\n",
      "Epoch 3600/10000, Train Loss: 0.06305089668689777, Val Loss: 0.10594935230435691\n",
      "Epoch 3700/10000, Train Loss: 0.05719666223499783, Val Loss: 0.09844503076851203\n",
      "Epoch 3800/10000, Train Loss: 0.05235604557818703, Val Loss: 0.09278143098804842\n",
      "Epoch 3900/10000, Train Loss: 0.048294573460865346, Val Loss: 0.08843252936528676\n",
      "Epoch 4000/10000, Train Loss: 0.04481065746602213, Val Loss: 0.08494491766204725\n",
      "Epoch 4100/10000, Train Loss: 0.041764256316309534, Val Loss: 0.0820326182919798\n",
      "Epoch 4200/10000, Train Loss: 0.039061377301557836, Val Loss: 0.07952939057660058\n",
      "Epoch 4300/10000, Train Loss: 0.03663219427992014, Val Loss: 0.07732580079619164\n",
      "Epoch 4400/10000, Train Loss: 0.03442279241823925, Val Loss: 0.0753385076615467\n",
      "Epoch 4500/10000, Train Loss: 0.03239400133558025, Val Loss: 0.07350461906915909\n",
      "Epoch 4600/10000, Train Loss: 0.030518704609327804, Val Loss: 0.07177935507747506\n",
      "Epoch 4700/10000, Train Loss: 0.028777679331634824, Val Loss: 0.07013058581919368\n",
      "Epoch 4800/10000, Train Loss: 0.027156497920813115, Val Loss: 0.06853400080061901\n",
      "Epoch 4900/10000, Train Loss: 0.025644217307856886, Val Loss: 0.0669710424965779\n",
      "Epoch 5000/10000, Train Loss: 0.02423288821971389, Val Loss: 0.0654284419098279\n",
      "Epoch 5100/10000, Train Loss: 0.02291681927354604, Val Loss: 0.06389791706766589\n",
      "Epoch 5200/10000, Train Loss: 0.0216915115807602, Val Loss: 0.06237592888942496\n",
      "Epoch 5300/10000, Train Loss: 0.020552803757938536, Val Loss: 0.06086381075081053\n",
      "Epoch 5400/10000, Train Loss: 0.019496475116780015, Val Loss: 0.05936780473141332\n",
      "Epoch 5500/10000, Train Loss: 0.018518110622356924, Val Loss: 0.05789818062362514\n",
      "Epoch 5600/10000, Train Loss: 0.01761301582284533, Val Loss: 0.056467309825280726\n",
      "Epoch 5700/10000, Train Loss: 0.01677615864749643, Val Loss: 0.055087362886048076\n",
      "Epoch 5800/10000, Train Loss: 0.01600219747130303, Val Loss: 0.05376843420787709\n",
      "Epoch 5900/10000, Train Loss: 0.015285617196986767, Val Loss: 0.05251750486128256\n",
      "Epoch 6000/10000, Train Loss: 0.014620937110114673, Val Loss: 0.051338209139387074\n",
      "Epoch 6100/10000, Train Loss: 0.014002929036890552, Val Loss: 0.05023114814184718\n",
      "Epoch 6200/10000, Train Loss: 0.013426792905546058, Val Loss: 0.04919448452911171\n",
      "Epoch 6300/10000, Train Loss: 0.012888261660847493, Val Loss: 0.048224631522775625\n",
      "Epoch 6400/10000, Train Loss: 0.0123836325806023, Val Loss: 0.04731692526094326\n",
      "Epoch 6500/10000, Train Loss: 0.01190973921597961, Val Loss: 0.046466217008476134\n",
      "Epoch 6600/10000, Train Loss: 0.011463885849474468, Val Loss: 0.0456673489013935\n",
      "Epoch 6700/10000, Train Loss: 0.011043766286551246, Val Loss: 0.04491549656910815\n",
      "Epoch 6800/10000, Train Loss: 0.01064738386476827, Val Loss: 0.044206379702617905\n",
      "Epoch 6900/10000, Train Loss: 0.010272982776113935, Val Loss: 0.04353635665566931\n",
      "Epoch 7000/10000, Train Loss: 0.00991899456039275, Val Loss: 0.042902428871507886\n",
      "Epoch 7100/10000, Train Loss: 0.009583999276284845, Val Loss: 0.04230218416975448\n",
      "Epoch 7200/10000, Train Loss: 0.00926669863062681, Val Loss: 0.04173370572637496\n",
      "Epoch 7300/10000, Train Loss: 0.008965897769288331, Val Loss: 0.041195468156320136\n",
      "Epoch 7400/10000, Train Loss: 0.008680492803771108, Val Loss: 0.04068623567243117\n",
      "Epoch 7500/10000, Train Loss: 0.00840946187667559, Val Loss: 0.04020497137029805\n",
      "Epoch 7600/10000, Train Loss: 0.008151858297391687, Val Loss: 0.03975076202024863\n",
      "Epoch 7700/10000, Train Loss: 0.007906804852535788, Val Loss: 0.03932275949582238\n",
      "Epoch 7800/10000, Train Loss: 0.007673488787464152, Val Loss: 0.03892013796553499\n",
      "Epoch 7900/10000, Train Loss: 0.0074511571972461004, Val Loss: 0.038542064922904726\n",
      "Epoch 8000/10000, Train Loss: 0.007239112703402117, Val Loss: 0.03818768372046409\n",
      "Epoch 8100/10000, Train Loss: 0.007036709365874013, Val Loss: 0.03785610525000607\n",
      "Epoch 8200/10000, Train Loss: 0.006843348815633996, Val Loss: 0.03754640658824556\n",
      "Epoch 8300/10000, Train Loss: 0.006658476608986616, Val Loss: 0.03725763468704462\n",
      "Epoch 8400/10000, Train Loss: 0.006481578809521157, Val Loss: 0.03698881346619222\n",
      "Epoch 8500/10000, Train Loss: 0.006312178802988133, Val Loss: 0.03673895293584462\n",
      "Epoch 8600/10000, Train Loss: 0.006149834346926277, Val Loss: 0.03650705922641789\n",
      "Epoch 8700/10000, Train Loss: 0.005994134852322002, Val Loss: 0.03629214463579512\n",
      "Epoch 8800/10000, Train Loss: 0.005844698890005766, Val Loss: 0.036093237018595306\n",
      "Epoch 8900/10000, Train Loss: 0.005701171910559866, Val Loss: 0.035909388039834414\n",
      "Epoch 9000/10000, Train Loss: 0.0055632241636081, Val Loss: 0.03573967999343008\n",
      "Epoch 9100/10000, Train Loss: 0.005430548800603397, Val Loss: 0.035583231041242724\n",
      "Epoch 9200/10000, Train Loss: 0.005302860144553413, Val Loss: 0.035439198857398026\n",
      "Epoch 9300/10000, Train Loss: 0.005179892110331741, Val Loss: 0.03530678276336223\n",
      "Epoch 9400/10000, Train Loss: 0.005061396760064784, Val Loss: 0.035185224511388356\n",
      "Epoch 9500/10000, Train Loss: 0.004947142979315355, Val Loss: 0.0350738079191748\n",
      "Epoch 9600/10000, Train Loss: 0.004836915261193754, Val Loss: 0.03497185758019027\n",
      "Epoch 9700/10000, Train Loss: 0.0047305125869616365, Val Loss: 0.034878736876454174\n",
      "Epoch 9800/10000, Train Loss: 0.0046277473930518035, Val Loss: 0.03479384550843181\n",
      "Epoch 9900/10000, Train Loss: 0.004528444615653316, Val Loss: 0.034716616734755366\n",
      "Final Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=20,n_hidden_2=20,n_output=y_train.shape[1])\n",
    "\n",
    "# loss, val_metrics, final_mse = ffn.train(X_train,y_train,X_val,y_val,10000,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b1ee0fd7d0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGfCAYAAACNytIiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGnUlEQVR4nO3deXxU1f3/8dcsmZmEZBIgZAGCgGwqyCoRl7pFKVLUblKlQmlrq8VWpb9aqQpfaxXrVlpFUVtrbaugVtEK1VIQcUGRVZF9RyBhzU4mmZnz++MmkwQCMtluknk/H495zNxzz73zmWs1755777kOY4xBRERExCZOuwsQERGR2KYwIiIiIrZSGBERERFbKYyIiIiIrRRGRERExFYKIyIiImIrhRERERGxlcKIiIiI2EphRERERGylMCIiIiK2cke7wZIlS3j44YdZsWIF+/bt4/XXX+eaa645pW0//PBDLrroIvr378/q1atP+TvD4TB79+4lKSkJh8MRbckiIiJiA2MMRUVFdO7cGafzxOMfUYeRkpISBg4cyA9/+EO+9a1vnfJ2+fn5jB8/nssuu4y8vLyovnPv3r1kZWVFW6qIiIi0ALt376Zr164nXB91GBk1ahSjRo2KupCbbrqJ66+/HpfLxdy5c6PaNikpCbB+jN/vj/q7RUREpPkVFhaSlZUV+Tt+IlGHkfr461//yrZt2/jHP/7B7373u6/sHwgECAQCkeWioiIA/H6/woiIiEgr81WXWDT5BaybN2/mzjvv5B//+Adu96lln+nTp5OcnBx56RSNiIhI29WkYSQUCnH99ddz77330qdPn1PebsqUKRQUFEReu3fvbsIqRURExE5NepqmqKiI5cuXs2rVKm655RbAujPGGIPb7ea///0vl1566XHbeb1evF5vU5YmIiIiLUSThhG/38/nn39eq+3JJ59k0aJFvPrqq/To0aMpv15ERERagajDSHFxMVu2bIksb9++ndWrV9OhQwe6devGlClT2LNnDy+88AJOp5P+/fvX2j4tLQ2fz3dcu4iIiMSmqMPI8uXLueSSSyLLkydPBmDChAk8//zz7Nu3j127djVehSIiItKmOYwxxu4ivkphYSHJyckUFBTo1l4REZFW4lT/fuvZNCIiImIrhRERERGxlcKIiIiI2EphRERERGylMCIiIiK2apYH5bVUR9/7E+bIDlznTMTbZYDd5YiIiMSkmB4Z2fn+P0lY/RfWr/v8qzuLiIhIk4jpMELlI42NCdtciIiISOyK6TBiqn6+woiIiIhtYjyMWCMjhEP2FiIiIhLDYjuMVJ2msbkOERGRWBbbYaTy5xujkRERERG7xHgYqTpNo7ERERERu8R0GKm6mwaNjIiIiNgmpsNI9WkajYyIiIjYJbbDSGRkRLf2ioiI2CW2w4jmGREREbFdTIeRyAysYYURERERu8R0GImMjGimEREREdvEdBhB14yIiIjYLqbDSPXdNAojIiIidonpMIImPRMREbFdTIeRyK29aGRERETELjEeRip/vu6mERERsU1Mh5Hqn68wIiIiYpfYDiORszQKIyIiInaJ6TCiZ9OIiIjYL7bDiEOTnomIiNgtpsMIDj2bRkRExG6xHUaqKIyIiIjYJqbDSOQ0ja4ZERERsU1shxF0mkZERMRuMR1Gqh6Up2fTiIiI2CfGw4j18x0KIyIiIraJ8TBSOeuZrhkRERGxTUyHkcg1I5pnRERExDYxHUaq5xkJ2VuHiIhIDIvtMIJO04iIiNgttsOIrhkRERGxXdRhZMmSJYwZM4bOnTvjcDiYO3fuSfu/9tprXH755XTq1Am/38+IESN455136ltvozKaDl5ERMR2UYeRkpISBg4cyMyZM0+p/5IlS7j88suZP38+K1as4JJLLmHMmDGsWrUq6mIbXWRkRGFERETELu5oNxg1ahSjRo065f4zZsyotfzAAw/wxhtv8O9//5vBgwdH+/WNrHKeERRGRERE7BJ1GGmocDhMUVERHTp0OGGfQCBAIBCILBcWFjZNMZWnaYyuGREREbFNs1/A+sgjj1BcXMy11157wj7Tp08nOTk58srKymqSWkxkBlaFEREREbs0axh58cUXuffee3n55ZdJS0s7Yb8pU6ZQUFAQee3evbtpCqq8ZETXjIiIiNin2U7TzJ49mx//+Me88sor5OTknLSv1+vF6/U2eU0Oh2ZgFRERsVuzjIy89NJLTJw4kZdeeonRo0c3x1eeksh08BoZERERsU3UIyPFxcVs2bIlsrx9+3ZWr15Nhw4d6NatG1OmTGHPnj288MILgHVqZsKECfzxj38kOzub3NxcAOLj40lOTm6kn1FPemqviIiI7aIeGVm+fDmDBw+O3JY7efJkBg8ezNSpUwHYt28fu3btivR/5plnCAaDTJo0iczMzMjr1ltvbaSfUH8OzTMiIiJiu6hHRi6++OKT3gr7/PPP11pevHhxtF/RbCIzsIqIiIhtYvuvsU7TiIiI2C62w0jk5yuMiIiI2CW2w0hknhHd2isiImKXmA4jDqfLetdpGhEREdvEdBgx1UMjttYhIiISy2I6jOgCVhEREfvFdBipnmdEIyMiIiJ2iekwUj3PiEZGRERE7BLTYcQROU2jkRERERG7xHQYQSMjIiIitovxMGJdM6KREREREfvEeBip+vkKIyIiInaJ6TDi0K29IiIitovpMFJ9mkZhRERExC4xHkZ0mkZERMRuCiPoAlYRERE7KYwADt3aKyIiYpvYDiOVT+11mpDNhYiIiMSu2A4jDiuMOFAYERERsUtMhxHjdAPg1N00IiIitonpMEIkjGhkRERExC4xHUacLl0zIiIiYreYDiNVIyO6ZkRERMQ+MR1GnLqbRkRExHYxHUYcLl0zIiIiYreYDiPOqjCiSc9ERERsE9NhRHfTiIiI2C+mw4hGRkREROwX22Gk8gJWl0ZGREREbBPTYcThigPAqVt7RUREbBPTYSRymkbTwYuIiNgmpsOIo3IGVpdGRkRERGwT02HEFTlNo5ERERERu8R0GKma9EwjIyIiIvaJ6TDiciuMiIiI2C2mw4jTqXlGRERE7BbbYaTyNI2bMBhjczUiIiKxKabDSNVpGgB0e6+IiIgtYjqMOCvvpgEgHLSvEBERkRgWdRhZsmQJY8aMoXPnzjgcDubOnfuV2yxevJghQ4bg9Xrp1asXzz//fD1KbXwut6t6QWFERETEFlGHkZKSEgYOHMjMmTNPqf/27dsZPXo0l1xyCatXr+a2227jxz/+Me+8807UxTa22iMjuqNGRETEDu6v7lLbqFGjGDVq1Cn3nzVrFj169ODRRx8F4IwzzuCDDz7gD3/4AyNHjoz26xuVy1X9802oAoeNtYiIiMSqJr9mZOnSpeTk5NRqGzlyJEuXLj3hNoFAgMLCwlqvpuB2xREyVgQJVQSa5DtERETk5Jo8jOTm5pKenl6rLT09ncLCQo4ePVrnNtOnTyc5OTnyysrKapLanC4H5VinahRGRERE7NEi76aZMmUKBQUFkdfu3bub5HvcTieByjASrqg7GImIiEjTivqakWhlZGSQl5dXqy0vLw+/3098fHyd23i9Xrxeb1OXhtOJRkZERERs1uQjIyNGjGDhwoW12hYsWMCIESOa+qu/ktvpJGCsMGIqymyuRkREJDZFHUaKi4tZvXo1q1evBqxbd1evXs2uXbsA6xTL+PHjI/1vuukmtm3bxh133MGGDRt48sknefnll7n99tsb5xc0gNMB5ZWDQxoZERERsUfUYWT58uUMHjyYwYMHAzB58mQGDx7M1KlTAdi3b18kmAD06NGDefPmsWDBAgYOHMijjz7Kn//8Z9tv6wVwOKovYEUjIyIiIraI+pqRiy++GHOSh8rVNbvqxRdfzKpVq6L9qmYRuWYkqJERERERO7TIu2maU4Wj6m4ahRERERE7KIw4PACEykttrkRERCQ2xXwYCVaOjITKNTIiIiJiB4URZ+XIiC5gFRERsUXMh5FQ5WmacFBhRERExA4xH0aCTmumV1OuMCIiImKHmA8j5a6Eyg9F9hYiIiISo2I+jARc7QBwBhRGRERE7BDzYaTcnQiAs7zQ5kpERERik8JIZRhx6TSNiIiILWI+jFS4kwBwVRTbXImIiEhsivkwEoyzRkbiggojIiIidlAYibNGRuIqdM2IiIiIHWI+jAR8nQBIKD8E4bDN1YiIiMSemA8jFb5OhI0DlwlC6SG7yxEREYk5MR9GEhJ8HCTZWijcY28xIiIiMSjmw0ii102uaW8tFO61txgREZEYFPNhJMnnZpdJtxYObba3GBERkRgU82Ek0RvH+nA3ayHvC3uLERERiUEKIz43G02WtbBvjb3FiIiIxCCFEa+bFeHehHHAgQ26bkRERKSZxXwYSfK5OYKfLzjdatg4396CREREYkzMh5EO7TwAzK0412pY9mdNfiYiItKMFEYSPLidDl4JXUTYkwgH1sMnT9ldloiISMyI+TDidDrolOSlkHbsHfprq/Gdu2DJIxDQw/NERESamtvuAlqCtCQv+wrKWN/1WroO2w7Ln4NF98H7j0HWOdB5CGQMgMyB0L4HOGM+w4mIiDQahREg3e8DCtiTfxRGPwZdh8OSh+DwNti22HpV8STBkPFw2VSI89lUsYiISNuhMAL0Skvkv+vy2Ly/GBwOGHQdDPwe5K2FLz+15h/J/dyaFK28CD6eCQW7Yezf7S5dRESk1VMYAXqnJwJYYaSKw2GdmskYUN0WCsKmt+GVH8D6N2H7+9DjwuYtVkREpI3RxQ9An/QkANbtLSQYOsltvS43nPENGHS9tfzZnGaoTkREpG1TGAH6ZfhJjo+jOBBkzZcFX73BGVdZ7zs/atrCREREYoDCCOByOjjv9I4A/Hdd7ldv0GWI9X54K5SdQngRERGRE1IYqXT1oC4AvLL8S0oCwZN3TugA7TpZn4/sbOLKRERE2jaFkUo5Z6TRrUMCh0vK+d28dRhjTr5BclfrveDLpi9ORESkDVMYqeR2Obnvmv4AvLRsN7e8tIrNeUUn3iASRnY3Q3UiIiJtl27treGiPp24/5v9uWfuWuZ9to95n+2jS0o8Q05rz8CuyQzMSuGszn4SPG5ol2ZtVHLQ3qJFRERaOYWRY4zLPo2zu6Twp0WbWbRhP3vyj7In/yj/XrMXsC52/dbgLtyfnIwHoCzfznJFRERaPYWROgzomsyz44dRHAiyelc+q3cfYc2XBXz2ZT55hQFeWfElg9OLuR7gaL7N1YqIiLRuCiMnkeh1c0HvVC7onRpp+3jbIX7w12WsOgjXxwFHj9hXoIiISBtQrwtYZ86cSffu3fH5fGRnZ7Ns2bKT9p8xYwZ9+/YlPj6erKwsbr/9dsrKyupVsN3O7dmR64Z3o8C0sxp0mkZERKRBog4jc+bMYfLkyUybNo2VK1cycOBARo4cyf79++vs/+KLL3LnnXcybdo01q9fz1/+8hfmzJnDb37zmwYXb5eRZ2VQRIK1UFZobzEiIiKtXNRh5LHHHuPGG29k4sSJnHnmmcyaNYuEhASee+65Ovt/9NFHnH/++Vx//fV0796dK664guuuu+4rR1Nasv5dkinDC0CovNTmakRERFq3qMJIeXk5K1asICcnp3oHTic5OTksXbq0zm3OO+88VqxYEQkf27ZtY/78+Vx55ZUn/J5AIEBhYWGtV0uS6HXTLtEPQFhhREREpEGiuoD14MGDhEIh0tPTa7Wnp6ezYcOGOre5/vrrOXjwIBdccAHGGILBIDfddNNJT9NMnz6de++9N5rSmp0/yQ+HwFFx1O5SREREWrUmn4F18eLFPPDAAzz55JOsXLmS1157jXnz5nHfffedcJspU6ZQUFAQee3e3fJmOU1JSQbAFToKXzV1vIiIiJxQVCMjqampuFwu8vLyarXn5eWRkZFR5zb33HMPN9xwAz/+8Y8BGDBgACUlJfzkJz/hrrvuwuk8Pg95vV68Xm80pTW7ZL8VRhwYCAYgzmdzRSIiIq1TVCMjHo+HoUOHsnDhwkhbOBxm4cKFjBgxos5tSktLjwscLpcL4KsfRteCJbRLrF6o0HUjIiIi9RX1pGeTJ09mwoQJDBs2jOHDhzNjxgxKSkqYOHEiAOPHj6dLly5Mnz4dgDFjxvDYY48xePBgsrOz2bJlC/fccw9jxoyJhJLWyJ8QT8C48TqCoOtGRERE6i3qMDJ27FgOHDjA1KlTyc3NZdCgQbz99tuRi1p37dpVayTk7rvvxuFwcPfdd7Nnzx46derEmDFjuP/++xvvV9ggOT6OMjx4CWpkREREpAEcphWcKyksLCQ5OZmCggL8fr/d5QDw7ob9nPnSOaQ78uGnSyBzoN0liYiItCin+ve7ye+maav88W7KTZy1EKqwtxgREZFWTGGknuLj3JRXneUKldtbjIiISCumMFJP8R5XdRgJBuwtRkREpBVTGKmn+DgXFZGREZ2mERERqS+FkXqKj3NRjnXNSLCizOZqREREWi+FkXryeZxUGGtkpLxcYURERKS+FEbqyeNyUu6wwkgwoDAiIiJSXwoj9eRwOAg5rNM0FeW6gFVERKS+FEYaIFwZRoI6TSMiIlJvCiMNEHZ6AAhWaGRERESkvhRGGiDktEZGQhoZERERqTeFkQYIV4YRo3lGRERE6k1hpAFCladpTFAjIyIiIvWlMNIApmpkJKhn04iIiNSXwkgDhCMjIwojIiIi9aUw0gDGVXXNiMKIiIhIfSmMNEDVyIhDT+0VERGpN4WRhnBZYURP7RUREak/hZEGMJVhxBHSyIiIiEh9KYw0gKNqZCSskREREZH6UhhpCHfVyIguYBUREakvhZEGMG4fAE6FERERkXpTGGmIypERZ1jXjIiIiNSXwkgDOCpHRlwKIyIiIvWmMNIA1WFEp2lERETqS2GkARxuL6AwIiIi0hAKIw3gjLNGRtwKIyIiIvWmMNIAkTBiFEZERETqS2GkAZxx8QC4jSY9ExERqS+FkQZweaxrRuJMORhjczUiIiKtk8JIA1SNjDgxelieiIhIPSmMNIDbG1+9ECyzrxAREZFWTGGkAdyVp2kA0JTwIiIi9aIw0gBxbhcB47YWNDIiIiJSLwojDeB1OwkQZy0ENSW8iIhIfSiMNIDX7VIYERERaSCFkQbwxTkpj4QRnaYRERGpD4WRBvC6XQSMRkZEREQaQmGkAbxx1deMmIqjNlcjIiLSOtUrjMycOZPu3bvj8/nIzs5m2bJlJ+2fn5/PpEmTyMzMxOv10qdPH+bPn1+vglsSX5yLEqy5RoJlhTZXIyIi0jq5o91gzpw5TJ48mVmzZpGdnc2MGTMYOXIkGzduJC0t7bj+5eXlXH755aSlpfHqq6/SpUsXdu7cSUpKSmPUbyuf20WRscJIRUlB1dUjIiIiEoWow8hjjz3GjTfeyMSJEwGYNWsW8+bN47nnnuPOO+88rv9zzz3H4cOH+eijj4iLs/5cd+/evWFVtxBxLgfFlSMjoaMFNlcjIiLSOkV1mqa8vJwVK1aQk5NTvQOnk5ycHJYuXVrnNm+++SYjRoxg0qRJpKen079/fx544AFCoVDDKm8BHA4HpY52AISO6jSNiIhIfUQ1MnLw4EFCoRDp6em12tPT09mwYUOd22zbto1FixYxbtw45s+fz5YtW/jZz35GRUUF06ZNq3ObQCBAIFB9d0phYcv9Q1/mTADA6JoRERGRemnyu2nC4TBpaWk888wzDB06lLFjx3LXXXcxa9asE24zffp0kpOTI6+srKymLrPeylyJgMKIiIhIfUUVRlJTU3G5XOTl5dVqz8vLIyMjo85tMjMz6dOnDy6XK9J2xhlnkJubS3l53Q+XmzJlCgUFBZHX7t27oymzWQVc7So/KIyIiIjUR1RhxOPxMHToUBYuXBhpC4fDLFy4kBEjRtS5zfnnn8+WLVsIh8ORtk2bNpGZmYnH46lzG6/Xi9/vr/Vqqcrd1siII6ALWEVEROoj6tM0kydP5tlnn+Vvf/sb69ev5+abb6akpCRyd8348eOZMmVKpP/NN9/M4cOHufXWW9m0aRPz5s3jgQceYNKkSY33K2xUHNcBAHfpAZsrERERaZ2ivrV37NixHDhwgKlTp5Kbm8ugQYN4++23Ixe17tq1C6ezOuNkZWXxzjvvcPvtt3P22WfTpUsXbr31Vn7961833q+wUUlcKgCeMoURERGR+nAYY4zdRXyVwsJCkpOTKSgoaHGnbH7x53f405fXWgv3HASXpj4TERGBU//7rWfTNJCzXSoVpvLi3OL99hYjIiLSCimMNJA/wcsBkq2Fon32FiMiItIKKYw0kN8Xx85w5W3Nh7bYW4yIiEgrpDDSQP54N1tNprVwYKO9xYiIiLRCCiMN5PfFscV0sRYObrK3GBERkVZIYaSBkuPj2FwVRnI/s7cYERGRVkhhpIFSEjysCZ9OCCfk74LCvXaXJCIi0qoojDRQZrKPYhLYYE6zGnZ8aG9BIiIirYzCSANlJPsAeC80wGpYN9e+YkRERFohhZEG8sW56NjOw9zQ+VbDpnegKNfeokRERFoRhZFGkJniY5PJ4kjHwRCugPcftbskERGRVkNhpBH0SUsC4N3OP7EaPv0L7FlhY0UiIiKth8JIIzgj03r4z39L+8JZ3wITgtd+AmUFNlcmIiLS8imMNIKzOlthZNXuI5grH4GkztbU8P+6EcIhm6sTERFp2RRGGsGQ09oTH+cirzDAF/lu+N4/wO2Dze/AO3eBMXaXKCIi0mIpjDQCX5yLr/VJBeCV5buhy1C4eqa18pOnYPGDNlYnIiLSsimMNJLxI7oDMPvT3WzZXwwDvgOjHrJWvvcgfDDDttpERERaMoWRRnLe6R25sHcqgWCYn/59OQeLA5D9U7j0HqvD/6bB/+7VKRsREZFjKIw0EofDwSPfHUiG38fWAyVc+/RSth8sgQt/CZdNszp98Bi8eQsEy+0tVkREpAVRGGlE6X4fL/3kXDKTfWw7UMLVT3zA/9bvhwsnw5g/gcMJq/4Bz4/WA/VEREQqKYw0sh6p7XjjlvMZ0i2FwrIgP35hOb9+9TOK+4+D6+aALxm+XAZPXwSb/mt3uSIiIrZTGGkCaUnWCMmNF/bA4YA5y3dz2aOL+VfRmYR/vBjS+0PJfnjxu/D6zXD0iN0li4iI2MZhTMu/orKwsJDk5GQKCgrw+/12lxOVj7cd4o5XP2PX4VIABnRJ5udf68Ll+57F8fGTgIH49nDRnTDsh+D22FuwiIhIIznVv98KI82grCLEXz/cwROLNlNSbs3I2rNTO27ve4RR26fjPrTR6ti+B5z3cxh0PcTF21ixiIhIwymMtEAHiwM8/+EOXli6g8KyIABeZ5jfZC7n2qIXiC8/bHVMSIWhE2Dg9ZDay8aKRURE6k9hpAUrKqvg9VV7+NeKL1nzpfUwvQTKuNa1mJs8b5Nh9kf6hrsMw3nWNdDn69CxFzgc9hQtIiISJYWRVmLL/iLe+SKPxRv3s2LnERwmxEjnp3zHtYSLnGtwOar/8RzydmV/6gjKO5+Dp+d5pHbpTcdEL06nAoqIiLQ8CiOtUEFpBSt3HWHVriOs2p3P3i93cGH5B1zqXMW5znV4HLWfAJxr2rPO9OBLT08OJfbhaPu+uDr1JiOlHRnJPjonx5OR7KNjO48Ci4iINDuFkTbiUHGArQdK2LE3D8f2xXQ4uIKs4jX0DG7FTei4/mUmjk2mKxvDWWwwWWw03dji6Ibbn0Fmso8zMv0M7pbC8B4d6ZKii2RFRKTpKIy0deWlBPespGjHair2fY77wDqSCjYRFy6rs/tB42djOIuNJotl4X68Hx5A326Z3DDiNK4a2AWXRk5ERKSRKYzEonAIjuyAvC9g/zrIW4vJWweHt+Gg9j/mgHGzKDyYPwevpCR9GDO+N4h+GTq2IiLSeBRGpFp5KRxYD3nrIPcz2PI/OLwtsnp+aDj3mx9y3/cv5dJ+6TYWKiIibcmp/v12N2NNYhdPAnQZar0AjLFGTj5+CrP6Ra50LWOQ2cJP/34HCT+6lnN7drS3XhERiSl6Nk0scjgg/Sy4+gkcP30P07EPnR2H+Zv7dzz0z3nsL6r7uhMREZGmoDAS6zIG4PjxAsKZg+ngKOaRivt56M2VdlclIiIxRGFEID4F57hXqGiXSU9nLoPWP8LH2w7ZXZWIiMQIhRGxJHYi7ttPA/B990LemPdvmwsSEZFYoTAi1XpexNEzrgXgWwdm8vHWgzYXJCIisUBhRGqJH3Uv5U4f5zg38enbf7O7HBERiQH1CiMzZ86ke/fu+Hw+srOzWbZs2SltN3v2bBwOB9dcc019vlaag78zpUNvAuDKvGfZsb/A5oJERKStizqMzJkzh8mTJzNt2jRWrlzJwIEDGTlyJPv37z/pdjt27OD//b//x4UXXljvYqV5pFz2S4qcyZzu3MfaeU/ZXY6IiLRxUYeRxx57jBtvvJGJEydy5plnMmvWLBISEnjuuedOuE0oFGLcuHHce++99OzZs0EFSzPw+ckdOAmAc3Y+TWlJkc0FiYhIWxZVGCkvL2fFihXk5ORU78DpJCcnh6VLl55wu9/+9rekpaXxox/9qP6VSrM6fdSt7HN0Ip3DbHzzEbvLERGRNiyqMHLw4EFCoRDp6bWfX5Kenk5ubm6d23zwwQf85S9/4dlnnz3l7wkEAhQWFtZ6SfNyenxsOuMXAPTa+CzhkiM2VyQiIm1Vk95NU1RUxA033MCzzz5LamrqKW83ffp0kpOTI6+srKwmrFJOZMg3fsImk0USJex+6367yxERkTYqqjCSmpqKy+UiLy+vVnteXh4ZGRnH9d+6dSs7duxgzJgxuN1u3G43L7zwAm+++SZut5utW7fW+T1TpkyhoKAg8tq9e3c0ZUojSUrwsbK3NTqSuf5vUPClzRWJiEhbFFUY8Xg8DB06lIULF0bawuEwCxcuZMSIEcf179evH59//jmrV6+OvK666iouueQSVq9efcIRD6/Xi9/vr/USe1xw5TiWhfvhoZySlyZCqMLukkREpI1xR7vB5MmTmTBhAsOGDWP48OHMmDGDkpISJk6cCMD48ePp0qUL06dPx+fz0b9//1rbp6SkABzXLi1T1w7teO70uzhj209Iyl0G79wFo35vPflXRESkEUQdRsaOHcuBAweYOnUqubm5DBo0iLfffjtyUeuuXbtwOjWxa1vy/Ssv5Y4ZN/NU3GOw7GlISocLf2l3WSIi0kY4jDHG7iK+SmFhIcnJyRQUFOiUjU2mvPY5vhVPMy3u71bDN2bAsIm21iQiIi3bqf791hCGnJLbc3rzqnsMjwevsRrmTYb1b9lak4iItA0KI3JK0vw+7vh6Xx4NfpdXzGVgwvDqD2HniSe7ExERORUKI3LKxmWfxuBu7bkz8ANWxo+AUABeGgt56+wuTUREWjGFETllTqeDh79zNm53HNcd+Sn7UwZBWQG89D04mm93eSIi0kopjEhUeqUl8ZsrzyCAh28cvIUKfzfI3wlvTIKWfy20iIi0QAojErXxI07ja306sT+YwP9zTMa4PLDhLVj1d7tLExGRVkhhRKLmcFina1IS4ngjL43FXW+yViyYBqWH7S1ORERaHYURqZd0v4/p3xwAwE2bz6GsfV84ehg+etzmykREpLVRGJF6GzUgk9EDMgmEXTxc8V2rcflfIFBkb2EiItKqKIxIg/zfVWeRHB/Hcwf7kR/fzbq7Zv2/7S5LRERaEYURaZBOSV7uGn0GBif/PHqu1bj2NXuLEhGRVkVhRBrs20O60jc9iX8FhlsN296FskJ7ixIRkVZDYUQazOV0cPvlvdlmOrOLDAgHYccHdpclIiKthMKINIqcM9LJTPbxXrC/1bDtXXsLEhGRVkNhRBqF2+Xk2mFZfBC2bvdlq8KIiIicGoURaTSjBmSwNHwmQeOEQ5shf5fdJYmISCugMCKNpm96EskdUlllelkNW/5nb0EiItIqKIxIo3E4HFzWL533QgOths0KIyIi8tUURqRRXdS3E4vDVhgx29+DYLnNFYmISEunMCKNakTPjmxx9eSA8eMoL4ZdS+0uSUREWjiFEWlUvjgX2T07sSg0xGpY+y97CxIRkRZPYUQa3cV9OzE3fL618MXrUHHU3oJERKRFUxiRRndx3zQ+Dp/BlyYVAoXwxVy7SxIRkRZMYUQaXY/UdnTrmMiLwUuthg//COGwvUWJiEiLpTAiTeLiPp34R+hyypwJcGA9bJxnd0kiItJCKYxIk7i4bxqFtGO240qr4b93Q0WZvUWJiEiLpDAiTeLcnh3xuJ08VHIlFQnpcGQHfDjD7rJERKQFUhiRJhHvcXFZvzRK8fF62s+sxvcegi+X21uYiIi0OAoj0mSuG94NgPt29CN45rfAhODVH0LpYZsrExGRlkRhRJrMBb1S6dYhgaKyEK9m/hJSToP8nTD7el0/IiIiEQoj0mScTgc/uqAHAI9/uJ+KsS+BN9maIv5fP4JgwOYKRUSkJVAYkSY19pws0pK87Mk/ysu7EmHs38HlgQ1vWSMk5SV2lygiIjZTGJEm5YtzcdNFpwPw2H83UZBxHlw/B+ISYMv/4C9XwKGtNlcpIiJ2UhiRJvf9c0/j9E7tOFRSzmMLNsLpl8INc6FdJ8hbC89cDJ/+RbO0iojEKIURaXIet5PfXt0fgBc+3snSrYegWzb8dAlknWs9v2beZPhLDmxdBMbYXLGIiDQnhRFpFuf3SmXssCyMgdvnrOZISTn4O8PE+fD134MnEfasgL9/0zp1s/ZfECy3u2wREWkGCiPSbKZddSY9U9uRW1jGz/65kvJgGJwuOPcm+PkKyL4Z3D74cpk1H8mM/rDofijca3fpIiLShBzGtPwx8cLCQpKTkykoKMDv99tdjjTA+n2FfOepjygpD/HtIV15+Dtn43Q6qjsU5cLy52DF81CcZ7U53XDm1ZB9E3Q9BxyOOvctIiIty6n+/VYYkWa3eON+fvS35YTChu+dk8UD3xxQO5CAdYpmw79h2Z9h10fV7Z2HwLk3w5nXgNvTrHWLiEh0TvXvd71O08ycOZPu3bvj8/nIzs5m2bJlJ+z77LPPcuGFF9K+fXvat29PTk7OSftL23dx3zQe+e7ZOB0w+9PdTH55NYFgqHYntwf6fxt++B/rQtdB3weXF/auhNdutE7hvDvdGkkREZFWLeowMmfOHCZPnsy0adNYuXIlAwcOZOTIkezfv7/O/osXL+a6667j3XffZenSpWRlZXHFFVewZ8+eBhcvrdc3B3dlxvcG43I6mLt6L9//8yccKj7BjKyZA+GamTB5HVx6NyRlWqdw3nsQ/nCWdX3Jro91F46ISCsV9Wma7OxszjnnHJ544gkAwuEwWVlZ/PznP+fOO+/8yu1DoRDt27fniSeeYPz48af0nTpN03a9t+kAt/xzJUWBIF3bxzPz+iEMzEo5+UahClj/JnzyDOz+uLo942wY/hNrRMWT0KR1i4jIV2uS0zTl5eWsWLGCnJyc6h04neTk5LB06dJT2kdpaSkVFRV06NAhmq+WNuqiPp14fdJ5nNYxgS+PHOXbT33Es0u2EQ6fJCO74qzA8aN3rFM4g79v3YWT+xm8eQs81g/engIHNzffDxERkXqLKowcPHiQUChEenp6rfb09HRyc0/t3P2vf/1rOnfuXCvQHCsQCFBYWFjrJW1Xr7Qk3rzlAq4ckEEwbLh//np++LdP2V90Ck/2zRwIV8+Eyesh5/+sJwOXFcDHT8ITw+BvV8G6N6zRFBERaZGadZ6RBx98kNmzZ/P666/j8/lO2G/69OkkJydHXllZWc1YpdghOT6OmdcP4f5v9sfrdrJ44wFG/mEJ8z7bd2o7SOgAF9wOv1gN416FPl8HHLD9PXh5PPyhP7z7ABSe4v5ERKTZRHXNSHl5OQkJCbz66qtcc801kfYJEyaQn5/PG2+8ccJtH3nkEX73u9/xv//9j2HDhp30ewKBAIFA9cWMhYWFZGVl6ZqRGLExt4jb56xm3T5rRGzMwM7cd/VZpCREeStv/i5rvpKVL0DJAavN5YUh4+GC2yC5a6PWLSIitTXJNSMej4ehQ4eycOHCSFs4HGbhwoWMGDHihNs99NBD3Hfffbz99ttfGUQAvF4vfr+/1ktiR9+MJOZOOp9fXNoLl9PBv9fs5fI/LGHRhrzodpTSDS6bCrevg+88B1nZEArAp8/CHwfBv2+zAouIiNgq6rtp5syZw4QJE3j66acZPnw4M2bM4OWXX2bDhg2kp6czfvx4unTpwvTp0wH4/e9/z9SpU3nxxRc5//zzI/tJTEwkMTHxlL5Td9PErjW785n88mq2HigB4NphXbn7G2fi98VFvzNjYMf78N5D1juAMw6G/gC+9v8gKaPxChcRkaadgfWJJ57g4YcfJjc3l0GDBvGnP/2J7OxsAC6++GK6d+/O888/D0D37t3ZuXPncfuYNm0a//d//9eoP0baprKKEI/+dyN//mA7xkCG38f0bw3gkn5p9d/pjg/hvd9b15QAuONh+I3WdScJutNLRKQxaDp4aXOWbT/MHa+uYcehUgC+NaQLU79xZvTXktS0fQksvM96OB+AJwnOuwXO/Rn49L81EZGGUBiRNulouTVK8pcPrVGSTkle7r+mP1ec1YBTLMbA5v/Covsg93OrLb6DdZHrOTdqAjURkXpSGJE2bcXOI9zx6prItSRjBnbm3qvOokO7BoyShMOw/g1YdD8cqpwwLTHDup5kyAQ9mE9EJEoKI9LmlVWE+OPCzTz93lbCBjq28/Dbq/sz+uzMhu04FITP5sDiB6Gg8m6blG5w0Z1w9lhwuRtevIhIDFAYkZjx2Zf5/OqVz9iYVwTAqP4Z/Pbq/nRK8jZsx8GANUfJkoetB/MBdOwNl/wGzrwGnM06Z6CISKujMCIxJRAMMXPRFp5cvJVg2NA+IY7/u+osrhrYGYfD0bCdl5dac5N88Ac4esRqyxgAl94Dva+Ahu5fRKSNUhiRmPTF3gJ+9cpnkdlbc85I54Fv9ifNf+LHD5yyskLrmTcfPQHl1igMXYfDZfdAj681fP8iIm2MwojErIpQmKcWb+XxRZupCBn8Pje/HtWP687phtPZCKMYpYetUZJlz0LwqNXW4yK4bBp0Hdrw/YuItBEKIxLzNuQWcsern/HZlwUADOyazH3X9OfsrimN8wVFubDkEev5N+HKpwIP/QHk3AvxjfQdIiKtmMKICBAMhXlh6U4eW7CJ4kAQhwPGZXfjV1f0IzmhHlPK1+XITuvOmzUvWsuJ6TDq99ZFrrqeRERimMKISA37C8u4f/563li9F4AO7TzcOaof3xnStXFO3YA1xfy/b62eo6T3SBj9KKRkNc7+RURaGYURkTos3XqIqW+sZfP+YgDO6uznrivP4LxeqY3zBcEAvP8YvP+odeomrh1cehcM/6nmJxGRmKMwInICFaEwf/1wO48v3EJRIAjApf3SmDKqH73TkxrnSw5stEZJdi21ljMHwpg/QufBjbN/EZFWQGFE5CscKg7w+KIt/OPjnQTDBqcDvje8G7+4tDcZyY1wK3A4DKtegAVToawAHE7IvgkuuQu8iQ3fv4hIC6cwInKKth0o5vdvb+CdL6xZVj1uJ+Oyu3HzRac3zvwkRXnwzhRY+y9r2d8VrnwY+o7SBa4i0qYpjIhEadn2wzz8zgY+3WHNsup1O7nh3NP46UWnN3xqeYDN/4N5t0N+5fNuenwNLv+tTt2ISJulMCJSD8YYPtxyiMcWbGTlrnwA4uNcjMvuxo1f60l6Q0dKykvgvYesmVxD5VZb/+/ARb+GTn0atm8RkRZGYUSkAYwxLNl8kMcWbGLN7nwAPC4n3x3WlZsuOp2sDgkN+4IjO+Hd+62nAwPggH6j4fzbIOuchu1bRKSFUBgRaQTGGBZvOsDMRVtYvtM6feNyOrh6UGd+dnEveqU18ELUfWusCdM2zq9uyxwEQ26wRkw0k6uItGIKIyKN7JNth3ji3S28v/kgYF17+vWzMph0SS/6d0lu2M4PbISP/gRr5lRPLe/2Qe/Lod8Y6HMFxLdv4C8QEWleCiMiTWTN7nxmvruF/67Li7Rd3LcTt1zSi2HdOzRs5yWHrFM3q/4O+9dVtzvd0G0EdL8QelwIXYaCuxEuqhURaUIKIyJNbGNuEU8t3sKba/YSrvy3aHiPDtxySS8u7J2KoyG37RoDuZ/B+rdgwzzY/0Xt9e54K5B0HmTdjdN5MHToqVuFRaRFURgRaSY7D5Uw672tvLriSypC1r9OZ3dNZtIlvbj8jPTGefbN4W2w9V3Y8QHseB9KDhzfx5cMaWdBp741Xv0gKVMhRURsoTAi0sz2FRzl2SXbeXHZTsoqwgD0SU/kZxf34htnZ+J2ORvni4yxrjHZswL2rrJeuZ9DKFB3f68fOvSAlNOgfffK12nQvgckZ4Hb0zh1iYgcQ2FExCaHigM89+F2XvhoZ+TZN6d1TOCmi07nW0O64HW7Gv9LQxWwfz0c2FD52mi9Dm8DEzrJhg5r5MTfGfyZ4O9SY7lz9ee4+MavWUTaPIUREZsVHK3g70t38NyHOzhcYk1wluH3cePXenLd8CwSPM3wFN9gwAokR3bUeO2s/hw8emr7iW9vBZPENGiXZr0f+zkxHRI6grMJwpaItEoKIyItRGl5kJeW7eaZJVvJK7ROpXRo5+H7557Gd4d2bfgEavVlDBTvh4IvoWgvFO6Dwj1QtA8K91a/TjWwgPUwwISOVjBp18l6T+xkhZZ2naBdqvVKqHzXiItIm6YwItLCBIIhXlu5h6cWb2XX4dJI+/m9OnLtsCxGnpWBL66FjSoYA2X5VigpyrUunC3eD8V5lZ/zoLjyvfQQEOV/TjyJVnipK6i061Tjc2V7XCM8uFBEmo3CiEgLFQyF+c/aXOZ8upsPthyMtCd4XFzSN41RAzK4pG8a7bzNcBqnMYWCUHrQCisl+ytDy/7q5ZID1jwqpQetz+Fg9N/hSYJ2HStDSifrc12hpSrcaC4WEVspjIi0ArsPl/KvlV/y6oov+fJI9ekQr9vJead35MLenfhan1RO75TYsHlLWhpjoKzAGk0pOQAllQGl9KAVWOr6XO/wcsyIS0IH6xqY+PYQX+NzVbtOHYk0GoURkVbEGMPnewr4z9pc/vP5PnYcKq21PsPv4/xeqZzTvT1DT2vP6Z0SG2f+ktai6nRRzZGVkoOVnw/WCDOHqtvrE17AmlCuVkBJOT64+JLBm1T57gef31qOS9CcLiI1KIyItFLGGDbmFbFk0wHe33yQT7YfpjwYrtXH73Mz5LT2DOnWnv5d/JyZmUy639u2Rk8aomZ4iYyyVJ4mOnqk8nW4+nNp5eeT3gZ9Chyu6mDiTa78XLlc87M3CTztrPDiSQRPQuVyO+vdk2B9drWyU3Uix1AYEWkjyipCfLrjMEu3HmLlriOs2V3A0Yrj/2i2T4jjzM5+zsz0c0amn95pSfTs1K71XXtiF2MgUFQ7pESCSn51gCk9bPULFEJZIQQKrGUT/sqviJrLWxlO2tUILzWX462RnDhfjffKV1x83e9u3zH948EVpxEdaRIKIyJtVEUozIZ9RazcdYRVu46wfl8RWw4UEwrX/a9yht/H6WntOL1TIqd3SqRnp3Z079iOzGRf480KG+uMgfKSGgGlyAopZYVWW6Co+nNZIZQXQ0WptU3Vq+ZyQ0doouVwHh9m3F5weayX22sFFpfXmrHX5bE+u+Ia2M9jPQTSGWfNT+OKq152uWusc4NT/1ttjRRGRGJIWUWIzXnFrNtXwLq9hazfV8S2g8UcLC4/4TYup4PMZB9Z7RPo2j6erA4JZHWIJ6t9AlkdEkhN9OKKpetSWgpjrMnqKkqt0FJeGVIqSuoOL8Ey61VRZs0JU1G1fPQE62q8tyYOZ+1wUiusnCzI1FznrtyPy3p3uCo/Vy47a7Yd87nObRzVn2utd9bY57Hrq14O6x3HMZ8r10XaT7Surn2cbJ2zjnXUXpeU0eh3oCmMiAj5peVsPVDCtgPFbD1QwtYDxWw7UMzuI0ePuw7lWG6ng7QkLxnJPjKSfaT7fWRWvmf4fWQmx5Pm97a8uVHk1FSFnuAx4aXqPVQOwXLrPRSwHjkQDFQul1d+rqhcV9U3in7Bcusi43CFdVt41edwsGlOeclX+9H/IOucRt3lqf791slkkTYsJcHD0NM8DD2tfa32cNhwoDjA7sOl7D5Syu7DR2t93ldwlGDYsLegjL0FZSf9jvYJcaQmeumY6CE10UtqopdOSV46tqtcTvKSWrlOwaUFcTis0zItcSK5cLh2ODk2rNRcDlVAOFRj3Vcsh0NW2DHhys+h2p/D4TraQlZ4i3yuub7Ge8314fDxbSZstWMq11W+11yute7Y5aq+dWx3bN+TfUdk/THrbDwVpjAiEoOcTgfpfmuUY1j3DsetD4bCHCgOsK+gjLyCMnILy8it4z0QDHOktIIjpRVs3v/V35vodZOa6KF9Ow8p8XG0T/CQnGC9pyTEkZJQ3W4tx5HodesuoVjjdILTA+iJ0rFCYUREjuN2OclMjicz+cQTgBljKDhaQV5hgIPFVa9y670owKGS6s8Hi8spD4UpDgQpDgSPm0flpLU4HaQkxJEcH0eSL44knxu/zwopST43Sb44En2Vn73uSJ+qNr8vDq/bqUAj0oIpjIhIvTgcDmskI8FDX5JO2tcYQ1EgGAkm+aXl5JdWcKS0nPyjFbWXSyus19FyyirCBMOmMuSc+GLcr+J2Ooj3uEjwuGjncUc+x3vctPO4IssJHjfxcS7aea11CXFV/ax1XrcTX5wLr9uJN86J1+3CV/mui31F6q9eYWTmzJk8/PDD5ObmMnDgQB5//HGGDx9+wv6vvPIK99xzDzt27KB37978/ve/58orr6x30SLSujgcDvy+OPy+OHp2OvXtyipCtUJKUVkFRWXW6ErV56JA0Hovq6C4rPpzUeUojDEQDJvK9iAQaJLf6HY6jgkrx7y7a4cXb5wTj8uJx+0kzuUgzuWsfFV/9ricxLmPWa7q4z5mObKvGssuZ2zN1CutVtRhZM6cOUyePJlZs2aRnZ3NjBkzGDlyJBs3biQtLe24/h999BHXXXcd06dP5xvf+AYvvvgi11xzDStXrqR///6N8iNEpG3yxbnISHaRkVy/iyzDYUNJuRVKSstDHC0PURIIUlphfbbagpTU+BzpV+NzaXmIoxUhyoNhyipCBIJhAsEQFaHqmxGDYUOwPERJeTPPEfIVnA7rtJvb6cDltEKKy+mILJ+4vXLZVdVWuY/KZfcxy1Xb1vyumts5nQ5cDut6JafDWu9yOCqXrVvNq9qdDqo/V7U7HDid4Kpsd9TaR43+deyjqj3yPVXb1bFPsUfUt/ZmZ2dzzjnn8MQTTwAQDofJysri5z//OXfeeedx/ceOHUtJSQlvvfVWpO3cc89l0KBBzJo165S+U7f2ikhLFAobAsEQgYpwJKCUVVjvgWC4sv3YthBllesqQtarvPK9ImhqL4cql4O1l4/9bK23loMnmPxOTk1VkHFUhhcH1cHGUaPdWrbaqpadlWGmKhw5HQ4cVE5HUrV83D5qftex/U68nTXgVWPZadXqqLG+ur4av8VZ+7dVrQf40QU9yOqQ0KjHs0lu7S0vL2fFihVMmTIl0uZ0OsnJyWHp0qV1brN06VImT55cq23kyJHMnTs3mq8WEWlxXE4HCR43CS3opo9w2FARrgwnQSvYhMKGYMgQDFd+DpvIe7AywFS3hyv71l4+cf/q9tpt1nJF5bahsCFkDMZULUPYGMKVy9Xv1m8IGVPjnch6ax9E9lm9j+o+4ar9VW4bquxzKv/XOxQ2WGNbsRfqrhrUudHDyKmKKowcPHiQUChEenp6rfb09HQ2bNhQ5za5ubl19s/NzT3h9wQCAQKB6vO6hYWF0ZQpIhKznE4HXqcLrxto3Mk0Wz1TV6CpCj1VQahyXbhGgKm5bA08VfcNh8FQcxvre076Ts3vqtqWGt9XYz1Vy1XfVb1duEZtx35HZF+YE+47bO08ss8Mv31zzrTIu2mmT5/Ovffea3cZIiLShjgcldfA2F2IHCeq6dZSU1NxuVzk5eXVas/LyyMjI6PObTIyMqLqDzBlyhQKCgoir927d0dTpoiIiLQiUYURj8fD0KFDWbhwYaQtHA6zcOFCRowYUec2I0aMqNUfYMGCBSfsD+D1evH7/bVeIiIi0jZFPVo1efJkJkyYwLBhwxg+fDgzZsygpKSEiRMnAjB+/Hi6dOnC9OnTAbj11lu56KKLePTRRxk9ejSzZ89m+fLlPPPMM437S0RERKRVijqMjB07lgMHDjB16lRyc3MZNGgQb7/9duQi1V27duGs8bCd8847jxdffJG7776b3/zmN/Tu3Zu5c+dqjhEREREB6jHPiB00z4iIiEjrc6p/v+17XrCIiIgICiMiIiJiM4URERERsZXCiIiIiNhKYURERERspTAiIiIitlIYEREREVspjIiIiIitWsXDC6vmZSssLLS5EhERETlVVX+3v2p+1VYRRoqKigDIysqyuRIRERGJVlFREcnJySdc3yqmgw+Hw+zdu5ekpCQcDkej7bewsJCsrCx2796taeabkI5z89Gxbh46zs1Dx7l5NOVxNsZQVFRE586daz237litYmTE6XTStWvXJtu/3+/X/9CbgY5z89Gxbh46zs1Dx7l5NNVxPtmISBVdwCoiIiK2UhgRERERW8V0GPF6vUybNg2v12t3KW2ajnPz0bFuHjrOzUPHuXm0hOPcKi5gFRERkbYrpkdGRERExH4KIyIiImIrhRERERGxlcKIiIiI2Cqmw8jMmTPp3r07Pp+P7Oxsli1bZndJLdb06dM555xzSEpKIi0tjWuuuYaNGzfW6lNWVsakSZPo2LEjiYmJfPvb3yYvL69Wn127djF69GgSEhJIS0vjV7/6FcFgsFafxYsXM2TIELxeL7169eL5559v6p/XYj344IM4HA5uu+22SJuOc+PYs2cP3//+9+nYsSPx8fEMGDCA5cuXR9YbY5g6dSqZmZnEx8eTk5PD5s2ba+3j8OHDjBs3Dr/fT0pKCj/60Y8oLi6u1eezzz7jwgsvxOfzkZWVxUMPPdQsv68lCIVC3HPPPfTo0YP4+HhOP/107rvvvlrPKdFxrp8lS5YwZswYOnfujMPhYO7cubXWN+dxfeWVV+jXrx8+n48BAwYwf/786H+QiVGzZ882Ho/HPPfcc+aLL74wN954o0lJSTF5eXl2l9YijRw50vz1r381a9euNatXrzZXXnml6datmykuLo70uemmm0xWVpZZuHChWb58uTn33HPNeeedF1kfDAZN//79TU5Ojlm1apWZP3++SU1NNVOmTIn02bZtm0lISDCTJ08269atM48//rhxuVzm7bffbtbf2xIsW7bMdO/e3Zx99tnm1ltvjbTrODfc4cOHzWmnnWZ+8IMfmE8++cRs27bNvPPOO2bLli2RPg8++KBJTk42c+fONWvWrDFXXXWV6dGjhzl69Gikz9e//nUzcOBA8/HHH5v333/f9OrVy1x33XWR9QUFBSY9Pd2MGzfOrF271rz00ksmPj7ePP300836e+1y//33m44dO5q33nrLbN++3bzyyismMTHR/PGPf4z00XGun/nz55u77rrLvPbaawYwr7/+eq31zXVcP/zwQ+NyucxDDz1k1q1bZ+6++24TFxdnPv/886h+T8yGkeHDh5tJkyZFlkOhkOncubOZPn26jVW1Hvv37zeAee+994wxxuTn55u4uDjzyiuvRPqsX7/eAGbp0qXGGOtfHqfTaXJzcyN9nnrqKeP3+00gEDDGGHPHHXeYs846q9Z3jR071owcObKpf1KLUlRUZHr37m0WLFhgLrrookgY0XFuHL/+9a/NBRdccML14XDYZGRkmIcffjjSlp+fb7xer3nppZeMMcasW7fOAObTTz+N9PnPf/5jHA6H2bNnjzHGmCeffNK0b98+ctyrvrtv376N/ZNapNGjR5sf/vCHtdq+9a1vmXHjxhljdJwby7FhpDmP67XXXmtGjx5dq57s7Gzz05/+NKrfEJOnacrLy1mxYgU5OTmRNqfTSU5ODkuXLrWxstajoKAAgA4dOgCwYsUKKioqah3Tfv360a1bt8gxXbp0KQMGDCA9PT3SZ+TIkRQWFvLFF19E+tTcR1WfWPvnMmnSJEaPHn3csdBxbhxvvvkmw4YN47vf/S5paWkMHjyYZ599NrJ++/bt5Obm1jpGycnJZGdn1zrOKSkpDBs2LNInJycHp9PJJ598Eunzta99DY/HE+kzcuRINm7cyJEjR5r6Z9ruvPPOY+HChWzatAmANWvW8MEHHzBq1ChAx7mpNOdxbaz/lsRkGDl48CChUKjWf6wB0tPTyc3Ntamq1iMcDnPbbbdx/vnn079/fwByc3PxeDykpKTU6lvzmObm5tZ5zKvWnaxPYWEhR48ebYqf0+LMnj2blStXMn369OPW6Tg3jm3btvHUU0/Ru3dv3nnnHW6++WZ+8Ytf8Le//Q2oPk4n+29Ebm4uaWlptda73W46dOgQ1T+LtuzOO+/ke9/7Hv369SMuLo7Bgwdz2223MW7cOEDHuak053E9UZ9oj3ureGqvtCyTJk1i7dq1fPDBB3aX0ubs3r2bW2+9lQULFuDz+ewup80Kh8MMGzaMBx54AIDBgwezdu1aZs2axYQJE2yuru14+eWX+ec//8mLL77IWWedxerVq7ntttvo3LmzjrPUEpMjI6mpqbhcruPuQMjLyyMjI8OmqlqHW265hbfeeot3332Xrl27RtozMjIoLy8nPz+/Vv+axzQjI6POY1617mR9/H4/8fHxjf1zWpwVK1awf/9+hgwZgtvtxu1289577/GnP/0Jt9tNenq6jnMjyMzM5Mwzz6zVdsYZZ7Br1y6g+jid7L8RGRkZ7N+/v9b6YDDI4cOHo/pn0Zb96le/ioyODBgwgBtuuIHbb789Muqn49w0mvO4nqhPtMc9JsOIx+Nh6NChLFy4MNIWDodZuHAhI0aMsLGylssYwy233MLrr7/OokWL6NGjR631Q4cOJS4urtYx3bhxI7t27Yoc0xEjRvD555/X+hdgwYIF+P3+yB+GESNG1NpHVZ9Y+edy2WWX8fnnn7N69erIa9iwYYwbNy7yWce54c4///zjbk3ftGkTp512GgA9evQgIyOj1jEqLCzkk08+qXWc8/PzWbFiRaTPokWLCIfDZGdnR/osWbKEioqKSJ8FCxbQt29f2rdv32S/r6UoLS3F6az9Z8blchEOhwEd56bSnMe10f5bEtXlrm3I7NmzjdfrNc8//7xZt26d+clPfmJSUlJq3YEg1W6++WaTnJxsFi9ebPbt2xd5lZaWRvrcdNNNplu3bmbRokVm+fLlZsSIEWbEiBGR9VW3nF5xxRVm9erV5u233zadOnWq85bTX/3qV2b9+vVm5syZMXXLaV1q3k1jjI5zY1i2bJlxu93m/vvvN5s3bzb//Oc/TUJCgvnHP/4R6fPggw+alJQU88Ybb5jPPvvMXH311XXeGjl48GDzySefmA8++MD07t271q2R+fn5Jj093dxwww1m7dq1Zvbs2SYhIaFN33Ja04QJE0yXLl0it/a+9tprJjU11dxxxx2RPjrO9VNUVGRWrVplVq1aZQDz2GOPmVWrVpmdO3caY5rvuH744YfG7XabRx55xKxfv95MmzZNt/ZG6/HHHzfdunUzHo/HDB8+3Hz88cd2l9RiAXW+/vrXv0b6HD161PzsZz8z7du3NwkJCeab3/ym2bdvX6397Nixw4waNcrEx8eb1NRU88tf/tJUVFTU6vPuu++aQYMGGY/HY3r27FnrO2LRsWFEx7lx/Pvf/zb9+/c3Xq/X9OvXzzzzzDO11ofDYXPPPfeY9PR04/V6zWWXXWY2btxYq8+hQ4fMddddZxITE43f7zcTJ040RUVFtfqsWbPGXHDBBcbr9ZouXbqYBx98sMl/W0tRWFhobr31VtOtWzfj8/lMz549zV133VXrVlEd5/p599136/xv8oQJE4wxzXtcX375ZdOnTx/j8XjMWWedZebNmxf173EYU2MqPBEREZFmFpPXjIiIiEjLoTAiIiIitlIYEREREVspjIiIiIitFEZERETEVgojIiIiYiuFEREREbGVwoiIiIjYSmFEREREbKUwIiIiIrZSGBERERFbKYyIiIiIrf4/I7JLK4AFW8gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(loss)\n",
    "# plt.plot(val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20000, Training Loss: 1.3889295147352105, Validation Loss: 1.329963241965588\n",
      "Epoch 100/20000, Training Loss: 0.5329836529633847, Validation Loss: 0.5794732572445114\n",
      "Epoch 200/20000, Training Loss: 0.48962784524830044, Validation Loss: 0.5390775654232026\n",
      "Epoch 300/20000, Training Loss: 0.46970795678241045, Validation Loss: 0.5179104040415549\n",
      "Epoch 400/20000, Training Loss: 0.4571055414023378, Validation Loss: 0.5032764832133534\n",
      "Epoch 500/20000, Training Loss: 0.4480888206299344, Validation Loss: 0.49221975277248425\n",
      "Epoch 600/20000, Training Loss: 0.44121459890624665, Validation Loss: 0.48349730648957123\n",
      "Epoch 700/20000, Training Loss: 0.43576034614321146, Validation Loss: 0.47641949951028567\n",
      "Epoch 800/20000, Training Loss: 0.4313079350288389, Validation Loss: 0.47055071217589756\n",
      "Epoch 900/20000, Training Loss: 0.4275933821479384, Validation Loss: 0.4655973747248898\n",
      "Epoch 1000/20000, Training Loss: 0.4244400995427411, Validation Loss: 0.46135381811518644\n",
      "Epoch 1100/20000, Training Loss: 0.4217248608525791, Validation Loss: 0.45767183033916636\n",
      "Epoch 1200/20000, Training Loss: 0.41935878075391253, Validation Loss: 0.4544420850784265\n",
      "Epoch 1300/20000, Training Loss: 0.4172759827093269, Validation Loss: 0.4515822706786517\n",
      "Epoch 1400/20000, Training Loss: 0.41542651714943063, Validation Loss: 0.4490292627510211\n",
      "Epoch 1500/20000, Training Loss: 0.41377176479969563, Validation Loss: 0.4467338373142276\n",
      "Epoch 1600/20000, Training Loss: 0.4122813575878912, Validation Loss: 0.4446570251817697\n",
      "Epoch 1700/20000, Training Loss: 0.4109310598257106, Validation Loss: 0.4427675503679005\n",
      "Epoch 1800/20000, Training Loss: 0.4097012756316726, Validation Loss: 0.44103999845074787\n",
      "Epoch 1900/20000, Training Loss: 0.40857597559904785, Validation Loss: 0.4394534852734767\n",
      "Epoch 2000/20000, Training Loss: 0.40754191068940165, Validation Loss: 0.43799067435130734\n",
      "Epoch 2100/20000, Training Loss: 0.4065880269906327, Validation Loss: 0.43663704116315644\n",
      "Epoch 2200/20000, Training Loss: 0.40570502355598603, Validation Loss: 0.435380314865962\n",
      "Epoch 2300/20000, Training Loss: 0.40488501386955145, Validation Loss: 0.4342100493263733\n",
      "Epoch 2400/20000, Training Loss: 0.4041212635006528, Validation Loss: 0.43311728967237517\n",
      "Epoch 2500/20000, Training Loss: 0.4034079845463123, Validation Loss: 0.43209431029163237\n",
      "Epoch 2600/20000, Training Loss: 0.40274017293402975, Validation Loss: 0.4311344069040559\n",
      "Epoch 2700/20000, Training Loss: 0.4021134784463953, Validation Loss: 0.43023173001498055\n",
      "Epoch 2800/20000, Training Loss: 0.4015240999926579, Validation Loss: 0.4293811503642322\n",
      "Epoch 2900/20000, Training Loss: 0.4009687005509854, Validation Loss: 0.4285781493550487\n",
      "Epoch 3000/20000, Training Loss: 0.4004443375760363, Validation Loss: 0.42781872916225816\n",
      "Epoch 3100/20000, Training Loss: 0.3999484056681609, Validation Loss: 0.4270993384753018\n",
      "Epoch 3200/20000, Training Loss: 0.3994785890406407, Validation Loss: 0.42641681076129395\n",
      "Epoch 3300/20000, Training Loss: 0.39903282187384703, Validation Loss: 0.42576831262814635\n",
      "Epoch 3400/20000, Training Loss: 0.3986092550615893, Validation Loss: 0.42515130039206617\n",
      "Epoch 3500/20000, Training Loss: 0.39820622817160456, Validation Loss: 0.4245634833528975\n",
      "Epoch 3600/20000, Training Loss: 0.3978222456850197, Validation Loss: 0.4240027925872433\n",
      "Epoch 3700/20000, Training Loss: 0.3974559567673716, Validation Loss: 0.42346735430650706\n",
      "Epoch 3800/20000, Training Loss: 0.39710613797001726, Validation Loss: 0.42295546701196174\n",
      "Epoch 3900/20000, Training Loss: 0.39677167837546434, Validation Loss: 0.4224655818242439\n",
      "Epoch 4000/20000, Training Loss: 0.396451566790723, Validation Loss: 0.4219962854795438\n",
      "Epoch 4100/20000, Training Loss: 0.39614488066474585, Validation Loss: 0.4215462855761917\n",
      "Epoch 4200/20000, Training Loss: 0.39585077646354166, Validation Loss: 0.42111439772853493\n",
      "Epoch 4300/20000, Training Loss: 0.39556848128279193, Validation Loss: 0.4206995343439425\n",
      "Epoch 4400/20000, Training Loss: 0.395297285515171, Validation Loss: 0.4203006947864955\n",
      "Epoch 4500/20000, Training Loss: 0.3950365364199249, Validation Loss: 0.4199169567297607\n",
      "Epoch 4600/20000, Training Loss: 0.39478563246705173, Validation Loss: 0.4195474685328041\n",
      "Epoch 4700/20000, Training Loss: 0.3945440183487463, Validation Loss: 0.41919144249970597\n",
      "Epoch 4800/20000, Training Loss: 0.39431118056751424, Validation Loss: 0.41884814890437516\n",
      "Epoch 4900/20000, Training Loss: 0.3940866435242155, Validation Loss: 0.4185169106803203\n",
      "Epoch 5000/20000, Training Loss: 0.3938699660407946, Validation Loss: 0.41819709868989313\n",
      "Epoch 5100/20000, Training Loss: 0.3936607382620561, Validation Loss: 0.4178881274999342\n",
      "Epoch 5200/20000, Training Loss: 0.3934585788888673, Validation Loss: 0.41758945160117056\n",
      "Epoch 5300/20000, Training Loss: 0.39326313270191854, Validation Loss: 0.4173005620174678\n",
      "Epoch 5400/20000, Training Loss: 0.39307406834085346, Validation Loss: 0.4170209832584495\n",
      "Epoch 5500/20000, Training Loss: 0.39289107630839315, Validation Loss: 0.4167502705752627\n",
      "Epoch 5600/20000, Training Loss: 0.39271386717315193, Validation Loss: 0.41648800748459713\n",
      "Epoch 5700/20000, Training Loss: 0.3925421699483161, Validation Loss: 0.416233803530616\n",
      "Epoch 5800/20000, Training Loss: 0.3923757306263187, Validation Loss: 0.4159872922583356\n",
      "Epoch 5900/20000, Training Loss: 0.39221431085217734, Validation Loss: 0.41574812937532435\n",
      "Epoch 6000/20000, Training Loss: 0.392057686720338, Validation Loss: 0.415515991081461\n",
      "Epoch 6100/20000, Training Loss: 0.3919056476817412, Validation Loss: 0.41529057254895413\n",
      "Epoch 6200/20000, Training Loss: 0.391757995549441, Validation Loss: 0.41507158653696624\n",
      "Epoch 6300/20000, Training Loss: 0.39161454359250253, Validation Loss: 0.41485876212703005\n",
      "Epoch 6400/20000, Training Loss: 0.39147511570911897, Validation Loss: 0.41465184356705365\n",
      "Epoch 6500/20000, Training Loss: 0.3913395456709364, Validation Loss: 0.41445058921310723\n",
      "Epoch 6600/20000, Training Loss: 0.3912076764314925, Validation Loss: 0.4142547705594043\n",
      "Epoch 6700/20000, Training Loss: 0.39107935949247785, Validation Loss: 0.4140641713479553\n",
      "Epoch 6800/20000, Training Loss: 0.39095445432222203, Validation Loss: 0.41387858675030653\n",
      "Epoch 6900/20000, Training Loss: 0.39083282782142464, Validation Loss: 0.4136978226145963\n",
      "Epoch 7000/20000, Training Loss: 0.39071435383168635, Validation Loss: 0.41352169477188105\n",
      "Epoch 7100/20000, Training Loss: 0.39059891268287067, Validation Loss: 0.4133500283963194\n",
      "Epoch 7200/20000, Training Loss: 0.3904863907757412, Validation Loss: 0.41318265741436405\n",
      "Epoch 7300/20000, Training Loss: 0.39037668019669125, Validation Loss: 0.4130194239586039\n",
      "Epoch 7400/20000, Training Loss: 0.3902696783617062, Validation Loss: 0.41286017786234486\n",
      "Epoch 7500/20000, Training Loss: 0.3901652876869866, Validation Loss: 0.41270477619139806\n",
      "Epoch 7600/20000, Training Loss: 0.39006341528392036, Validation Loss: 0.41255308280990083\n",
      "Epoch 7700/20000, Training Loss: 0.38996397267631483, Validation Loss: 0.41240496797729637\n",
      "Epoch 7800/20000, Training Loss: 0.38986687553800675, Validation Loss: 0.41226030797387786\n",
      "Epoch 7900/20000, Training Loss: 0.38977204344914573, Validation Loss: 0.4121189847525461\n",
      "Epoch 8000/20000, Training Loss: 0.38967939966961135, Validation Loss: 0.41198088561464996\n",
      "Epoch 8100/20000, Training Loss: 0.38958887092816374, Validation Loss: 0.41184590290797674\n",
      "Epoch 8200/20000, Training Loss: 0.38950038722606245, Validation Loss: 0.4117139337451341\n",
      "Epoch 8300/20000, Training Loss: 0.38941388165400087, Validation Loss: 0.4115848797407256\n",
      "Epoch 8400/20000, Training Loss: 0.3893292902213062, Validation Loss: 0.41145864676586064\n",
      "Epoch 8500/20000, Training Loss: 0.38924655169645445, Validation Loss: 0.4113351447186762\n",
      "Epoch 8600/20000, Training Loss: 0.38916560745802603, Validation Loss: 0.41121428730965043\n",
      "Epoch 8700/20000, Training Loss: 0.38908640135531175, Validation Loss: 0.41109599186060747\n",
      "Epoch 8800/20000, Training Loss: 0.38900887957784047, Validation Loss: 0.41098017911639384\n",
      "Epoch 8900/20000, Training Loss: 0.38893299053316854, Validation Loss: 0.41086677306830066\n",
      "Epoch 9000/20000, Training Loss: 0.3888586847323182, Validation Loss: 0.41075570078838164\n",
      "Epoch 9100/20000, Training Loss: 0.38878591468231477, Validation Loss: 0.4106468922738828\n",
      "Epoch 9200/20000, Training Loss: 0.3887146347853051, Validation Loss: 0.41054028030106865\n",
      "Epoch 9300/20000, Training Loss: 0.3886448012437941, Validation Loss: 0.4104358002877863\n",
      "Epoch 9400/20000, Training Loss: 0.38857637197156475, Validation Loss: 0.4103333901641582\n",
      "Epoch 9500/20000, Training Loss: 0.38850930650988613, Validation Loss: 0.4102329902508462\n",
      "Epoch 9600/20000, Training Loss: 0.38844356594864493, Validation Loss: 0.41013454314437164\n",
      "Epoch 9700/20000, Training Loss: 0.388379112852063, Validation Loss: 0.41003799360901666\n",
      "Epoch 9800/20000, Training Loss: 0.38831591118869235, Validation Loss: 0.40994328847486505\n",
      "Epoch 9900/20000, Training Loss: 0.3882539262653987, Validation Loss: 0.4098503765415822\n",
      "Epoch 10000/20000, Training Loss: 0.38819312466507117, Validation Loss: 0.409759208487554\n",
      "Epoch 10100/20000, Training Loss: 0.38813347418781324, Validation Loss: 0.4096697367840399\n",
      "Epoch 10200/20000, Training Loss: 0.3880749437953865, Validation Loss: 0.4095819156140194\n",
      "Epoch 10300/20000, Training Loss: 0.388017503558701, Validation Loss: 0.4094957007954307\n",
      "Epoch 10400/20000, Training Loss: 0.3879611246081545, Validation Loss: 0.4094110497085292\n",
      "Epoch 10500/20000, Training Loss: 0.3879057790866434, Validation Loss: 0.4093279212271044\n",
      "Epoch 10600/20000, Training Loss: 0.3878514401050758, Validation Loss: 0.4092462756533211\n",
      "Epoch 10700/20000, Training Loss: 0.387798081700233, Validation Loss: 0.40916607465595983\n",
      "Epoch 10800/20000, Training Loss: 0.3877456787948346, Validation Loss: 0.4090872812118516\n",
      "Epoch 10900/20000, Training Loss: 0.3876942071596723, Validation Loss: 0.40900985955031555\n",
      "Epoch 11000/20000, Training Loss: 0.38764364337768953, Validation Loss: 0.4089337751004187\n",
      "Epoch 11100/20000, Training Loss: 0.38759396480988717, Validation Loss: 0.40885899444089224\n",
      "Epoch 11200/20000, Training Loss: 0.38754514956295094, Validation Loss: 0.4087854852525503\n",
      "Epoch 11300/20000, Training Loss: 0.3874971764584961, Validation Loss: 0.4087132162730617\n",
      "Epoch 11400/20000, Training Loss: 0.38745002500383724, Validation Loss: 0.4086421572539435\n",
      "Epoch 11500/20000, Training Loss: 0.38740367536419423, Validation Loss: 0.40857227891964787\n",
      "Epoch 11600/20000, Training Loss: 0.38735810833625167, Validation Loss: 0.4085035529286233\n",
      "Epoch 11700/20000, Training Loss: 0.3873133053229958, Validation Loss: 0.4084359518362392\n",
      "Epoch 11800/20000, Training Loss: 0.38726924830975684, Validation Loss: 0.4083694490594724\n",
      "Epoch 11900/20000, Training Loss: 0.3872259198413893, Validation Loss: 0.4083040188432558\n",
      "Epoch 12000/20000, Training Loss: 0.3871833030005263, Validation Loss: 0.40823963622839815\n",
      "Epoch 12100/20000, Training Loss: 0.38714138138685056, Validation Loss: 0.40817627702099224\n",
      "Epoch 12200/20000, Training Loss: 0.387100139097326, Validation Loss: 0.40811391776322836\n",
      "Epoch 12300/20000, Training Loss: 0.3870595607073374, Validation Loss: 0.40805253570553923\n",
      "Epoch 12400/20000, Training Loss: 0.3870196312526901, Validation Loss: 0.40799210878000586\n",
      "Epoch 12500/20000, Training Loss: 0.3869803362124242, Validation Loss: 0.4079326155749577\n",
      "Epoch 12600/20000, Training Loss: 0.38694166149239934, Validation Loss: 0.4078740353107044\n",
      "Epoch 12700/20000, Training Loss: 0.386903593409611, Validation Loss: 0.4078163478163413\n",
      "Epoch 12800/20000, Training Loss: 0.3868661186771992, Validation Loss: 0.4077595335075728\n",
      "Epoch 12900/20000, Training Loss: 0.38682922439011463, Validation Loss: 0.4077035733655021\n",
      "Epoch 13000/20000, Training Loss: 0.38679289801140804, Validation Loss: 0.4076484489163375\n",
      "Epoch 13100/20000, Training Loss: 0.38675712735911155, Validation Loss: 0.4075941422119698\n",
      "Epoch 13200/20000, Training Loss: 0.3867219005936825, Validation Loss: 0.4075406358113791\n",
      "Epoch 13300/20000, Training Loss: 0.3866872062059807, Validation Loss: 0.4074879127628249\n",
      "Epoch 13400/20000, Training Loss: 0.3866530330057531, Validation Loss: 0.4074359565867864\n",
      "Epoch 13500/20000, Training Loss: 0.38661937011060193, Validation Loss: 0.4073847512596144\n",
      "Epoch 13600/20000, Training Loss: 0.38658620693540996, Validation Loss: 0.40733428119785786\n",
      "Epoch 13700/20000, Training Loss: 0.3865535331822042, Validation Loss: 0.407284531243237\n",
      "Epoch 13800/20000, Training Loss: 0.3865213388304336, Validation Loss: 0.40723548664822834\n",
      "Epoch 13900/20000, Training Loss: 0.3864896141276436, Validation Loss: 0.4071871330622351\n",
      "Epoch 14000/20000, Training Loss: 0.3864583495805266, Validation Loss: 0.4071394565183135\n",
      "Epoch 14100/20000, Training Loss: 0.38642753594633245, Validation Loss: 0.40709244342043055\n",
      "Epoch 14200/20000, Training Loss: 0.38639716422461995, Validation Loss: 0.40704608053122693\n",
      "Epoch 14300/20000, Training Loss: 0.3863672256493362, Validation Loss: 0.40700035496026393\n",
      "Epoch 14400/20000, Training Loss: 0.38633771168120573, Validation Loss: 0.4069552541527294\n",
      "Epoch 14500/20000, Training Loss: 0.3863086140004178, Validation Loss: 0.4069107658785839\n",
      "Epoch 14600/20000, Training Loss: 0.3862799244995965, Validation Loss: 0.4068668782221272\n",
      "Epoch 14700/20000, Training Loss: 0.38625163527704115, Validation Loss: 0.40682357957196424\n",
      "Epoch 14800/20000, Training Loss: 0.3862237386302263, Validation Loss: 0.4067808586113545\n",
      "Epoch 14900/20000, Training Loss: 0.3861962270495477, Validation Loss: 0.40673870430892756\n",
      "Epoch 15000/20000, Training Loss: 0.38616909321230386, Validation Loss: 0.4066971059097483\n",
      "Epoch 15100/20000, Training Loss: 0.3861423299769048, Validation Loss: 0.4066560529267161\n",
      "Epoch 15200/20000, Training Loss: 0.3861159303772946, Validation Loss: 0.40661553513228416\n",
      "Epoch 15300/20000, Training Loss: 0.3860898876175815, Validation Loss: 0.406575542550485\n",
      "Epoch 15400/20000, Training Loss: 0.3860641950668649, Validation Loss: 0.40653606544924875\n",
      "Epoch 15500/20000, Training Loss: 0.3860388462542514, Validation Loss: 0.40649709433300124\n",
      "Epoch 15600/20000, Training Loss: 0.3860138348640516, Validation Loss: 0.40645861993553223\n",
      "Epoch 15700/20000, Training Loss: 0.38598915473115025, Validation Loss: 0.4064206332131177\n",
      "Epoch 15800/20000, Training Loss: 0.3859647998365418, Validation Loss: 0.40638312533789256\n",
      "Epoch 15900/20000, Training Loss: 0.3859407643030262, Validation Loss: 0.4063460876914556\n",
      "Epoch 16000/20000, Training Loss: 0.38591704239105507, Validation Loss: 0.40630951185870323\n",
      "Epoch 16100/20000, Training Loss: 0.38589362849472575, Validation Loss: 0.40627338962187964\n",
      "Epoch 16200/20000, Training Loss: 0.38587051713791426, Validation Loss: 0.40623771295483413\n",
      "Epoch 16300/20000, Training Loss: 0.38584770297054316, Validation Loss: 0.4062024740174791\n",
      "Epoch 16400/20000, Training Loss: 0.3858251807649779, Validation Loss: 0.40616766515044017\n",
      "Epoch 16500/20000, Training Loss: 0.3858029454125472, Validation Loss: 0.40613327886988765\n",
      "Epoch 16600/20000, Training Loss: 0.3857809919201821, Validation Loss: 0.4060993078625479\n",
      "Epoch 16700/20000, Training Loss: 0.38575931540716846, Validation Loss: 0.4060657449808816\n",
      "Epoch 16800/20000, Training Loss: 0.3857379111020095, Validation Loss: 0.40603258323842606\n",
      "Epoch 16900/20000, Training Loss: 0.3857167743393939, Validation Loss: 0.40599981580529343\n",
      "Epoch 17000/20000, Training Loss: 0.38569590055726366, Validation Loss: 0.40596743600382007\n",
      "Epoch 17100/20000, Training Loss: 0.3856752852939802, Validation Loss: 0.40593543730435866\n",
      "Epoch 17200/20000, Training Loss: 0.3856549241855832, Validation Loss: 0.4059038133212116\n",
      "Epoch 17300/20000, Training Loss: 0.3856348129631399, Validation Loss: 0.4058725578086957\n",
      "Epoch 17400/20000, Training Loss: 0.3856149474501797, Validation Loss: 0.4058416646573368\n",
      "Epoch 17500/20000, Training Loss: 0.3855953235602127, Validation Loss: 0.4058111278901877\n",
      "Epoch 17600/20000, Training Loss: 0.3855759372943276, Validation Loss: 0.40578094165926415\n",
      "Epoch 17700/20000, Training Loss: 0.385556784738867, Validation Loss: 0.40575110024209604\n",
      "Epoch 17800/20000, Training Loss: 0.38553786206317603, Validation Loss: 0.40572159803838986\n",
      "Epoch 17900/20000, Training Loss: 0.385519165517423, Validation Loss: 0.40569242956679424\n",
      "Epoch 18000/20000, Training Loss: 0.3855006914304884, Validation Loss: 0.40566358946177083\n",
      "Epoch 18100/20000, Training Loss: 0.3854824362079193, Validation Loss: 0.4056350724705619\n",
      "Epoch 18200/20000, Training Loss: 0.38546439632994883, Validation Loss: 0.40560687345025265\n",
      "Epoch 18300/20000, Training Loss: 0.38544656834957597, Validation Loss: 0.4055789873649256\n",
      "Epoch 18400/20000, Training Loss: 0.38542894889070495, Validation Loss: 0.4055514092829028\n",
      "Epoch 18500/20000, Training Loss: 0.38541153464634137, Validation Loss: 0.40552413437407314\n",
      "Epoch 18600/20000, Training Loss: 0.3853943223768436, Validation Loss: 0.4054971579073008\n",
      "Epoch 18700/20000, Training Loss: 0.38537730890822697, Validation Loss: 0.40547047524791313\n",
      "Epoch 18800/20000, Training Loss: 0.3853604911305197, Validation Loss: 0.4054440818552655\n",
      "Epoch 18900/20000, Training Loss: 0.38534386599616677, Validation Loss: 0.40541797328037676\n",
      "Epoch 19000/20000, Training Loss: 0.3853274305184834, Validation Loss: 0.40539214516364\n",
      "Epoch 19100/20000, Training Loss: 0.3853111817701529, Validation Loss: 0.4053665932325977\n",
      "Epoch 19200/20000, Training Loss: 0.3852951168817695, Validation Loss: 0.405341313299785\n",
      "Epoch 19300/20000, Training Loss: 0.3852792330404239, Validation Loss: 0.40531630126063567\n",
      "Epoch 19400/20000, Training Loss: 0.3852635274883302, Validation Loss: 0.4052915530914516\n",
      "Epoch 19500/20000, Training Loss: 0.38524799752149325, Validation Loss: 0.40526706484742847\n",
      "Epoch 19600/20000, Training Loss: 0.38523264048841344, Validation Loss: 0.4052428326607426\n",
      "Epoch 19700/20000, Training Loss: 0.38521745378883004, Validation Loss: 0.4052188527386898\n",
      "Epoch 19800/20000, Training Loss: 0.38520243487249983, Validation Loss: 0.4051951213618802\n",
      "Epoch 19900/20000, Training Loss: 0.3851875812380111, Validation Loss: 0.4051716348824847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4051486209911782"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear = LinearNetwork(config)\n",
    "\n",
    "# losses, val_losses = linear.logistic_regression(X_train,y_train,X_val,y_val,epochs=20000,lr=0.0001)\n",
    "# val_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1318a2f20>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(losses)\n",
    "# plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
