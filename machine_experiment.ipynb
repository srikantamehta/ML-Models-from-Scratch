{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from src.cross_validation import CrossValidation\n",
    "from src.evaluation import Evaluation\n",
    "from models.knn import KNN\n",
    "from models.null_model import NullModelClassification, NullModelRegression\n",
    "from data_configs.configs import *\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "config = machine_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)\n",
    "classification_nullmodel = NullModelClassification(config=config)\n",
    "regression_nullmodel = NullModelRegression(config=config)\n",
    "knn_model = KNN(config)\n",
    "null_model = NullModelRegression(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "\n",
    "raw_data_2 = raw_data.drop(columns=['vendor_name', 'model_name', 'ERP'])\n",
    "\n",
    "data_1 = data_processor.impute_missing_values(raw_data_2)\n",
    "\n",
    "data_2 = data_processor.encode_nominal_features(data_1)\n",
    "\n",
    "data_3 = data_processor.encode_ordinal_features(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['MYCT', 'MMIN', 'MMAX', 'CACH', 'CHMIN', 'CHMAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0071906613411797165"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1/(statistics.stdev(data_train[config['target_column']]))\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with k=1: 18692.819047619043\n",
      "Average score with k=2: 17661.420809542098\n",
      "Average score with k=3: 18995.569669901513\n",
      "Average score with k=4: 19611.49917804948\n",
      "Average score with k=5: 20429.088741870484\n",
      "Average score with k=6: 21261.43363838865\n",
      "Average score with k=7: 21813.683190323467\n",
      "Average score with k=8: 22263.245673862275\n",
      "Average score with k=9: 23109.50510701653\n",
      "Average score with k=10: 24044.911264984657\n",
      "Average score with k=11: 24791.148339346157\n",
      "Average score with k=12: 25524.400740610767\n",
      "Average score with k=13: 26372.642408441294\n",
      "Average score with k=14: 27191.311096247235\n",
      "Best k is 2 with the lowest average score of 17661.420809542098\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        data_train_standardized = data_processor.standardize_data(train_set_1, train_set_1, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(train_set_1,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=k, gamma=gamma)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average MSE score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Gamma ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with g=0.4: 17774.013336162112\n",
      "Average score with g=0.6: 17734.10464995688\n",
      "Average score with g=0.8: 17696.562832538002\n",
      "Average score with g=1.0: 17661.420809542098\n",
      "Average score with g=1.2: 17628.695996424074\n",
      "Average score with g=1.4: 17598.390491571034\n",
      "Average score with g=1.6: 17570.491502459376\n",
      "Best g is 1.6 with the lowest average score of 17570.491502459376\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(0.4,1.6,0.2)\n",
    "scores_dict = {}\n",
    "\n",
    "for g in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        data_train_standardized = data_processor.standardize_data(train_set_1, train_set_1, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(train_set_1,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=best_k, gamma=gamma*g)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with g={round(g,2)}: {average_score}\")\n",
    "    scores_dict[g] = average_score\n",
    "\n",
    "best_g = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best g is {round(best_g,2)} with the lowest average MSE score of {scores_dict[best_g]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous study MSE score (ERP vs PRP): 1737.3349282296651\n",
      "Average null model MSE score: 19073.865381296106\n",
      "Average MSE score for k=2, g=1.6: 7512.193936429954\n",
      "Average MAE score for k=2, g=1.6: 34.97747282263274\n",
      "Average r2 score for k=2, g=1.6: 0.6523904298418604\n",
      "Average pearson score for k=2, g=1.6: 0.8250440267334402\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "pearson_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "    data_train_standardized = data_processor.standardize_data(train_set, train_set, features=features)\n",
    "    data_test_standardized = data_processor.standardize_data(train_set,test_set,features=features)  \n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_regression(data_test_standardized, data_train_standardized, k=best_k, gamma=best_g*gamma)['Predicted Value']\n",
    "    \n",
    "    mse_score = Evaluation.mean_squared_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    mae_score = Evaluation.mean_absolute_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    r2_score = Evaluation.r2_coefficient(data_test_standardized[config['target_column']], predictions_1)\n",
    "    pearson_score = Evaluation.pearsons_correlation(data_test_standardized[config['target_column']], predictions_1)\n",
    "    \n",
    "    mse_scores.append(mse_score)\n",
    "    mae_scores.append(mae_score)\n",
    "    r2_scores.append(r2_score)\n",
    "    pearson_scores.append(pearson_score)\n",
    "\n",
    "    null_model_prediction = null_model.naive_regression(test_set)\n",
    "    null_model_score = Evaluation.mean_squared_error(test_set[config['target_column']],null_model_prediction)\n",
    "    null_model_scores.append(null_model_score) \n",
    "\n",
    "\n",
    "average_mse_score = sum(mse_scores) / len(mse_scores)\n",
    "average_mae_score = sum(mae_scores) / len(mae_scores)\n",
    "average_r2_score = sum(r2_scores) / len(r2_scores)\n",
    "average_pearson_score = sum(pearson_scores) / len(pearson_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "study_score = Evaluation.mean_squared_error(raw_data['PRP'],raw_data['ERP'])\n",
    "print(f\"Previous study MSE score (ERP vs PRP): {study_score}\")\n",
    "print(f\"Average null model MSE score: {average_null_model_score}\")\n",
    "print(f\"Average MSE score for k={best_k}, g={round(best_g,2)}: {average_mse_score}\")\n",
    "print(f\"Average MAE score for k={best_k}, g={round(best_g,2)}: {average_mae_score}\")\n",
    "print(f\"Average r2 score for k={best_k}, g={round(best_g,2)}: {average_r2_score}\")\n",
    "print(f\"Average pearson score for k={best_k}, g={round(best_g,2)}: {average_pearson_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edited KNN ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.83073308779512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = statistics.stdev(data_3[config['target_column']])\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Epsilon ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with e=0.1: 38778.07607077246\n",
      "Average score with e=0.3: 30175.29096799274\n",
      "Average score with e=0.5: 28840.71963897106\n",
      "Average score with e=0.7: 30513.332061414454\n",
      "Average score with e=0.9: 27420.17226756271\n",
      "Average score with e=1.1: 26313.910440406624\n",
      "Average score with e=1.3: 24855.84380880785\n",
      "Average score with e=1.5: 24464.997863820077\n",
      "Average score with e=1.7: 22967.24611659685\n",
      "Average score with e=1.9: 22948.6021755046\n",
      "Best e is 1.9 with the lowest average score of 22948.6021755046\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(0.1,2,0.2)\n",
    "scores_dict = {}\n",
    "\n",
    "for e in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        edited_train_set = knn_model.edited_knn_regression(train_set_1,epsilon=epsilon*e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(edited_train_set, edited_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(edited_train_set,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=best_k, gamma=gamma*best_g)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average score with e={round(e,2)}: {average_score}\")\n",
    "    scores_dict[e] = average_score\n",
    "\n",
    "best_e = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best e is {round(best_e,2)} with the lowest average MSE score of {scores_dict[best_e]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with k=1: 24474.57619047619\n",
      "Average score with k=2: 22948.6021755046\n",
      "Average score with k=3: 23417.646879133114\n",
      "Average score with k=4: 23214.49718321472\n",
      "Average score with k=5: 23913.237952320113\n",
      "Average score with k=6: 24213.783169836\n",
      "Average score with k=7: 24775.255090475788\n",
      "Average score with k=8: 25360.162611618678\n",
      "Average score with k=9: 25766.91935639654\n",
      "Best k is 2 with the lowest average score of 22948.6021755046\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,10,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        edited_train_set = knn_model.edited_knn_regression(train_set_1,epsilon=epsilon*best_e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(edited_train_set, edited_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(edited_train_set,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=k, gamma=gamma*best_g)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average MSE score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning gamma ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with g=0.4: 23044.22232910942\n",
      "Average score with g=0.6: 23026.51392144658\n",
      "Average score with g=0.8: 23009.47285176583\n",
      "Average score with g=1.0: 22993.13512696233\n",
      "Average score with g=1.2: 22977.530115185957\n",
      "Average score with g=1.4: 22962.68046607314\n",
      "Average score with g=1.6: 22948.6021755046\n",
      "Best g is 1.6 with the lowest average score of 22948.6021755046\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(0.4,1.6,0.2)\n",
    "scores_dict = {}\n",
    "\n",
    "for g in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        edited_train_set = knn_model.edited_knn_regression(train_set_1,epsilon=epsilon*best_e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(edited_train_set, edited_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(edited_train_set,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=best_k, gamma=g*gamma)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with g={round(g,2)}: {average_score}\")\n",
    "    scores_dict[g] = average_score\n",
    "\n",
    "best_g = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best g is {round(best_g,2)} with the lowest average MSE score of {scores_dict[best_g]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous study MSE score (ERP vs PRP): 1737.3349282296651\n",
      "Average null model MSE score: 19073.865381296106\n",
      "Average MSE score for k=2, g=1.6: 7127.359746567536\n",
      "Average MAE score for k=2, g=1.6: 34.31842830852905\n",
      "Average r2 score for k=2, g=1.6: 0.6771279508406438\n",
      "Average pearson score for k=2, g=1.6: 0.8389989397901749\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "pearson_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "    edited_train_set = knn_model.edited_knn_regression(train_set,epsilon=epsilon*best_e)\n",
    "\n",
    "    data_train_standardized = data_processor.standardize_data(edited_train_set, edited_train_set, features=features)\n",
    "    data_test_standardized = data_processor.standardize_data(edited_train_set,test_set,features=features)  \n",
    "\n",
    "    predictions_1 = knn_model.knn_regression(data_test_standardized, data_train_standardized, k=best_k, gamma=best_g*gamma)['Predicted Value']\n",
    "\n",
    "    mse_score = Evaluation.mean_squared_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    mae_score = Evaluation.mean_absolute_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    r2_score = Evaluation.r2_coefficient(data_test_standardized[config['target_column']], predictions_1)\n",
    "    pearson_score = Evaluation.pearsons_correlation(data_test_standardized[config['target_column']], predictions_1)\n",
    "\n",
    "    mse_scores.append(mse_score)\n",
    "    mae_scores.append(mae_score)\n",
    "    r2_scores.append(r2_score)\n",
    "    pearson_scores.append(pearson_score)\n",
    "    \n",
    "    null_model_prediction = null_model.naive_regression(test_set)\n",
    "    null_model_score = Evaluation.mean_squared_error(test_set[config['target_column']],null_model_prediction)\n",
    "    null_model_scores.append(null_model_score) \n",
    "\n",
    "average_mse_score = sum(mse_scores) / len(mse_scores)\n",
    "average_mae_score = sum(mae_scores) / len(mae_scores)\n",
    "average_r2_score = sum(r2_scores) / len(r2_scores)\n",
    "average_pearson_score = sum(pearson_scores) / len(pearson_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "study_score = Evaluation.mean_squared_error(raw_data['PRP'],raw_data['ERP'])\n",
    "print(f\"Previous study MSE score (ERP vs PRP): {study_score}\")\n",
    "print(f\"Average null model MSE score: {average_null_model_score}\")\n",
    "print(f\"Average MSE score for k={best_k}, g={round(best_g,2)}: {average_mse_score}\")\n",
    "print(f\"Average MAE score for k={best_k}, g={round(best_g,2)}: {average_mae_score}\")\n",
    "print(f\"Average r2 score for k={best_k}, g={round(best_g,2)}: {average_r2_score}\")\n",
    "print(f\"Average pearson score for k={best_k}, g={round(best_g,2)}: {average_pearson_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensed Knn ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Epsilon ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with e=0.1: 16831.909325167486\n",
      "Average score with e=0.2: 16622.10938873105\n",
      "Average score with e=0.3: 16676.972152913604\n",
      "Average score with e=0.4: 16597.17988102914\n",
      "Average score with e=0.5: 17271.058594167604\n",
      "Average score with e=0.6: 17124.081283354746\n",
      "Average score with e=0.7: 17799.686305741547\n",
      "Average score with e=0.8: 17648.96196470669\n",
      "Average score with e=0.9: 18461.582560444607\n",
      "Average score with e=1.0: 18377.353322048326\n",
      "Average score with e=1.1: 21654.81205508003\n",
      "Average score with e=1.2: 22051.750319240764\n",
      "Average score with e=1.3: 21005.169735518288\n",
      "Average score with e=1.4: 22102.608294654634\n",
      "Best e is 0.4 with the lowest average score of 16597.17988102914\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(0.1,1.5,0.1)\n",
    "scores_dict = {}\n",
    "\n",
    "for e in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        condesed_train_set = knn_model.condensed_knn_regression(train_set_1,epsilon=epsilon*e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(condesed_train_set, condesed_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(condesed_train_set,data_val,features=features)\n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=best_k, gamma=gamma*best_g)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with e={round(e,2)}: {average_score}\")\n",
    "    scores_dict[e] = average_score\n",
    "\n",
    "best_e = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best e is {round(best_e,2)} with the lowest average MSE score of {scores_dict[best_e]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with k=1: 17034.11904761905\n",
      "Average score with k=2: 16550.627740105392\n",
      "Average score with k=3: 17074.35149938288\n",
      "Average score with k=4: 17889.902595161308\n",
      "Average score with k=5: 18579.63829344037\n",
      "Average score with k=6: 20596.358101661066\n",
      "Average score with k=7: 21998.601298828304\n",
      "Average score with k=8: 23560.51594537509\n",
      "Average score with k=9: 24401.306788125607\n",
      "Best k is 2 with the lowest average score of 16550.627740105392\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,10,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        condesed_train_set = knn_model.condensed_knn_regression(train_set_1,epsilon=epsilon*best_e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(condesed_train_set, condesed_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(condesed_train_set,data_val,features=features)\n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=k, gamma=gamma*best_g)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average MSE score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Gamma ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score with g=0.4: 16766.557055963276\n",
      "Average score with g=0.6: 17206.52999071452\n",
      "Average score with g=0.8: 16927.210128364364\n",
      "Average score with g=1.0: 16732.402670162737\n",
      "Average score with g=1.2: 16914.488121853763\n",
      "Average score with g=1.4: 16754.375624588796\n",
      "Average score with g=1.6: 16735.741788337487\n",
      "Best g is 1.0 with the lowest average score of 16732.402670162737\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(0.4,1.6,0.2)\n",
    "scores_dict = {}\n",
    "\n",
    "for g in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "        \n",
    "        condesed_train_set = knn_model.condensed_knn_regression(train_set_1,epsilon=epsilon*best_e)\n",
    "\n",
    "        data_train_standardized = data_processor.standardize_data(condesed_train_set, condesed_train_set, features=features)\n",
    "        data_val_standardized = data_processor.standardize_data(condesed_train_set,data_val,features=features)  \n",
    "\n",
    "        predictions_1 = knn_model.knn_regression(data_val_standardized, data_train_standardized, k=best_k, gamma=g*gamma)['Predicted Value']\n",
    "        score_1 = Evaluation().mean_squared_error(data_val_standardized[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average MSE score with g={round(g,2)}: {average_score}\")\n",
    "    scores_dict[g] = average_score\n",
    "\n",
    "best_g = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best g is {round(best_g,2)} with the lowest average MSE score of {scores_dict[best_g]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous study MSE score (ERP vs PRP): 1737.3349282296651\n",
      "Average null model MSE score: 19073.865381296106\n",
      "Average MSE score for k=2, g=1.0: 7988.312280238902\n",
      "Average MAE score for k=2, g=1.0: 39.19266728667226\n",
      "Average r2 score for k=2, g=1.0: 0.5946295516054472\n",
      "Average pearson score for k=2, g=1.0: 0.8051534168319087\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "pearson_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "    condesed_train_set_train_set = knn_model.condensed_knn_regression(train_set,epsilon=epsilon*best_e)\n",
    "\n",
    "    data_train_standardized = data_processor.standardize_data(condesed_train_set_train_set, condesed_train_set_train_set, features=features)\n",
    "    data_test_standardized = data_processor.standardize_data(condesed_train_set_train_set,test_set,features=features)  \n",
    "\n",
    "    predictions_1 = knn_model.knn_regression(data_test_standardized, data_train_standardized, k=best_k, gamma=best_g*gamma)['Predicted Value']\n",
    "\n",
    "    mse_score = Evaluation.mean_squared_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    mae_score = Evaluation.mean_absolute_error(data_test_standardized[config['target_column']], predictions_1)\n",
    "    r2_score = Evaluation.r2_coefficient(data_test_standardized[config['target_column']], predictions_1)\n",
    "    pearson_score = Evaluation.pearsons_correlation(data_test_standardized[config['target_column']], predictions_1)\n",
    "\n",
    "    mse_scores.append(mse_score)\n",
    "    mae_scores.append(mae_score)\n",
    "    r2_scores.append(r2_score)\n",
    "    pearson_scores.append(pearson_score)\n",
    "    \n",
    "    null_model_prediction = null_model.naive_regression(test_set)\n",
    "    null_model_score = Evaluation.mean_squared_error(test_set[config['target_column']],null_model_prediction)\n",
    "    null_model_scores.append(null_model_score) \n",
    "\n",
    "average_mse_score = sum(mse_scores) / len(mse_scores)\n",
    "average_mae_score = sum(mae_scores) / len(mae_scores)\n",
    "average_r2_score = sum(r2_scores) / len(r2_scores)\n",
    "average_pearson_score = sum(pearson_scores) / len(pearson_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "study_score = Evaluation.mean_squared_error(raw_data['PRP'],raw_data['ERP'])\n",
    "print(f\"Previous study MSE score (ERP vs PRP): {study_score}\")\n",
    "print(f\"Average null model MSE score: {average_null_model_score}\")\n",
    "print(f\"Average MSE score for k={best_k}, g={round(best_g,2)}: {average_mse_score}\")\n",
    "print(f\"Average MAE score for k={best_k}, g={round(best_g,2)}: {average_mae_score}\")\n",
    "print(f\"Average r2 score for k={best_k}, g={round(best_g,2)}: {average_r2_score}\")\n",
    "print(f\"Average pearson score for k={best_k}, g={round(best_g,2)}: {average_pearson_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
