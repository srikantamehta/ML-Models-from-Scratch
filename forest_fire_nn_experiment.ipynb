{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from data_configs.configs import *\n",
    "from models.neural_networks import *\n",
    "from src.cross_validation import CrossValidation\n",
    "import numpy as np\n",
    "\n",
    "config = forest_fires_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srikanta\\Documents\\Intro to Machine Learning\\programming_assignment_1\\src\\data_preprocessor.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "data_1 = data_processor.impute_missing_values(raw_data)\n",
    "data_2 = data_processor.encode_nominal_features(data_1)\n",
    "data_3 = data_processor.encode_ordinal_features(data_2)\n",
    "data_4 = data_processor.standardize_data(data_3,data_3,features=['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH',\n",
    "       'wind', 'rain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.007337</td>\n",
       "      <td>0.569309</td>\n",
       "      <td>-1.966538</td>\n",
       "      <td>0.357375</td>\n",
       "      <td>-0.805180</td>\n",
       "      <td>-1.322045</td>\n",
       "      <td>-1.828706</td>\n",
       "      <td>-0.860113</td>\n",
       "      <td>-1.840857</td>\n",
       "      <td>0.411326</td>\n",
       "      <td>1.497164</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.007337</td>\n",
       "      <td>-0.243765</td>\n",
       "      <td>1.109046</td>\n",
       "      <td>-1.089853</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>-1.178399</td>\n",
       "      <td>0.488418</td>\n",
       "      <td>-0.509195</td>\n",
       "      <td>-0.153130</td>\n",
       "      <td>-0.691786</td>\n",
       "      <td>-1.740070</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.007337</td>\n",
       "      <td>-0.243765</td>\n",
       "      <td>1.109046</td>\n",
       "      <td>0.839784</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>-1.048806</td>\n",
       "      <td>0.560173</td>\n",
       "      <td>-0.509195</td>\n",
       "      <td>-0.738668</td>\n",
       "      <td>-0.691786</td>\n",
       "      <td>-1.516813</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.439531</td>\n",
       "      <td>1.382383</td>\n",
       "      <td>-1.966538</td>\n",
       "      <td>0.357375</td>\n",
       "      <td>0.191177</td>\n",
       "      <td>-1.211188</td>\n",
       "      <td>-1.896429</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>-1.823636</td>\n",
       "      <td>3.230391</td>\n",
       "      <td>-0.009824</td>\n",
       "      <td>0.602572</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.439531</td>\n",
       "      <td>1.382383</td>\n",
       "      <td>-1.966538</td>\n",
       "      <td>1.322193</td>\n",
       "      <td>-0.243597</td>\n",
       "      <td>-0.930142</td>\n",
       "      <td>-1.796859</td>\n",
       "      <td>0.126843</td>\n",
       "      <td>-1.289763</td>\n",
       "      <td>3.352959</td>\n",
       "      <td>-1.237741</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.289244</td>\n",
       "      <td>-1.056839</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>1.322193</td>\n",
       "      <td>-1.638496</td>\n",
       "      <td>-0.845829</td>\n",
       "      <td>0.474309</td>\n",
       "      <td>-1.561947</td>\n",
       "      <td>1.534597</td>\n",
       "      <td>-0.753070</td>\n",
       "      <td>-0.735411</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>6.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>-1.153631</td>\n",
       "      <td>-0.243765</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>1.322193</td>\n",
       "      <td>-1.638496</td>\n",
       "      <td>-0.845829</td>\n",
       "      <td>0.474309</td>\n",
       "      <td>-1.561947</td>\n",
       "      <td>0.518517</td>\n",
       "      <td>1.637006</td>\n",
       "      <td>0.994835</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>54.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>1.007337</td>\n",
       "      <td>-0.243765</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>1.322193</td>\n",
       "      <td>-1.638496</td>\n",
       "      <td>-0.845829</td>\n",
       "      <td>0.474309</td>\n",
       "      <td>-1.561947</td>\n",
       "      <td>0.397965</td>\n",
       "      <td>1.575722</td>\n",
       "      <td>1.497164</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>11.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>-1.585825</td>\n",
       "      <td>-0.243765</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>0.839784</td>\n",
       "      <td>0.680298</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.269122</td>\n",
       "      <td>0.499693</td>\n",
       "      <td>1.155720</td>\n",
       "      <td>-0.140230</td>\n",
       "      <td>-0.009824</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.575144</td>\n",
       "      <td>-1.056839</td>\n",
       "      <td>1.548415</td>\n",
       "      <td>-1.089853</td>\n",
       "      <td>-2.018923</td>\n",
       "      <td>-1.684282</td>\n",
       "      <td>-1.778719</td>\n",
       "      <td>-1.737406</td>\n",
       "      <td>-1.220876</td>\n",
       "      <td>-0.814354</td>\n",
       "      <td>0.269248</td>\n",
       "      <td>-0.073197</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X         Y     month       day      FFMC       DMC        DC  \\\n",
       "0    1.007337  0.569309 -1.966538  0.357375 -0.805180 -1.322045 -1.828706   \n",
       "1    1.007337 -0.243765  1.109046 -1.089853 -0.008094 -1.178399  0.488418   \n",
       "2    1.007337 -0.243765  1.109046  0.839784 -0.008094 -1.048806  0.560173   \n",
       "3    1.439531  1.382383 -1.966538  0.357375  0.191177 -1.211188 -1.896429   \n",
       "4    1.439531  1.382383 -1.966538  1.322193 -0.243597 -0.930142 -1.796859   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512 -0.289244 -1.056839  0.230308  1.322193 -1.638496 -0.845829  0.474309   \n",
       "513 -1.153631 -0.243765  0.230308  1.322193 -1.638496 -0.845829  0.474309   \n",
       "514  1.007337 -0.243765  0.230308  1.322193 -1.638496 -0.845829  0.474309   \n",
       "515 -1.585825 -0.243765  0.230308  0.839784  0.680298  0.548471  0.269122   \n",
       "516  0.575144 -1.056839  1.548415 -1.089853 -2.018923 -1.684282 -1.778719   \n",
       "\n",
       "          ISI      temp        RH      wind      rain   area  \n",
       "0   -0.860113 -1.840857  0.411326  1.497164 -0.073197   0.00  \n",
       "1   -0.509195 -0.153130 -0.691786 -1.740070 -0.073197   0.00  \n",
       "2   -0.509195 -0.738668 -0.691786 -1.516813 -0.073197   0.00  \n",
       "3   -0.004751 -1.823636  3.230391 -0.009824  0.602572   0.00  \n",
       "4    0.126843 -1.289763  3.352959 -1.237741 -0.073197   0.00  \n",
       "..        ...       ...       ...       ...       ...    ...  \n",
       "512 -1.561947  1.534597 -0.753070 -0.735411 -0.073197   6.44  \n",
       "513 -1.561947  0.518517  1.637006  0.994835 -0.073197  54.29  \n",
       "514 -1.561947  0.397965  1.575722  1.497164 -0.073197  11.16  \n",
       "515  0.499693  1.155720 -0.140230 -0.009824 -0.073197   0.00  \n",
       "516 -1.737406 -1.220876 -0.814354  0.269248 -0.073197   0.00  \n",
       "\n",
       "[517 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested params: {'lr': 0.01, 'epochs': 16000}, Score: 11819.780022251009\n",
      "Tested params: {'lr': 0.0001, 'epochs': 13000}, Score: 11828.787510428112\n",
      "Tested params: {'lr': 0.01, 'epochs': 17000}, Score: 11819.780931823669\n",
      "Tested params: {'lr': 0.001, 'epochs': 7000}, Score: 11820.36575148849\n",
      "Tested params: {'lr': 0.0001, 'epochs': 16000}, Score: 11824.882212850265\n",
      "Tested params: {'lr': 0.001, 'epochs': 10000}, Score: 11820.133409512215\n",
      "Tested params: {'lr': 0.001, 'epochs': 14000}, Score: 11819.993792405912\n",
      "Tested params: {'lr': 0.0001, 'epochs': 5000}, Score: 11889.59319662521\n",
      "Tested params: {'lr': 0.0001, 'epochs': 6000}, Score: 11872.823363443113\n",
      "Tested params: {'lr': 0.0001, 'epochs': 9000}, Score: 11843.760521772716\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000}, Score: 11825.828948611146\n",
      "Tested params: {'lr': 0.01, 'epochs': 14000}, Score: 11819.788488746464\n",
      "Tested params: {'lr': 0.001, 'epochs': 3000}, Score: 11821.248063406603\n",
      "Tested params: {'lr': 0.0001, 'epochs': 11000}, Score: 11834.202357277962\n",
      "Tested params: {'lr': 0.0001, 'epochs': 20000}, Score: 11822.75623173856\n",
      "Best parameters: {'lr': 0.01, 'epochs': 16000}, Best score: 11819.780022251009\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.01,0.001,0.0001],\n",
    "    'epochs': np.linspace(1000, 20000, num=20).astype(int).tolist()\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_linear = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-1]\n",
    "        y_train = train_data[:,-1:]\n",
    "\n",
    "        data_test = data_val.to_numpy()\n",
    "        X_val = data_test[:,:-1]\n",
    "        y_val = data_test[:,-1:]\n",
    "        \n",
    "        linear = LinearNetwork(config)\n",
    "\n",
    "        _, val_losses = linear.linear_regression(X_train,y_train,X_val,y_val,epochs=params['epochs'],lr=params['lr'],patience=500)\n",
    "\n",
    "        score = np.min(val_losses)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 20000:\n",
    "            print(f\"Skipping params: {params} due to high score: {score}\")\n",
    "            break  # Exit the current for-loop\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_linear = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_linear}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested params: {'lr': 1e-05, 'epochs': 10000, 'n_hidden': 24}, Score: 11498.186988527003\n",
      "Tested params: {'lr': 1e-05, 'epochs': 14000, 'n_hidden': 24}, Score: 11513.23804232743\n",
      "Tested params: {'lr': 0.0001, 'epochs': 14000, 'n_hidden': 24}, Score: 11219.388976638676\n",
      "Tested params: {'lr': 1e-06, 'epochs': 10000, 'n_hidden': 69}, Score: 11801.315391473094\n",
      "Tested params: {'lr': 1e-06, 'epochs': 12000, 'n_hidden': 69}, Score: 11801.243392953105\n",
      "Tested params: {'lr': 1e-07, 'epochs': 8000, 'n_hidden': 39}, Score: 12109.51509712179\n",
      "Tested params: {'lr': 0.0001, 'epochs': 16000, 'n_hidden': 39}, Score: 11054.069701419236\n",
      "Tested params: {'lr': 0.0001, 'epochs': 12000, 'n_hidden': 24}, Score: 11192.01457359001\n",
      "Tested params: {'lr': 1e-07, 'epochs': 10000, 'n_hidden': 39}, Score: 12094.17687331309\n",
      "Tested params: {'lr': 0.0001, 'epochs': 6000, 'n_hidden': 39}, Score: 11119.10452062768\n",
      "Tested params: {'lr': 0.0001, 'epochs': 14000, 'n_hidden': 39}, Score: 11117.69384485923\n",
      "Tested params: {'lr': 1e-06, 'epochs': 8000, 'n_hidden': 69}, Score: 11801.397026444236\n",
      "Tested params: {'lr': 1e-05, 'epochs': 6000, 'n_hidden': 24}, Score: 11497.371540579723\n",
      "Tested params: {'lr': 0.0001, 'epochs': 6000, 'n_hidden': 54}, Score: 11186.442635494044\n",
      "Tested params: {'lr': 0.0001, 'epochs': 2000, 'n_hidden': 54}, Score: 11203.963191430685\n",
      "Best parameters: {'lr': 0.0001, 'epochs': 16000, 'n_hidden': 39}, Best score: 11054.069701419236\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(2000, 20000, 2000).tolist(),\n",
    "    'n_hidden': np.arange(2*X_val.shape[1], 80, 15)\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_ffn = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "           \n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-1]\n",
    "        y_train = train_data[:,-1:]\n",
    "\n",
    "        data_test = data_val.to_numpy()\n",
    "        X_val = data_test[:,:-1]\n",
    "        y_val = data_test[:,-1:]\n",
    "\n",
    "        ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=params['n_hidden'],n_hidden_2=params['n_hidden'],n_output=y_train.shape[1])\n",
    "\n",
    "        _, val_losses, _ = ffn.train(X_train,y_train,X_val,y_val,epochs=params['epochs'],lr=params['lr'],patience=500)\n",
    "\n",
    "        score = np.min(val_losses)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 20000:\n",
    "            print(f\"Skipping params: {params} due to high score: {score}\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_ffn = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_ffn}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping params: {'lr': 1e-07, 'epochs': 5000, 'n_encoder': 10} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_encoder': 10}, Score: 0.8896446470171627\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 3} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 3}, Score: 0.36345232183022846\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 5000, 'n_encoder': 4} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 5000, 'n_encoder': 4}, Score: 0.8884484672388151\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 3} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 3}, Score: 0.3635605953150631\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 19000, 'n_encoder': 3} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 19000, 'n_encoder': 3}, Score: 0.8889814781702957\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 13000, 'n_encoder': 4} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 13000, 'n_encoder': 4}, Score: 0.8869433993046425\n",
      "Skipping params: {'lr': 1e-05, 'epochs': 15000, 'n_encoder': 2} due to high score.\n",
      "Tested params: {'lr': 1e-05, 'epochs': 15000, 'n_encoder': 2}, Score: 0.5482022062476095\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 7000, 'n_encoder': 6} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 7000, 'n_encoder': 6}, Score: 0.21485489223177717\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 9000, 'n_encoder': 3} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 9000, 'n_encoder': 3}, Score: 0.36774516988530764\n",
      "Skipping params: {'lr': 0.001, 'epochs': 15000, 'n_encoder': 5} due to high score.\n",
      "Tested params: {'lr': 0.001, 'epochs': 15000, 'n_encoder': 5}, Score: 0.2481483198780132\n",
      "Tested params: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 9}, Score: 0.05018200780081529\n",
      "Skipping params: {'lr': 0.0001, 'epochs': 11000, 'n_encoder': 4} due to high score.\n",
      "Tested params: {'lr': 0.0001, 'epochs': 11000, 'n_encoder': 4}, Score: 0.2741384961074807\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 13000, 'n_encoder': 4} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 13000, 'n_encoder': 4}, Score: 0.8875237126457928\n",
      "Skipping params: {'lr': 1e-07, 'epochs': 7000, 'n_encoder': 6} due to high score.\n",
      "Tested params: {'lr': 1e-07, 'epochs': 7000, 'n_encoder': 6}, Score: 0.889366776244006\n",
      "Skipping params: {'lr': 1e-06, 'epochs': 5000, 'n_encoder': 10} due to high score.\n",
      "Tested params: {'lr': 1e-06, 'epochs': 5000, 'n_encoder': 10}, Score: 0.8883971973564776\n",
      "Best parameters: {'lr': 0.0001, 'epochs': 15000, 'n_encoder': 9}, Best score: 0.05018200780081529\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.001,0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(5000, 20000, 2000).tolist(),\n",
    "    'n_encoder': np.arange(2,X_val.shape[1]-1,1).tolist()\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_auto = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-1]\n",
    "        y_train = train_data[:,-1:]\n",
    "\n",
    "        data_test = data_val.to_numpy()\n",
    "        X_val = data_test[:,:-1]\n",
    "        y_val = data_test[:,-1:]\n",
    "\n",
    "        # Check if the shapes of X_train and X_val are not equal\n",
    "        if X_train.shape[1] != X_val.shape[1]:\n",
    "            # print(f\"Shape mismatch between training and validation sets, skipping params: {params}\")\n",
    "            continue\n",
    "\n",
    "        autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=params['n_encoder'])\n",
    "\n",
    "        losses = autoE.train(X_train, max_epochs=params['epochs'], lr=params['lr'])\n",
    "\n",
    "        score = np.min(losses)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 0.2:\n",
    "            print(f\"Skipping params: {params} due to high score.\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_auto = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_auto}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03222871492591887"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'])\n",
    "losses = autoE.train(X_train, max_epochs=best_params_auto['epochs'], lr=best_params_auto['lr'])\n",
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'lr': 0.001, 'epochs': 13000, 'n_hidden_2': 12}\n",
      "Tested params: {'lr': 0.001, 'epochs': 13000, 'n_hidden_2': 12}, Score: 11370.621890064813\n",
      "Testing params: {'lr': 1e-06, 'epochs': 15000, 'n_hidden_2': 12}\n",
      "Tested params: {'lr': 1e-06, 'epochs': 15000, 'n_hidden_2': 12}, Score: 11779.616973655495\n",
      "Testing params: {'lr': 1e-07, 'epochs': 17000, 'n_hidden_2': 12}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 17000, 'n_hidden_2': 12}, Score: 11904.929435976323\n",
      "Testing params: {'lr': 1e-07, 'epochs': 13000, 'n_hidden_2': 42}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 13000, 'n_hidden_2': 42}, Score: 11896.836321112314\n",
      "Testing params: {'lr': 1e-06, 'epochs': 5000, 'n_hidden_2': 27}\n",
      "Tested params: {'lr': 1e-06, 'epochs': 5000, 'n_hidden_2': 27}, Score: 11793.131274095216\n",
      "Testing params: {'lr': 1e-05, 'epochs': 11000, 'n_hidden_2': 12}\n",
      "Tested params: {'lr': 1e-05, 'epochs': 11000, 'n_hidden_2': 12}, Score: 11664.505041509288\n",
      "Testing params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 57}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 57}, Score: 12111.894136480289\n",
      "Testing params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden_2': 12}\n",
      "Tested params: {'lr': 1e-05, 'epochs': 15000, 'n_hidden_2': 12}, Score: 11638.407615897573\n",
      "Testing params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden_2': 72}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden_2': 72}, Score: 11249.69062501283\n",
      "Testing params: {'lr': 1e-05, 'epochs': 19000, 'n_hidden_2': 27}\n",
      "Tested params: {'lr': 1e-05, 'epochs': 19000, 'n_hidden_2': 27}, Score: 11667.33808557228\n",
      "Testing params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 72}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 5000, 'n_hidden_2': 72}, Score: 12106.389462475798\n",
      "Testing params: {'lr': 1e-07, 'epochs': 13000, 'n_hidden_2': 42}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 13000, 'n_hidden_2': 42}, Score: 11897.594004294147\n",
      "Testing params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden_2': 87}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 13000, 'n_hidden_2': 87}, Score: 11448.056683338884\n",
      "Testing params: {'lr': 0.0001, 'epochs': 5000, 'n_hidden_2': 42}\n",
      "Tested params: {'lr': 0.0001, 'epochs': 5000, 'n_hidden_2': 42}, Score: 11296.623273101524\n",
      "Testing params: {'lr': 1e-07, 'epochs': 19000, 'n_hidden_2': 87}\n",
      "Tested params: {'lr': 1e-07, 'epochs': 19000, 'n_hidden_2': 87}, Score: 11823.68267428725\n",
      "Best parameters: {'lr': 0.0001, 'epochs': 11000, 'n_hidden_2': 72}, Best score: 11249.69062501283\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "iterations = 15\n",
    "\n",
    "param_space = {\n",
    "    'lr': [0.001,0.0001,0.00001,0.000001,0.0000001],\n",
    "    'epochs': np.arange(5000, 20000, 2000).tolist(),\n",
    "    'n_hidden_2': np.arange(X_val.shape[1], 100, 15)\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params_combined = {}\n",
    "\n",
    "for _ in range(iterations):\n",
    "\n",
    "    # Randomly select parameters\n",
    "    params = {key: random.choice(value) for key, value in param_space.items()}\n",
    "    scores = []\n",
    "    print(f\"Testing params: {params}\")\n",
    "\n",
    "    for i, (train_set, _) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "    \n",
    "        train_data = train_set.to_numpy()\n",
    "        X_train = train_data[:,:-1]\n",
    "        y_train = train_data[:,-1:]\n",
    "\n",
    "        data_test = data_val.to_numpy()\n",
    "        X_val = data_test[:,:-1]\n",
    "        y_val = data_test[:,-1:]\n",
    "\n",
    "        # Check if the shapes of X_train and X_val are not equal\n",
    "        if X_train.shape[1] != X_val.shape[1]:\n",
    "            # print(f\"Shape mismatch between training and validation sets, skipping params: {params}\")\n",
    "            continue\n",
    "        \n",
    "        combined = CombinedModel(autoE,n_hidden_2=params['n_hidden_2'],n_output=y_val.shape[1])\n",
    "\n",
    "        _, val_losses, _ = combined.train(X_train,y_train,X_val,y_val,epochs=params['epochs'], lr=params['lr'],patience=500)\n",
    "\n",
    "\n",
    "        score = np.min(val_losses)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Skip to the next parameter set if score > 0.2\n",
    "        if score > 20000:\n",
    "            print(f\"Skipping params: {params} due to high score: {score}\")\n",
    "            break  # Exit the current for-loop\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    print(f\"Tested params: {params}, Score: {avg_score}\")\n",
    "    \n",
    "    if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_params_combined = params\n",
    "        \n",
    "\n",
    "print(f\"Best parameters: {best_params_combined}, Best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Tested params: {'lr': 0.01, 'epochs': 16000}, Average Score: 2237.108078609387\n",
      "FFN Model Tested params: {'lr': 0.0001, 'epochs': 16000, 'n_hidden': 39}, Average Score: 3521.054371271656\n",
      "Combined Model Tested params: {'lr': 0.0001, 'epochs': 11000, 'n_hidden_2': 72}, Average Score: 3622.0208441939803\n",
      "Linear Model Scores: [1304.7808768035572, 3386.636341275841, 3752.5912184967847, 871.8372902787091, 726.1568087436916, 3686.7443507585876, 3352.404813505192, 918.5268670484784, 3276.221611414388, 1095.180607768637]\n",
      "FFN Model Scores: [2603.0794672276, 4040.692980443502, 4463.162019507141, 2277.2606742025732, 4373.690773184636, 4137.253112747011, 4119.396057970436, 2653.659881415204, 3764.771655659738, 2777.5770903587177]\n",
      "Combined Model Scores: [2627.8988420024116, 3865.602814941358, 4311.933490705745, 3987.0219743719736, 4767.137510008973, 4241.772235046759, 4409.962990530322, 2220.242034365619, 3895.3078579055236, 1893.3286920611174]\n"
     ]
    }
   ],
   "source": [
    "linear_scores = []\n",
    "ffn_scores = []\n",
    "combined_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=False)):\n",
    "\n",
    "    train_data = train_set.to_numpy()\n",
    "    X_train = train_data[:,:-1]\n",
    "    y_train = train_data[:,-1:]\n",
    "\n",
    "    test_data = test_set.to_numpy()\n",
    "    X_test = test_data[:,:-1]\n",
    "    y_test = test_data[:,-1:]\n",
    "\n",
    "    # Check if the shapes of X_train and X_val are not equal\n",
    "    if X_train.shape[1] != X_test.shape[1]:\n",
    "        # print(f\"Shape mismatch between training and validation sets, skipping params: {params}\")\n",
    "        continue\n",
    "    \n",
    "    linear = LinearNetwork(config)\n",
    "    _, linear_val_losses = linear.linear_regression(X_train,y_train,X_test,y_test,epochs=best_params_linear['epochs'],lr=best_params_linear['lr'],patience=np.inf)\n",
    "\n",
    "    ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=best_params_ffn['n_hidden'],n_hidden_2=best_params_ffn['n_hidden'],n_output=y_train.shape[1])\n",
    "    _, ffn_val_losses, _ = ffn.train(X_train,y_train,X_test,y_test,epochs=best_params_ffn['epochs'],lr=best_params_ffn['lr'],patience=np.inf)\n",
    "\n",
    "    autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=best_params_auto['n_encoder'])\n",
    "    losses = autoE.train(X_train, max_epochs=best_params_auto['epochs'], lr=best_params_auto['lr'])\n",
    "    combined = CombinedModel(autoE,n_hidden_2=best_params_combined['n_hidden_2'],n_output=y_test.shape[1])\n",
    "    _, combined_val_losses, _ = combined.train(X_train,y_train,X_test,y_test,epochs=best_params_combined['epochs'], lr=best_params_combined['lr'],patience=np.inf)\n",
    "\n",
    "\n",
    "    linear_score = linear_val_losses[-1]\n",
    "    ffn_score = ffn_val_losses[-1]\n",
    "    combined_score = combined_val_losses[-1]\n",
    "    \n",
    "    linear_scores.append(linear_score)\n",
    "    ffn_scores.append(ffn_score)\n",
    "    combined_scores.append(combined_score)\n",
    "\n",
    "avg_score_linear = np.mean(linear_scores)\n",
    "avg_score_ffn = np.mean(ffn_scores)\n",
    "avg_score_combined = np.mean(combined_scores)\n",
    "\n",
    "print(f\"Linear Model Tested params: {best_params_linear}, Average Score: {avg_score_linear}\")\n",
    "print(f\"FFN Model Tested params: {best_params_ffn}, Average Score: {avg_score_ffn}\")\n",
    "print(f\"Combined Model Tested params: {best_params_combined}, Average Score: {avg_score_combined}\")\n",
    "\n",
    "print(f\"Linear Model Scores: {linear_scores}\")\n",
    "print(f\"FFN Model Scores: {ffn_scores}\")\n",
    "print(f\"Combined Model Scores: {combined_scores}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model vs. FFN Model: t-statistic = -2.5691178132538472, p-value = 0.0193113665656318\n",
      "Linear Model vs. Combined Model: t-statistic = -2.6222846242724414, p-value = 0.01727001207036789\n",
      "FFN Model vs. Combined Model: t-statistic = -0.24426083332465923, p-value = 0.8097913806026779\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(linear_scores, ffn_scores)\n",
    "print(f\"Linear Model vs. FFN Model: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Comparing Linear Model vs. Combined Model\n",
    "t_stat, p_val = stats.ttest_ind(linear_scores, combined_scores)\n",
    "print(f\"Linear Model vs. Combined Model: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Comparing FFN Model vs. Combined Model\n",
    "t_stat, p_val = stats.ttest_ind(ffn_scores, combined_scores)\n",
    "print(f\"FFN Model vs. Combined Model: t-statistic = {t_stat}, p-value = {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA result: F-statistic = 5.112869414658074, p-value = 0.013092600437659113\n",
      "     Multiple Comparison of Means - Tukey HSD, FWER=0.05     \n",
      "=============================================================\n",
      " group1  group2  meandiff  p-adj    lower      upper   reject\n",
      "-------------------------------------------------------------\n",
      "Combined    FFN  -100.9665 0.9762 -1298.2519  1096.319  False\n",
      "Combined Linear -1384.9128 0.0209 -2582.1982 -187.6273   True\n",
      "     FFN Linear -1283.9463 0.0337 -2481.2318  -86.6608   True\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine all scores into a single array, and create an array of labels\n",
    "scores = np.concatenate([linear_scores, ffn_scores, combined_scores])\n",
    "labels = ['Linear'] * len(linear_scores) + ['FFN'] * len(ffn_scores) + ['Combined'] * len(combined_scores)\n",
    "\n",
    "# Conduct ANOVA\n",
    "anova_result = stats.f_oneway(linear_scores, ffn_scores, combined_scores)\n",
    "print(f\"ANOVA result: F-statistic = {anova_result.statistic}, p-value = {anova_result.pvalue}\")\n",
    "\n",
    "# If ANOVA shows significant differences, conduct post-hoc testing with Tukey's HSD\n",
    "if anova_result.pvalue < 0.05:\n",
    "    tukey = pairwise_tukeyhsd(endog=scores, groups=labels, alpha=0.05)\n",
    "    print(tukey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_train.to_numpy()\n",
    "# X_train = data[:,:-1]\n",
    "# y_train = data[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = data_val.to_numpy()\n",
    "# X_val = data_test[:,:-1]\n",
    "# y_val = data_test[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoE = AutoEncoder(config,n_input=X_train.shape[1],n_encoder=4)\n",
    "\n",
    "# autoE.train(X_train, max_epochs=30000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = CombinedModel(autoE,n_hidden_2=24,n_output=1)\n",
    "\n",
    "# MSEs, val_metrics, final_mse = combined.train(X_train,y_train,X_val,y_val,epochs=3000,lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = CombinedModel(autoE,n_hidden_2=100,n_output=1)\n",
    "\n",
    "# MSEs, val_metrics, final_mse = combined.train(X_train,y_train,X_val,y_val,epochs=5000,lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn = FeedForwardNetwork(config,n_input=X_train.shape[1],n_hidden_1=20,n_hidden_2=50,n_output=1)\n",
    "\n",
    "# MSEs, val_metrics, final_mse = ffn.train(X_train,y_train,X_val,y_val,5000,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(MSEs)\n",
    "# plt.plot(val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearNetwork(config)\n",
    "\n",
    "train_MSEs, val_MSEs = linear.linear_regression(X_train,y_train,X_val,y_val,epochs=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20f64903310>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_MSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_,output = ffn.forward_pass(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE = np.mean((y_val-output)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(val_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
