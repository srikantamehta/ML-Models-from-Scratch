{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import DataProcessor\n",
    "from src.cross_validation import CrossValidation\n",
    "from src.evaluation import Evaluation\n",
    "from models.knn import KNN\n",
    "from models.null_model import NullModelClassification\n",
    "from data_configs.configs import *\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "config = car_config\n",
    "data_processor = DataProcessor(config=config)\n",
    "cross_validator = CrossValidation(config=config)\n",
    "knn_model = KNN(config)\n",
    "null_model = NullModelClassification(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data_processor.load_data()\n",
    "\n",
    "data_1 = data_processor.impute_missing_values(raw_data)\n",
    "\n",
    "data_2 = data_processor.encode_nominal_features(data_1)\n",
    "\n",
    "data_3 = data_processor.encode_ordinal_features(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      buying  maint  doors  persons  lug_boot  safety  Class\n",
       "0          0      0      0        0         0       0  unacc\n",
       "1          0      0      0        0         0       1  unacc\n",
       "2          0      0      0        0         0       2  unacc\n",
       "3          0      0      0        0         1       0  unacc\n",
       "4          0      0      0        0         1       1  unacc\n",
       "...      ...    ...    ...      ...       ...     ...    ...\n",
       "1723       3      3      3        2         1       1   good\n",
       "1724       3      3      3        2         1       2  vgood\n",
       "1725       3      3      3        2         2       0  unacc\n",
       "1726       3      3      3        2         2       1   good\n",
       "1727       3      3      3        2         2       2  vgood\n",
       "\n",
       "[1728 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = cross_validator.random_partition(data_3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 Loss score with k=1: 0.11040462427745665\n",
      "Average 0-1 Loss score with k=2: 0.12283236994219653\n",
      "Average 0-1 Loss score with k=3: 0.08554913294797688\n",
      "Average 0-1 Loss score with k=4: 0.09190751445086706\n",
      "Average 0-1 Loss score with k=5: 0.07658959537572255\n",
      "Average 0-1 Loss score with k=6: 0.0861271676300578\n",
      "Average 0-1 Loss score with k=7: 0.07976878612716762\n",
      "Average 0-1 Loss score with k=8: 0.08439306358381501\n",
      "Average 0-1 Loss score with k=9: 0.08699421965317918\n",
      "Average 0-1 Loss score with k=10: 0.08439306358381501\n",
      "Average 0-1 Loss score with k=11: 0.09132947976878612\n",
      "Average 0-1 Loss score with k=12: 0.09421965317919076\n",
      "Average 0-1 Loss score with k=13: 0.09479768786127168\n",
      "Average 0-1 Loss score with k=14: 0.09190751445086705\n",
      "Best k is 5 with the lowest average 0-1 loss score of 0.07658959537572255\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        data_val_stand = data_processor.standardize_data(train_set_1, data_val, features=features)\n",
    "        data_train_stand = data_processor.standardize_data(train_set_1,train_set_1, features=features)\n",
    "\n",
    "        predictions_1 = knn_model.knn_classifier(data_val_stand, data_train_stand, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val_stand[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 Loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.2945007235890015\n",
      "Average null model Precision score: 0.4977297525974856\n",
      "Average null model Recall score: 0.7054992764109985\n",
      "Average null model F1 score: 0.5836758958553583\n",
      "Average KNN 0-1 score for k=5: 0.06816208393632418\n",
      "Average Precision score for k=5: 0.9325392039311449\n",
      "Average Recall score for k=5: 0.9318379160636759\n",
      "Average F1 score for k=5: 0.9296356367983849\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Lists to store null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    data_test_stand = data_processor.standardize_data(train_set, test_set, features=features)\n",
    "    data_train_stand = data_processor.standardize_data(train_set,train_set, features=features)\n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(data_test_stand, data_train_stand, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(data_test_stand[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(data_test_stand[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(data_test_stand[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(data_test_stand[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Evaluate null model\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edited KNN ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 loss score with k=1: 0.16647398843930636\n",
      "Average 0-1 loss score with k=2: 0.18150289017341042\n",
      "Average 0-1 loss score with k=3: 0.1598265895953757\n",
      "Average 0-1 loss score with k=4: 0.16271676300578033\n",
      "Average 0-1 loss score with k=5: 0.1676300578034682\n",
      "Average 0-1 loss score with k=6: 0.17283236994219653\n",
      "Average 0-1 loss score with k=7: 0.17254335260115608\n",
      "Average 0-1 loss score with k=8: 0.1713872832369942\n",
      "Average 0-1 loss score with k=9: 0.17803468208092485\n",
      "Average 0-1 loss score with k=10: 0.17947976878612718\n",
      "Average 0-1 loss score with k=11: 0.18612716763005782\n",
      "Average 0-1 loss score with k=12: 0.18670520231213875\n",
      "Average 0-1 loss score with k=13: 0.1921965317919075\n",
      "Average 0-1 loss score with k=14: 0.1945086705202312\n",
      "Best k is 3 with the lowest average 0-1 loss score of 0.1598265895953757\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        edited_train_set = knn_model.edited_knn_classificaton(train_set_1)\n",
    "\n",
    "        data_val_stand = data_processor.standardize_data(edited_train_set, data_val, features=features)\n",
    "        data_train_stand = data_processor.standardize_data(edited_train_set,edited_train_set, features=features)\n",
    "\n",
    "        predictions_1 = knn_model.knn_classifier(data_val_stand, data_train_stand, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val_stand[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.2945007235890015\n",
      "Average null model Precision score: 0.4977297525974856\n",
      "Average null model Recall score: 0.7054992764109985\n",
      "Average null model F1 score: 0.5836758958553583\n",
      "Average KNN 0-1 score for k=3: 0.14037626628075256\n",
      "Average Precision score for k=3: 0.8467414337010151\n",
      "Average Recall score for k=3: 0.8596237337192475\n",
      "Average F1 score for k=3: 0.8435838341675639\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Lists to store null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    edited_train_set = knn_model.edited_knn_classificaton(train_set)\n",
    "\n",
    "    data_test_stand = data_processor.standardize_data(edited_train_set, test_set, features=features)\n",
    "    data_train_stand = data_processor.standardize_data(edited_train_set,edited_train_set, features=features)\n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(data_test_stand, data_train_stand, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(data_test_stand[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(data_test_stand[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(data_test_stand[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(data_test_stand[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Evaluate null model\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensed Knn ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning k ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0-1 Loss score with k=1: 0.1121387283236994\n",
      "Average 0-1 Loss score with k=2: 0.1283236994219653\n",
      "Average 0-1 Loss score with k=3: 0.09277456647398843\n",
      "Average 0-1 Loss score with k=4: 0.10867052023121386\n",
      "Average 0-1 Loss score with k=5: 0.08092485549132947\n",
      "Average 0-1 Loss score with k=6: 0.09132947976878612\n",
      "Average 0-1 Loss score with k=7: 0.08092485549132947\n",
      "Average 0-1 Loss score with k=8: 0.08930635838150289\n",
      "Average 0-1 Loss score with k=9: 0.08121387283236994\n",
      "Average 0-1 Loss score with k=10: 0.09364161849710984\n",
      "Average 0-1 Loss score with k=11: 0.08901734104046244\n",
      "Average 0-1 Loss score with k=12: 0.10057803468208093\n",
      "Average 0-1 Loss score with k=13: 0.09421965317919076\n",
      "Average 0-1 Loss score with k=14: 0.10173410404624277\n",
      "Best k is 5 with the lowest average 0-1 loss score of 0.08092485549132947\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = np.arange(1,15,1)\n",
    "scores_dict = {}\n",
    "\n",
    "for k in hyperparameters: \n",
    "    scores = []\n",
    "    for i, (train_set_1, train_set_2) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "        \n",
    "        condesed_train_set = knn_model.condensed_knn_classification(train_set_1)\n",
    "\n",
    "        data_val_stand = data_processor.standardize_data(condesed_train_set, data_val, features=features)\n",
    "        data_train_stand = data_processor.standardize_data(condesed_train_set,condesed_train_set, features=features)\n",
    "\n",
    "        predictions_1 = knn_model.knn_classifier(data_val_stand, data_train_stand, k=k)['Predicted Class']\n",
    "        score_1 = Evaluation().zero_one_loss(data_val_stand[config['target_column']], predictions_1)\n",
    "        scores.append(score_1)\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average 0-1 Loss score with k={k}: {average_score}\")\n",
    "    scores_dict[k] = average_score\n",
    "\n",
    "best_k = min(scores_dict, key=scores_dict.get)\n",
    "print(f\"Best k is {best_k} with the lowest average 0-1 loss score of {scores_dict[best_k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average null model 0-1 loss score: 0.2945007235890015\n",
      "Average null model Precision score: 0.4977297525974856\n",
      "Average null model Recall score: 0.7054992764109985\n",
      "Average null model F1 score: 0.5836758958553583\n",
      "Average KNN 0-1 score for k=5: 0.07337192474674384\n",
      "Average Precision score for k=5: 0.9315580120665174\n",
      "Average Recall score for k=5: 0.9266280752532563\n",
      "Average F1 score for k=5: 0.9265942947019177\n"
     ]
    }
   ],
   "source": [
    "zero_one_loss_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "null_model_scores = []\n",
    "\n",
    "# Lists to store null model metrics\n",
    "null_model_precision_scores = []\n",
    "null_model_recall_scores = []\n",
    "null_model_f1_scores = []\n",
    "\n",
    "for i, (train_set, test_set) in enumerate(cross_validator.cross_validation(data_train, n_splits=2, n_repeats=5, random_state=42, stratify=True)):\n",
    "    \n",
    "    condesed_train_set = knn_model.condensed_knn_classification(train_set)\n",
    "\n",
    "    data_test_stand = data_processor.standardize_data(condesed_train_set, test_set, features=features)\n",
    "    data_train_stand = data_processor.standardize_data(condesed_train_set,condesed_train_set, features=features)\n",
    "\n",
    "    # Train and evaluate \n",
    "    predictions_1 = knn_model.knn_classifier(data_test_stand, data_train_stand, k=best_k)['Predicted Class']\n",
    "    \n",
    "    zero_one_loss_score = Evaluation.zero_one_loss(data_test_stand[config['target_column']], predictions_1)\n",
    "    precision_score = Evaluation.precision(data_test_stand[config['target_column']], predictions_1)\n",
    "    recall_score = Evaluation.recall(data_test_stand[config['target_column']], predictions_1)\n",
    "    f1_score = Evaluation.f1_score(data_test_stand[config['target_column']], predictions_1)\n",
    "    \n",
    "    zero_one_loss_scores.append(zero_one_loss_score)\n",
    "    precision_scores.append(precision_score)\n",
    "    recall_scores.append(recall_score)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # Evaluate null model\n",
    "    null_model_prediction = null_model.naive_classifier(test_set)\n",
    "    null_model_zero_one_loss = Evaluation.zero_one_loss(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_precision = Evaluation.precision(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_recall = Evaluation.recall(test_set[config['target_column']], null_model_prediction)\n",
    "    null_model_f1 = Evaluation.f1_score(test_set[config['target_column']], null_model_prediction)\n",
    "    \n",
    "    null_model_scores.append(null_model_zero_one_loss)\n",
    "    null_model_precision_scores.append(null_model_precision)\n",
    "    null_model_recall_scores.append(null_model_recall)\n",
    "    null_model_f1_scores.append(null_model_f1)\n",
    "\n",
    "average_01_score = sum(zero_one_loss_scores) / len(zero_one_loss_scores)\n",
    "average_precision_score = sum(precision_scores) / len(precision_scores)\n",
    "average_recall_score = sum(recall_scores) / len(recall_scores)\n",
    "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "average_null_model_score = sum(null_model_scores) / len(null_model_scores)\n",
    "average_null_model_precision = sum(null_model_precision_scores) / len(null_model_precision_scores)\n",
    "average_null_model_recall = sum(null_model_recall_scores) / len(null_model_recall_scores)\n",
    "average_null_model_f1 = sum(null_model_f1_scores) / len(null_model_f1_scores)\n",
    "\n",
    "print(f\"Average null model 0-1 loss score: {average_null_model_score}\")\n",
    "print(f\"Average null model Precision score: {average_null_model_precision}\")\n",
    "print(f\"Average null model Recall score: {average_null_model_recall}\")\n",
    "print(f\"Average null model F1 score: {average_null_model_f1}\")\n",
    "print(f\"Average KNN 0-1 score for k={best_k}: {average_01_score}\")\n",
    "print(f\"Average Precision score for k={best_k}: {average_precision_score}\")\n",
    "print(f\"Average Recall score for k={best_k}: {average_recall_score}\")\n",
    "print(f\"Average F1 score for k={best_k}: {average_f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
